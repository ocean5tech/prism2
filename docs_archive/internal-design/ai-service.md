# AI Service - å†…éƒ¨è®¾è®¡æ–‡æ¡£

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯

- **æ¨¡å—åç§°**: AI Service (AIæ¨¡å‹æœåŠ¡)
- **æŠ€æœ¯æ ˆ**: Ollama + Qwen2.5-7B + FastAPI + APIå°è£…
- **éƒ¨ç½²ç«¯å£**: 11434
- **ä¾æ®**: å¤–éƒ¨è®¾è®¡æ–‡æ¡£è§„èŒƒ

---

## ğŸ“ æ–‡ä»¶ç»“æ„å’Œæƒé™

```
/home/wyatt/prism2/ai-service/
â”œâ”€â”€ ollama/                               # OllamaæœåŠ¡ç›®å½• (755)
â”‚   â”œâ”€â”€ docker-compose.yml               # Ollama Dockeré…ç½® (644)
â”‚   â”œâ”€â”€ .env                             # Ollamaç¯å¢ƒå˜é‡ (600)
â”‚   â””â”€â”€ models/                          # æ¨¡å‹å­˜å‚¨ç›®å½• (755)
â”‚       â”œâ”€â”€ qwen2.5-7b/                  # Qwen2.5-7Bæ¨¡å‹ (755)
â”‚       â”œâ”€â”€ deepseek-coder-1.3b/         # DeepSeek Coderæ¨¡å‹ (755)
â”‚       â””â”€â”€ bge-large-zh-v1.5/           # ä¸­æ–‡å‘é‡æ¨¡å‹ (755)
â”œâ”€â”€ api_wrapper/                         # APIå°è£…å±‚ (755)
â”‚   â”œâ”€â”€ __init__.py                      # (644)
â”‚   â”œâ”€â”€ main.py                          # FastAPIåº”ç”¨å…¥å£ (644)
â”‚   â”œâ”€â”€ core/                            # æ ¸å¿ƒé…ç½® (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # (644)
â”‚   â”‚   â”œâ”€â”€ config.py                    # é…ç½®ç®¡ç† (644)
â”‚   â”‚   â””â”€â”€ dependencies.py              # ä¾èµ–æ³¨å…¥ (644)
â”‚   â”œâ”€â”€ api/                             # APIè·¯ç”± (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # (644)
â”‚   â”‚   â””â”€â”€ v1/                          # APIç‰ˆæœ¬1 (755)
â”‚   â”‚       â”œâ”€â”€ __init__.py              # (644)
â”‚   â”‚       â”œâ”€â”€ generate.py              # æ–‡æœ¬ç”Ÿæˆç«¯ç‚¹ (644)
â”‚   â”‚       â”œâ”€â”€ embed.py                 # å‘é‡åµŒå…¥ç«¯ç‚¹ (644)
â”‚   â”‚       â”œâ”€â”€ models.py                # æ¨¡å‹ç®¡ç†ç«¯ç‚¹ (644)
â”‚   â”‚       â””â”€â”€ health.py                # å¥åº·æ£€æŸ¥ç«¯ç‚¹ (644)
â”‚   â”œâ”€â”€ services/                        # ä¸šåŠ¡æœåŠ¡å±‚ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # (644)
â”‚   â”‚   â”œâ”€â”€ ollama_client.py             # Ollamaå®¢æˆ·ç«¯ (644)
â”‚   â”‚   â”œâ”€â”€ generation_service.py        # æ–‡æœ¬ç”ŸæˆæœåŠ¡ (644)
â”‚   â”‚   â”œâ”€â”€ embedding_service.py         # å‘é‡åµŒå…¥æœåŠ¡ (644)
â”‚   â”‚   â”œâ”€â”€ model_manager.py             # æ¨¡å‹ç®¡ç†æœåŠ¡ (644)
â”‚   â”‚   â””â”€â”€ cache_service.py             # ç¼“å­˜æœåŠ¡ (644)
â”‚   â”œâ”€â”€ models/                          # æ•°æ®æ¨¡å‹ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # (644)
â”‚   â”‚   â”œâ”€â”€ requests.py                  # è¯·æ±‚æ¨¡å‹ (644)
â”‚   â”‚   â””â”€â”€ responses.py                 # å“åº”æ¨¡å‹ (644)
â”‚   â”œâ”€â”€ middleware/                      # ä¸­é—´ä»¶ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # (644)
â”‚   â”‚   â”œâ”€â”€ rate_limit.py                # é™æµä¸­é—´ä»¶ (644)
â”‚   â”‚   â””â”€â”€ logging.py                   # æ—¥å¿—ä¸­é—´ä»¶ (644)
â”‚   â””â”€â”€ utils/                           # å·¥å…·å‡½æ•° (755)
â”‚       â”œâ”€â”€ __init__.py                  # (644)
â”‚       â”œâ”€â”€ text_processor.py            # æ–‡æœ¬å¤„ç†å·¥å…· (644)
â”‚       â”œâ”€â”€ performance_monitor.py       # æ€§èƒ½ç›‘æ§ (644)
â”‚       â””â”€â”€ prompt_templates.py          # æç¤ºè¯æ¨¡æ¿ (644)
â”œâ”€â”€ scripts/                             # è¿ç»´è„šæœ¬ (755)
â”‚   â”œâ”€â”€ install_models.sh                # æ¨¡å‹å®‰è£…è„šæœ¬ (755)
â”‚   â”œâ”€â”€ start_services.sh                # æœåŠ¡å¯åŠ¨è„šæœ¬ (755)
â”‚   â”œâ”€â”€ health_check.sh                  # å¥åº·æ£€æŸ¥è„šæœ¬ (755)
â”‚   â””â”€â”€ benchmark.py                     # æ€§èƒ½åŸºå‡†æµ‹è¯• (755)
â”œâ”€â”€ requirements.txt                     # Pythonä¾èµ– (644)
â””â”€â”€ README.md                            # éƒ¨ç½²è¯´æ˜ (644)
```

---

## ğŸ¤– OllamaæœåŠ¡é…ç½®

### Docker Composeé…ç½® (ollama/docker-compose.yml)
```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: prism2-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # GPUé…ç½® (å¦‚æœæœ‰GPU)
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Ollamaé…ç½®
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_QUEUE=10

      # æ¨¡å‹é…ç½®
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_KEEP_ALIVE=24h

    volumes:
      # æ¨¡å‹æŒä¹…åŒ–å­˜å‚¨
      - ./models:/root/.ollama/models
      - ./logs:/var/log/ollama

    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    networks:
      - prism2-network

    # GPUæ”¯æŒ (å¦‚æœæœ‰NVIDIA GPU)
    runtime: nvidia

networks:
  prism2-network:
    external: true
    name: prism2_default
```

### Ollamaç¯å¢ƒé…ç½® (ollama/.env)
```bash
# OllamaæœåŠ¡é…ç½®
OLLAMA_HOST=0.0.0.0
OLLAMA_PORT=11434
OLLAMA_ORIGINS=*

# æ€§èƒ½é…ç½®
OLLAMA_MAX_LOADED_MODELS=2
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_QUEUE=10
OLLAMA_KEEP_ALIVE=24h

# æ¨¡å‹å­˜å‚¨
OLLAMA_MODELS=/root/.ollama/models

# GPUé…ç½® (å¦‚æœæœ‰GPU)
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all

# æ—¥å¿—é…ç½®
OLLAMA_LOG_LEVEL=INFO
```

---

## ğŸ”Œ APIæ¥å£å®šä¹‰ (ä¸¥æ ¼æŒ‰ç…§å¤–éƒ¨è®¾è®¡)

### åŸºç¡€é…ç½®
```python
# APIå°è£…æœåŠ¡é…ç½®
API_SERVICE_PORT = int(os.getenv('API_SERVICE_PORT', '11435'))
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
MAX_CONCURRENT_REQUESTS = int(os.getenv('MAX_CONCURRENT_REQUESTS', '5'))
REQUEST_TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', '300'))
```

### API1: æ–‡æœ¬ç”Ÿæˆ (å¯¹æ¥å¤–éƒ¨è®¾è®¡æ¥å£)
- **URL**: `POST /api/ai/generate`
- **è¾“å…¥å‚æ•°**: ä¸¥æ ¼æŒ‰ç…§å¤–éƒ¨è®¾è®¡çš„AIç”Ÿæˆè¯·æ±‚
  ```python
  class AIGenerateRequest(BaseModel):
      prompt: str                          # ç”¨æˆ·æç¤ºè¯
      context: Optional[str] = None        # èƒŒæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯
      model: str = "qwen2.5:7b"           # ä½¿ç”¨çš„æ¨¡å‹
      max_tokens: int = 1000              # æœ€å¤§tokenæ•°
      temperature: float = 0.7            # æ¸©åº¦å‚æ•° 0-1
      top_p: float = 0.9                  # Top-pé‡‡æ ·
      top_k: int = 40                     # Top-ké‡‡æ ·
      system_prompt: Optional[str] = None  # ç³»ç»Ÿæç¤ºè¯
      stream: bool = False                 # æ˜¯å¦æµå¼è¿”å›
  ```
- **è¾“å‡ºç»“æœ**: å¤–éƒ¨è®¾è®¡çš„AIGenerateResponse
  ```python
  class AIGenerateResponse(BaseModel):
      generated_text: str                  # ç”Ÿæˆçš„æ–‡æœ¬
      model_used: str                      # ä½¿ç”¨çš„æ¨¡å‹
      tokens_used: int                     # æ¶ˆè€—çš„tokenæ•°
      generation_time: float               # ç”Ÿæˆæ—¶é—´(ç§’)
      confidence_score: float              # ç½®ä¿¡åº¦è¯„åˆ† 0-1
      finish_reason: str                   # å®ŒæˆåŸå› : stop/length/error
      metadata: Dict[str, Any]             # æ‰©å±•å…ƒæ•°æ®
  ```
- **èµ„æº**: OllamaæœåŠ¡ã€æ¨¡å‹æ¨ç†å¼•æ“
- **é€»è¾‘**: æ¥æ”¶ç”Ÿæˆè¯·æ±‚ï¼ŒéªŒè¯å‚æ•°æœ‰æ•ˆæ€§ï¼Œè°ƒç”¨Ollama APIæ‰§è¡Œæ¨ç†ï¼Œç›‘æ§ç”Ÿæˆè¿‡ç¨‹ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼Œè¿”å›ç”Ÿæˆç»“æœå’Œç»Ÿè®¡ä¿¡æ¯

### API2: æ–‡æœ¬å‘é‡åµŒå…¥ (å¯¹æ¥RAG Service)
- **URL**: `POST /api/ai/embed`
- **è¾“å…¥å‚æ•°**: å¤–éƒ¨è®¾è®¡çš„å‘é‡åµŒå…¥è¯·æ±‚
  ```python
  class EmbeddingRequest(BaseModel):
      text: Union[str, List[str]]          # å¾…åµŒå…¥çš„æ–‡æœ¬(æ”¯æŒæ‰¹é‡)
      model: str = "bge-large-zh-v1.5"    # å‘é‡æ¨¡å‹
      normalize: bool = True               # æ˜¯å¦æ ‡å‡†åŒ–å‘é‡
      encoding_format: str = "float"       # ç¼–ç æ ¼å¼: float/base64
  ```
- **è¾“å‡ºç»“æœ**: å¤–éƒ¨è®¾è®¡çš„EmbeddingResponse
  ```python
  class EmbeddingResponse(BaseModel):
      embedding: Union[List[float], List[List[float]]]  # å‘é‡æ•°æ®
      model_used: str                      # ä½¿ç”¨çš„æ¨¡å‹
      text_length: int                     # æ–‡æœ¬é•¿åº¦
      embedding_time: float                # åµŒå…¥æ—¶é—´(ç§’)
      dimension: int                       # å‘é‡ç»´åº¦
      token_count: int                     # tokenæ•°é‡
  ```
- **èµ„æº**: bge-large-zh-v1.5å‘é‡æ¨¡å‹ã€å‘é‡è®¡ç®—å¼•æ“
- **é€»è¾‘**: æ¥æ”¶æ–‡æœ¬åµŒå…¥è¯·æ±‚ï¼Œæ”¯æŒå•ä¸ªå’Œæ‰¹é‡æ–‡æœ¬å¤„ç†ï¼Œè°ƒç”¨ä¸­æ–‡å‘é‡æ¨¡å‹ç”Ÿæˆembeddingï¼Œæ‰§è¡Œå‘é‡æ ‡å‡†åŒ–ï¼Œè¿”å›å‘é‡æ•°æ®å’Œæ€§èƒ½ç»Ÿè®¡

### API3: æ¨¡å‹ç®¡ç† (å¯¹æ¥Open WebUI)
- **URL**: `GET /api/ai/models`
- **è¾“å…¥å‚æ•°**: æ¨¡å‹æŸ¥è¯¢è¯·æ±‚
  ```python
  class ModelListRequest(BaseModel):
      include_details: bool = False        # æ˜¯å¦åŒ…å«è¯¦ç»†ä¿¡æ¯
      filter_loaded: Optional[bool] = None # è¿‡æ»¤å·²åŠ è½½çš„æ¨¡å‹
  ```
- **è¾“å‡ºç»“æœ**: æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
  ```python
  class ModelInfo(BaseModel):
      name: str                           # æ¨¡å‹åç§°
      size: str                           # æ¨¡å‹å¤§å°
      modified_at: datetime               # ä¿®æ”¹æ—¶é—´
      digest: str                         # æ¨¡å‹æ‘˜è¦
      status: str                         # çŠ¶æ€: "loaded" | "unloaded"
      parameters: Dict[str, Any]          # æ¨¡å‹å‚æ•°
      capabilities: List[str]             # æ¨¡å‹èƒ½åŠ›

  class ModelListResponse(BaseModel):
      models: List[ModelInfo]
      total_count: int
      loaded_count: int
      available_memory: str
  ```
- **èµ„æº**: Ollamaæ¨¡å‹ç®¡ç†ã€ç³»ç»Ÿèµ„æºç›‘æ§
- **é€»è¾‘**: æŸ¥è¯¢Ollamaä¸­çš„æ¨¡å‹åˆ—è¡¨ï¼Œè·å–æ¨¡å‹çŠ¶æ€å’Œå…ƒæ•°æ®ï¼Œæ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µï¼Œè¿”å›å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯å’ŒçŠ¶æ€

### API4: æ¨¡å‹åŠ è½½/å¸è½½
- **URL**: `POST /api/ai/models/{model_name}/load`
- **URL**: `POST /api/ai/models/{model_name}/unload`
- **è¾“å…¥å‚æ•°**: æ¨¡å‹æ“ä½œè¯·æ±‚
  ```python
  class ModelOperationRequest(BaseModel):
      force: bool = False                 # æ˜¯å¦å¼ºåˆ¶æ“ä½œ
      timeout: int = 300                  # æ“ä½œè¶…æ—¶æ—¶é—´
  ```
- **è¾“å‡ºç»“æœ**: æ“ä½œç»“æœ
  ```python
  class ModelOperationResponse(BaseModel):
      success: bool                       # æ“ä½œæ˜¯å¦æˆåŠŸ
      model_name: str                     # æ¨¡å‹åç§°
      operation: str                      # æ“ä½œç±»å‹
      execution_time: float               # æ‰§è¡Œæ—¶é—´
      memory_usage: str                   # å†…å­˜ä½¿ç”¨æƒ…å†µ
      message: str                        # çŠ¶æ€æ¶ˆæ¯
  ```
- **èµ„æº**: Ollamaæ¨¡å‹ç®¡ç†ã€å†…å­˜ç›‘æ§
- **é€»è¾‘**: æ‰§è¡Œæ¨¡å‹çš„åŠ è½½æˆ–å¸è½½æ“ä½œï¼Œç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¤„ç†æ“ä½œè¶…æ—¶å’Œé”™è¯¯ï¼Œè¿”å›æ“ä½œçŠ¶æ€å’Œèµ„æºä¿¡æ¯

### API5: æµå¼æ–‡æœ¬ç”Ÿæˆ (æ”¯æŒå®æ—¶æ¨ç†)
- **URL**: `POST /api/ai/generate/stream`
- **è¾“å…¥å‚æ•°**: æµå¼ç”Ÿæˆè¯·æ±‚
  ```python
  class StreamGenerateRequest(BaseModel):
      prompt: str                         # ç”¨æˆ·æç¤ºè¯
      model: str = "qwen2.5:7b"          # ä½¿ç”¨çš„æ¨¡å‹
      max_tokens: int = 1000             # æœ€å¤§tokenæ•°
      temperature: float = 0.7            # æ¸©åº¦å‚æ•°
      system_prompt: Optional[str] = None # ç³»ç»Ÿæç¤ºè¯
  ```
- **è¾“å‡ºç»“æœ**: æœåŠ¡å™¨ç«¯äº‹ä»¶æµ (SSE)
  ```python
  # SSEäº‹ä»¶æ ¼å¼
  data: {
      "type": "token",
      "content": "ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ",
      "token_count": 10,
      "is_final": false
  }

  data: {
      "type": "final",
      "total_tokens": 150,
      "generation_time": 2.5,
      "finish_reason": "stop"
  }
  ```
- **èµ„æº**: Ollamaæµå¼APIã€WebSocketè¿æ¥
- **é€»è¾‘**: å»ºç«‹æµå¼è¿æ¥ï¼Œé€tokenç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼Œå®æ—¶æ¨é€ç”Ÿæˆè¿›åº¦ï¼Œç›‘æ§ç”ŸæˆçŠ¶æ€ï¼Œåœ¨å®Œæˆæ—¶å‘é€æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯

---

## ğŸ”§ æ ¸å¿ƒæœåŠ¡å®ç°

### 1. OllamaClient (Ollamaå®¢æˆ·ç«¯)
- **æ–‡ä»¶**: `api_wrapper/services/ollama_client.py`
- **åŠŸèƒ½**: OllamaæœåŠ¡çš„ç»Ÿä¸€å®¢æˆ·ç«¯æ¥å£
- **è¾“å…¥**: APIè¯·æ±‚å‚æ•°
- **è¾“å‡º**: Ollamaå“åº”æ•°æ®
- **èµ„æº**: HTTPå®¢æˆ·ç«¯ã€è¿æ¥æ± 
- **é€»è¾‘**: ç®¡ç†ä¸Ollamaçš„HTTPè¿æ¥ï¼Œå®ç°è¯·æ±‚é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼Œæä¾›ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£ï¼Œå¤„ç†å¹¶å‘è¯·æ±‚å’Œè¿æ¥æ± ç®¡ç†

### 2. GenerationService (æ–‡æœ¬ç”ŸæˆæœåŠ¡)
- **æ–‡ä»¶**: `api_wrapper/services/generation_service.py`
- **åŠŸèƒ½**: æ™ºèƒ½æ–‡æœ¬ç”Ÿæˆå’Œæ¨ç†ç®¡ç†
- **è¾“å…¥**: ç”Ÿæˆè¯·æ±‚å’Œå‚æ•°
- **è¾“å‡º**: ç”Ÿæˆæ–‡æœ¬å’Œæ€§èƒ½ç»Ÿè®¡
- **èµ„æº**: Ollamaæ¨ç†å¼•æ“ã€æç¤ºè¯æ¨¡æ¿
- **é€»è¾‘**: å¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚ï¼Œä¼˜åŒ–æç¤ºè¯æ¨¡æ¿ï¼Œç®¡ç†ç”Ÿæˆå‚æ•°ï¼Œç›‘æ§æ¨ç†æ€§èƒ½ï¼Œå®ç°ç¼“å­˜ç­–ç•¥æé«˜æ•ˆç‡

### 3. EmbeddingService (å‘é‡åµŒå…¥æœåŠ¡)
- **æ–‡ä»¶**: `api_wrapper/services/embedding_service.py`
- **åŠŸèƒ½**: æ–‡æœ¬å‘é‡åŒ–å’ŒåµŒå…¥ç®¡ç†
- **è¾“å…¥**: æ–‡æœ¬å†…å®¹å’ŒåµŒå…¥å‚æ•°
- **è¾“å‡º**: å‘é‡æ•°æ®å’Œå¤„ç†ç»Ÿè®¡
- **é€»è¾‘**: è°ƒç”¨å‘é‡æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†æé«˜æ•ˆç‡ï¼Œå®ç°å‘é‡ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—ï¼Œæä¾›å‘é‡æ ‡å‡†åŒ–å’Œæ ¼å¼è½¬æ¢

### 4. ModelManager (æ¨¡å‹ç®¡ç†æœåŠ¡)
- **æ–‡ä»¶**: `api_wrapper/services/model_manager.py`
- **åŠŸèƒ½**: AIæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
- **è¾“å…¥**: æ¨¡å‹æ“ä½œè¯·æ±‚
- **è¾“å‡º**: æ¨¡å‹çŠ¶æ€å’Œèµ„æºä¿¡æ¯
- **é€»è¾‘**: ç®¡ç†æ¨¡å‹çš„åŠ è½½ã€å¸è½½å’Œåˆ‡æ¢ï¼Œç›‘æ§æ¨¡å‹å†…å­˜ä½¿ç”¨ï¼Œå®ç°æ™ºèƒ½æ¨¡å‹è°ƒåº¦ï¼Œç»´æŠ¤æ¨¡å‹çŠ¶æ€ç¼“å­˜

### 5. CacheService (ç¼“å­˜æœåŠ¡)
- **æ–‡ä»¶**: `api_wrapper/services/cache_service.py`
- **åŠŸèƒ½**: æ™ºèƒ½ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
- **è¾“å…¥**: è¯·æ±‚å‚æ•°å’Œç¼“å­˜ç­–ç•¥
- **è¾“å‡º**: ç¼“å­˜å‘½ä¸­æˆ–åŸå§‹ç»“æœ
- **é€»è¾‘**: å®ç°å¤šå±‚çº§ç¼“å­˜ç­–ç•¥ï¼Œç¼“å­˜å¸¸ç”¨çš„ç”Ÿæˆç»“æœå’Œå‘é‡æ•°æ®ï¼Œç®¡ç†ç¼“å­˜è¿‡æœŸå’Œæ¸…ç†ï¼Œæä¾›ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡

---

## ğŸš€ æ¨¡å‹é…ç½®å’Œä¼˜åŒ–

### æ”¯æŒçš„AIæ¨¡å‹é…ç½®
```python
# æ¨¡å‹é…ç½®å­—å…¸
SUPPORTED_MODELS = {
    "qwen2.5:7b": {
        "type": "text_generation",
        "size": "7B",
        "context_length": 8192,
        "memory_requirement": "6GB",
        "optimal_batch_size": 1,
        "default_parameters": {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "repeat_penalty": 1.1
        },
        "specialties": ["financial_analysis", "general_chat", "reasoning"],
        "languages": ["zh", "en"]
    },
    "deepseek-coder:1.3b": {
        "type": "code_generation",
        "size": "1.3B",
        "context_length": 4096,
        "memory_requirement": "2GB",
        "optimal_batch_size": 2,
        "default_parameters": {
            "temperature": 0.3,
            "top_p": 0.8,
            "top_k": 20
        },
        "specialties": ["code_generation", "programming", "algorithms"],
        "languages": ["python", "javascript", "sql"]
    },
    "bge-large-zh-v1.5": {
        "type": "text_embedding",
        "size": "335M",
        "embedding_dimension": 1024,
        "memory_requirement": "1GB",
        "optimal_batch_size": 8,
        "specialties": ["chinese_embedding", "semantic_search"],
        "languages": ["zh"]
    }
}
```

### æ€§èƒ½ä¼˜åŒ–é…ç½®
```python
# æ¨ç†ä¼˜åŒ–é…ç½®
INFERENCE_CONFIG = {
    "batch_processing": {
        "enabled": True,
        "max_batch_size": 4,
        "batch_timeout": 100  # ms
    },
    "caching": {
        "enabled": True,
        "ttl": 3600,  # 1 hour
        "max_cache_size": "1GB"
    },
    "model_switching": {
        "auto_unload": True,
        "idle_timeout": 1800,  # 30 minutes
        "memory_threshold": 0.8
    },
    "concurrent_requests": {
        "max_concurrent": 5,
        "queue_size": 20,
        "timeout": 300
    }
}
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•

### æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ (scripts/benchmark.py)
```python
#!/usr/bin/env python3
"""
AI Service æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
"""
import asyncio
import time
import statistics
import requests
import json
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class AIServiceBenchmark:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.results = {}

    async def test_text_generation(self, model="qwen2.5:7b", num_requests=10):
        """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆæ€§èƒ½"""
        print(f"æµ‹è¯•æ–‡æœ¬ç”Ÿæˆæ€§èƒ½ ({model})...")

        test_prompts = [
            "åˆ†æå¹³å®‰é“¶è¡Œçš„æŠ•èµ„ä»·å€¼",
            "è§£é‡Šä»€ä¹ˆæ˜¯è‚¡ç¥¨å¸‚åœº",
            "æè¿°æŠ€æœ¯åˆ†æçš„åŸºæœ¬åŸç†",
            "ä»€ä¹ˆæ˜¯è´¢åŠ¡æŠ¥è¡¨åˆ†æ"
        ]

        latencies = []
        token_rates = []

        for i in range(num_requests):
            prompt = test_prompts[i % len(test_prompts)]
            start_time = time.time()

            response = requests.post(f"{self.base_url}/api/ai/generate", json={
                "prompt": prompt,
                "model": model,
                "max_tokens": 500,
                "temperature": 0.7
            })

            end_time = time.time()
            latency = end_time - start_time

            if response.status_code == 200:
                data = response.json()
                tokens_used = data.get("tokens_used", 0)
                token_rate = tokens_used / latency if latency > 0 else 0

                latencies.append(latency)
                token_rates.append(token_rate)

                print(f"  è¯·æ±‚ {i+1}: {latency:.2f}s, {token_rate:.1f} tokens/s")

        # ç»Ÿè®¡ç»“æœ
        avg_latency = statistics.mean(latencies)
        avg_token_rate = statistics.mean(token_rates)
        p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)

        self.results[f"{model}_generation"] = {
            "avg_latency": avg_latency,
            "p95_latency": p95_latency,
            "avg_token_rate": avg_token_rate,
            "success_rate": len(latencies) / num_requests
        }

        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
        print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}s")
        print(f"  å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_token_rate:.1f} tokens/s")

    async def test_embedding_performance(self, num_requests=50):
        """æµ‹è¯•å‘é‡åµŒå…¥æ€§èƒ½"""
        print("æµ‹è¯•å‘é‡åµŒå…¥æ€§èƒ½...")

        test_texts = [
            "å¹³å®‰é“¶è¡Œæ˜¯ä¸­å›½ä¸»è¦çš„è‚¡ä»½åˆ¶å•†ä¸šé“¶è¡Œä¹‹ä¸€",
            "æŠ€æœ¯åˆ†ææ˜¯é€šè¿‡ç ”ç©¶ä»·æ ¼å›¾è¡¨æ¥é¢„æµ‹å¸‚åœºèµ°åŠ¿çš„æ–¹æ³•",
            "åŸºæœ¬é¢åˆ†æå…³æ³¨å…¬å¸çš„è´¢åŠ¡å¥åº·çŠ¶å†µå’Œå†…åœ¨ä»·å€¼",
            "æŠ•èµ„ç»„åˆç®¡ç†éœ€è¦è€ƒè™‘é£é™©åˆ†æ•£å’Œèµ„äº§é…ç½®"
        ]

        latencies = []

        for i in range(num_requests):
            text = test_texts[i % len(test_texts)]
            start_time = time.time()

            response = requests.post(f"{self.base_url}/api/ai/embed", json={
                "text": text,
                "model": "bge-large-zh-v1.5"
            })

            end_time = time.time()
            latency = end_time - start_time

            if response.status_code == 200:
                latencies.append(latency)
                print(f"  åµŒå…¥ {i+1}: {latency:.3f}s")

        # ç»Ÿè®¡ç»“æœ
        avg_latency = statistics.mean(latencies)
        throughput = len(latencies) / sum(latencies)

        self.results["embedding"] = {
            "avg_latency": avg_latency,
            "throughput": throughput,
            "success_rate": len(latencies) / num_requests
        }

        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.3f}s")
        print(f"  ååé‡: {throughput:.1f} req/s")

    async def test_concurrent_load(self, concurrent_users=5, requests_per_user=5):
        """æµ‹è¯•å¹¶å‘è´Ÿè½½æ€§èƒ½"""
        print(f"æµ‹è¯•å¹¶å‘è´Ÿè½½ ({concurrent_users} ç”¨æˆ·, æ¯ç”¨æˆ· {requests_per_user} è¯·æ±‚)...")

        async def user_session(user_id):
            latencies = []
            for i in range(requests_per_user):
                start_time = time.time()

                response = requests.post(f"{self.base_url}/api/ai/generate", json={
                    "prompt": f"ç”¨æˆ·{user_id}çš„æµ‹è¯•è¯·æ±‚{i+1}",
                    "model": "qwen2.5:7b",
                    "max_tokens": 100
                })

                end_time = time.time()
                if response.status_code == 200:
                    latencies.append(end_time - start_time)

            return latencies

        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(asyncio.run, user_session(i)) for i in range(concurrent_users)]
            all_latencies = []
            for future in futures:
                all_latencies.extend(future.result())

        # ç»Ÿè®¡ç»“æœ
        if all_latencies:
            avg_latency = statistics.mean(all_latencies)
            max_latency = max(all_latencies)
            throughput = len(all_latencies) / max(all_latencies)

            self.results["concurrent_load"] = {
                "avg_latency": avg_latency,
                "max_latency": max_latency,
                "throughput": throughput,
                "success_rate": len(all_latencies) / (concurrent_users * requests_per_user)
            }

            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
            print(f"  æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}s")
            print(f"  ç³»ç»Ÿååé‡: {throughput:.1f} req/s")

    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*50)
        print("AI Service æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*50)

        for test_name, metrics in self.results.items():
            print(f"\n{test_name.upper()}:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {metric}: {value:.3f}")
                else:
                    print(f"  {metric}: {value}")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open("benchmark_report.json", "w") as f:
            json.dump(self.results, f, indent=2, default=str)

        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: benchmark_report.json")

async def main():
    benchmark = AIServiceBenchmark()

    # æ‰§è¡Œå„é¡¹æµ‹è¯•
    await benchmark.test_text_generation("qwen2.5:7b", 10)
    await benchmark.test_embedding_performance(20)
    await benchmark.test_concurrent_load(3, 5)

    # ç”ŸæˆæŠ¥å‘Š
    benchmark.generate_report()

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ”’ ç¯å¢ƒé…ç½®

### APIå°è£…æœåŠ¡ç¯å¢ƒå˜é‡ (api_wrapper/.env)
```bash
# APIæœåŠ¡é…ç½®
API_SERVICE_PORT=11435
DEBUG=false

# Ollamaè¿æ¥é…ç½®
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_REQUEST_TIMEOUT=300
OLLAMA_MAX_RETRIES=3

# æ€§èƒ½é…ç½®
MAX_CONCURRENT_REQUESTS=5
REQUEST_QUEUE_SIZE=20
MODEL_LOAD_TIMEOUT=600

# ç¼“å­˜é…ç½®
REDIS_URL=redis://localhost:6379/6
CACHE_TTL=3600
MAX_CACHE_SIZE=1048576000  # 1GB

# é™æµé…ç½®
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_TOKENS_PER_DAY=50000

# ç›‘æ§é…ç½®
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO

# å®‰å…¨é…ç½®
API_KEY_REQUIRED=true
ALLOWED_HOSTS=["localhost", "127.0.0.1"]
CORS_ORIGINS=["http://localhost:3000", "http://localhost:3001"]
```

### ä¾èµ–é…ç½® (requirements.txt)
```txt
fastapi==0.104.1
uvicorn==0.24.0
httpx==0.25.2
redis==5.0.1
pydantic==2.5.0
python-multipart==0.0.6
slowapi==0.1.9
prometheus-client==0.19.0
structlog==23.2.0
tenacity==8.2.3
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### å…³é”®æ€§èƒ½æŒ‡æ ‡
- **æ¨ç†å»¶è¿Ÿ**: æ–‡æœ¬ç”Ÿæˆå’Œå‘é‡åµŒå…¥çš„å“åº”æ—¶é—´
- **ååé‡**: æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°é‡
- **æ¨¡å‹åˆ©ç”¨ç‡**: å„æ¨¡å‹çš„ä½¿ç”¨é¢‘ç‡å’Œè´Ÿè½½
- **å†…å­˜ä½¿ç”¨**: æ¨¡å‹å ç”¨çš„å†…å­˜å’Œç³»ç»Ÿèµ„æº
- **é”™è¯¯ç‡**: æ¨ç†å¤±è´¥å’Œè¶…æ—¶çš„æ¯”ä¾‹

### å‘Šè­¦è§„åˆ™é…ç½®
```python
ALERT_RULES = {
    'high_latency': {
        'threshold': 10.0,              # å»¶è¿Ÿè¶…è¿‡10ç§’
        'action': 'slack+email'
    },
    'low_throughput': {
        'threshold': 0.5,               # ååé‡ä½äº0.5 req/s
        'action': 'slack'
    },
    'memory_usage': {
        'threshold': 0.9,               # å†…å­˜ä½¿ç”¨è¶…è¿‡90%
        'action': 'email+restart'
    },
    'model_load_failure': {
        'threshold': 1,                 # æ¨¡å‹åŠ è½½å¤±è´¥
        'action': 'email+slack'
    }
}
```

---

*æ–‡æ¡£æ›´æ–°æ—¶é—´: 2025-09-16*
*ä¸¥æ ¼éµå¾ªå¤–éƒ¨è®¾è®¡è§„èŒƒï¼Œç¡®ä¿æ¥å£ä¸€è‡´æ€§*