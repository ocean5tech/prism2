#!/usr/bin/env python3
"""
Task Coordination MCP Server (Port 8009)

Orchestrates complex tasks across multiple MCP servers and provides
intelligent workflow management for financial analysis.
"""

import asyncio
import json
import httpx
from typing import Any, Dict, List, Optional, Sequence, Union
from datetime import datetime, timedelta
import uuid

# MCP SDK imports
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel,
    CallToolRequest,
    ListToolsRequest,
    ListResourcesRequest,
    ReadResourceRequest
)

# Shared modules imports
import sys
import os
sys.path.append('/home/wyatt/prism2/mcp_servers/shared')

from config import config
from logger import get_logger, log_mcp_call
from database import data_manager

logger = get_logger("Coordination-MCP")

# Initialize MCP server
server = Server("coordination-mcp")

class TaskOrchestrator:
    """ä»»åŠ¡ç¼–æ’å™¨ - åè°ƒå¤šä¸ªMCPæœåŠ¡å®Œæˆå¤æ‚ä»»åŠ¡"""

    def __init__(self):
        self.mcp_services = {
            "realtime_data": "http://localhost:8006",
            "structured_data": "http://localhost:8007",
            "rag_enhanced": "http://localhost:8008"
        }
        self.task_history = []
        self.active_workflows = {}

    async def call_mcp_service(self, service_name: str, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨æŒ‡å®šMCPæœåŠ¡çš„å·¥å…·"""
        try:
            if service_name not in self.mcp_services:
                return {"success": False, "error": f"æœªçŸ¥çš„MCPæœåŠ¡: {service_name}"}

            base_url = self.mcp_services[service_name]

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{base_url}/call_tool",
                    json={
                        "method": "tools/call",
                        "params": {
                            "name": tool_name,
                            "arguments": arguments
                        }
                    }
                )

                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"æˆåŠŸè°ƒç”¨ {service_name}.{tool_name}")
                    return {"success": True, "data": result}
                else:
                    logger.warning(f"MCPæœåŠ¡è°ƒç”¨å¤±è´¥: {service_name}.{tool_name} - {response.status_code}")
                    return {"success": False, "error": f"HTTP {response.status_code}: {response.text}"}

        except httpx.TimeoutException:
            logger.error(f"MCPæœåŠ¡è°ƒç”¨è¶…æ—¶: {service_name}.{tool_name}")
            return {"success": False, "error": "æœåŠ¡è°ƒç”¨è¶…æ—¶"}
        except Exception as e:
            logger.error(f"MCPæœåŠ¡è°ƒç”¨å¼‚å¸¸: {service_name}.{tool_name} - {e}")
            return {"success": False, "error": str(e)}

    async def execute_comprehensive_analysis(self, stock_code: str, analysis_type: str = "full") -> Dict[str, Any]:
        """æ‰§è¡Œç»¼åˆåˆ†æå·¥ä½œæµ"""
        workflow_id = str(uuid.uuid4())[:8]
        logger.info(f"å¯åŠ¨ç»¼åˆåˆ†æå·¥ä½œæµ {workflow_id} for {stock_code}")

        workflow_results = {
            "workflow_id": workflow_id,
            "stock_code": stock_code,
            "analysis_type": analysis_type,
            "start_time": datetime.now().isoformat(),
            "steps": [],
            "final_summary": None
        }

        try:
            # Step 1: è·å–å®æ—¶æ•°æ®
            logger.info(f"[{workflow_id}] Step 1: è·å–å®æ—¶æ•°æ®")
            realtime_result = await self.call_mcp_service(
                "realtime_data",
                "get_realtime_price",
                {"stock_code": stock_code}
            )
            workflow_results["steps"].append({
                "step": 1,
                "name": "å®æ—¶æ•°æ®è·å–",
                "success": realtime_result["success"],
                "data": realtime_result.get("data"),
                "timestamp": datetime.now().isoformat()
            })

            # Step 2: è·å–å†å²æ•°æ®å’Œè´¢åŠ¡ä¿¡æ¯
            logger.info(f"[{workflow_id}] Step 2: è·å–ç»“æ„åŒ–æ•°æ®")
            historical_result = await self.call_mcp_service(
                "structured_data",
                "get_historical_data",
                {"stock_code": stock_code, "period": "daily", "limit": 30}
            )
            workflow_results["steps"].append({
                "step": 2,
                "name": "å†å²æ•°æ®è·å–",
                "success": historical_result["success"],
                "data": historical_result.get("data"),
                "timestamp": datetime.now().isoformat()
            })

            # Step 3: è·å–è´¢åŠ¡æŠ¥è¡¨
            if analysis_type in ["full", "financial"]:
                logger.info(f"[{workflow_id}] Step 3: è·å–è´¢åŠ¡æŠ¥è¡¨")
                financial_result = await self.call_mcp_service(
                    "structured_data",
                    "get_financial_reports",
                    {"stock_code": stock_code, "report_type": "all", "periods": 4}
                )
                workflow_results["steps"].append({
                    "step": 3,
                    "name": "è´¢åŠ¡æŠ¥è¡¨è·å–",
                    "success": financial_result["success"],
                    "data": financial_result.get("data"),
                    "timestamp": datetime.now().isoformat()
                })

            # Step 4: RAGå¢å¼ºåˆ†æ
            logger.info(f"[{workflow_id}] Step 4: RAGèƒŒæ™¯åˆ†æ")
            rag_context_result = await self.call_mcp_service(
                "rag_enhanced",
                "search_financial_context",
                {"query": f"{stock_code} æœ€æ–°åŠ¨æ€ æŠ•èµ„åˆ†æ", "stock_code": stock_code, "max_results": 5}
            )
            workflow_results["steps"].append({
                "step": 4,
                "name": "RAGèƒŒæ™¯åˆ†æ",
                "success": rag_context_result["success"],
                "data": rag_context_result.get("data"),
                "timestamp": datetime.now().isoformat()
            })

            # Step 5: æ–°é—»æƒ…æ„Ÿåˆ†æ
            logger.info(f"[{workflow_id}] Step 5: æ–°é—»æƒ…æ„Ÿåˆ†æ")
            sentiment_result = await self.call_mcp_service(
                "rag_enhanced",
                "get_news_sentiment",
                {"stock_code": stock_code, "days": 7}
            )
            workflow_results["steps"].append({
                "step": 5,
                "name": "æ–°é—»æƒ…æ„Ÿåˆ†æ",
                "success": sentiment_result["success"],
                "data": sentiment_result.get("data"),
                "timestamp": datetime.now().isoformat()
            })

            # Step 6: ç”Ÿæˆå¸‚åœºæ´å¯Ÿ
            if analysis_type == "full":
                logger.info(f"[{workflow_id}] Step 6: ç”Ÿæˆå¸‚åœºæ´å¯Ÿ")
                insight_result = await self.call_mcp_service(
                    "rag_enhanced",
                    "generate_market_insight",
                    {
                        "topic": f"{stock_code} ç»¼åˆæŠ•èµ„åˆ†æ",
                        "stock_codes": [stock_code],
                        "analysis_depth": "comprehensive"
                    }
                )
                workflow_results["steps"].append({
                    "step": 6,
                    "name": "å¸‚åœºæ´å¯Ÿç”Ÿæˆ",
                    "success": insight_result["success"],
                    "data": insight_result.get("data"),
                    "timestamp": datetime.now().isoformat()
                })

            # ç”Ÿæˆæœ€ç»ˆç»¼åˆåˆ†ææ‘˜è¦
            workflow_results["final_summary"] = self._generate_workflow_summary(workflow_results)
            workflow_results["end_time"] = datetime.now().isoformat()
            workflow_results["duration"] = self._calculate_duration(workflow_results["start_time"], workflow_results["end_time"])

            # ä¿å­˜åˆ°å·¥ä½œæµå†å²
            self.task_history.append(workflow_results)

            logger.info(f"å·¥ä½œæµ {workflow_id} å®Œæˆï¼Œè€—æ—¶ {workflow_results['duration']}")
            return workflow_results

        except Exception as e:
            logger.error(f"å·¥ä½œæµ {workflow_id} æ‰§è¡Œå¤±è´¥: {e}")
            workflow_results["error"] = str(e)
            workflow_results["end_time"] = datetime.now().isoformat()
            return workflow_results

    def _generate_workflow_summary(self, workflow_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œæ‘˜è¦"""
        successful_steps = [step for step in workflow_results["steps"] if step["success"]]
        failed_steps = [step for step in workflow_results["steps"] if not step["success"]]

        return {
            "total_steps": len(workflow_results["steps"]),
            "successful_steps": len(successful_steps),
            "failed_steps": len(failed_steps),
            "success_rate": len(successful_steps) / len(workflow_results["steps"]) * 100 if workflow_results["steps"] else 0,
            "completed_tasks": [step["name"] for step in successful_steps],
            "failed_tasks": [step["name"] for step in failed_steps] if failed_steps else [],
            "status": "completed" if len(failed_steps) == 0 else "partial_failure" if successful_steps else "failed"
        }

    def _calculate_duration(self, start_time: str, end_time: str) -> str:
        """è®¡ç®—å·¥ä½œæµæ‰§è¡Œæ—¶é—´"""
        try:
            start = datetime.fromisoformat(start_time)
            end = datetime.fromisoformat(end_time)
            duration = end - start
            return f"{duration.total_seconds():.2f}ç§’"
        except:
            return "æœªçŸ¥"

    async def health_check_all_services(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ‰€æœ‰MCPæœåŠ¡çš„å¥åº·çŠ¶æ€"""
        health_status = {}

        for service_name, base_url in self.mcp_services.items():
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(f"{base_url}/health")
                    health_status[service_name] = {
                        "status": "healthy" if response.status_code == 200 else "unhealthy",
                        "response_code": response.status_code,
                        "url": base_url
                    }
            except Exception as e:
                health_status[service_name] = {
                    "status": "unreachable",
                    "error": str(e),
                    "url": base_url
                }

        return health_status

# åˆå§‹åŒ–ä»»åŠ¡ç¼–æ’å™¨
task_orchestrator = TaskOrchestrator()

@server.list_tools()
async def handle_list_tools() -> List[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="comprehensive_stock_analysis",
            description="æ‰§è¡Œå…¨é¢çš„è‚¡ç¥¨ç»¼åˆåˆ†æã€‚åè°ƒå¤šä¸ªMCPæœåŠ¡æä¾›å®Œæ•´çš„æŠ•èµ„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬å®æ—¶æ•°æ®ã€å†å²è¡¨ç°ã€è´¢åŠ¡çŠ¶å†µã€å¸‚åœºæƒ…æ„Ÿç­‰ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "stock_code": {
                        "type": "string",
                        "description": "è‚¡ç¥¨ä»£ç ï¼Œå¦‚'688469'"
                    },
                    "analysis_type": {
                        "type": "string",
                        "enum": ["quick", "standard", "full"],
                        "description": "åˆ†ææ·±åº¦ï¼šå¿«é€Ÿã€æ ‡å‡†ã€å…¨é¢",
                        "default": "standard"
                    },
                    "include_forecast": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«è¶‹åŠ¿é¢„æµ‹",
                        "default": False
                    }
                },
                "required": ["stock_code"]
            }
        ),
        Tool(
            name="cross_service_query",
            description="è·¨æœåŠ¡æŸ¥è¯¢å·¥å…·ã€‚å¯ä»¥åŒæ—¶æŸ¥è¯¢å¤šä¸ªMCPæœåŠ¡å¹¶æ•´åˆç»“æœã€‚ç”¨äºè·å–ç‰¹å®šä¿¡æ¯çš„ç»¼åˆè§†å›¾ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "query_plan": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "service": {"type": "string"},
                                "tool": {"type": "string"},
                                "arguments": {"type": "object"}
                            },
                            "required": ["service", "tool", "arguments"]
                        },
                        "description": "æŸ¥è¯¢è®¡åˆ’ï¼ŒåŒ…å«è¦è°ƒç”¨çš„æœåŠ¡ã€å·¥å…·å’Œå‚æ•°"
                    },
                    "parallel_execution": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢",
                        "default": True
                    }
                },
                "required": ["query_plan"]
            }
        ),
        Tool(
            name="workflow_status",
            description="æŸ¥çœ‹å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å’Œå†å²è®°å½•ã€‚å¯ä»¥æŸ¥è¯¢ç‰¹å®šå·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯æˆ–è·å–å†å²æ‰§è¡Œç»Ÿè®¡ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "workflow_id": {
                        "type": "string",
                        "description": "å·¥ä½œæµIDï¼Œå¯é€‰ï¼Œå¦‚ä¸æä¾›åˆ™è¿”å›æœ€è¿‘çš„å·¥ä½œæµ"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "è¿”å›çš„å†å²è®°å½•æ•°é‡",
                        "default": 10
                    }
                }
            }
        ),
        Tool(
            name="service_health_check",
            description="æ£€æŸ¥æ‰€æœ‰MCPæœåŠ¡çš„å¥åº·çŠ¶æ€ã€‚ç”¨äºç›‘æ§ç³»ç»ŸçŠ¶æ€å’Œè¯Šæ–­æœåŠ¡å¯ç”¨æ€§é—®é¢˜ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "detailed": {
                        "type": "boolean",
                        "description": "æ˜¯å¦è¿”å›è¯¦ç»†çš„å¥åº·æ£€æŸ¥ä¿¡æ¯",
                        "default": False
                    }
                }
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚"""

    log_mcp_call(logger, name, arguments)

    try:
        if name == "comprehensive_stock_analysis":
            return await _handle_comprehensive_stock_analysis(arguments)
        elif name == "cross_service_query":
            return await _handle_cross_service_query(arguments)
        elif name == "workflow_status":
            return await _handle_workflow_status(arguments)
        elif name == "service_health_check":
            return await _handle_service_health_check(arguments)
        else:
            error_msg = f"æœªçŸ¥çš„å·¥å…·: {name}"
            log_mcp_call(logger, name, arguments, error=error_msg)
            return [TextContent(type="text", text=f"âŒ é”™è¯¯: {error_msg}")]

    except Exception as e:
        error_msg = f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {str(e)}"
        log_mcp_call(logger, name, arguments, error=error_msg)
        return [TextContent(type="text", text=f"âŒ é”™è¯¯: {error_msg}")]

async def _handle_comprehensive_stock_analysis(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†ç»¼åˆè‚¡ç¥¨åˆ†æ"""

    stock_code = arguments.get("stock_code", "")
    analysis_type = arguments.get("analysis_type", "standard")
    include_forecast = arguments.get("include_forecast", False)

    if not stock_code:
        return [TextContent(type="text", text="âŒ è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")]

    # æ‰§è¡Œç»¼åˆåˆ†æå·¥ä½œæµ
    workflow_result = await task_orchestrator.execute_comprehensive_analysis(stock_code, analysis_type)

    # æ ¼å¼åŒ–ç»“æœ
    result_text = f"ğŸ“Š {stock_code} ç»¼åˆåˆ†ææŠ¥å‘Š\n"
    result_text += f"ğŸ†” å·¥ä½œæµID: {workflow_result['workflow_id']}\n"
    result_text += f"ğŸ“… åˆ†ææ—¶é—´: {workflow_result['start_time'][:19]}\n"
    result_text += f"â±ï¸ æ‰§è¡Œè€—æ—¶: {workflow_result.get('duration', 'æœªçŸ¥')}\n"
    result_text += f"ğŸ“ˆ åˆ†ææ·±åº¦: {analysis_type}\n\n"

    # å·¥ä½œæµæ‰§è¡Œæ‘˜è¦
    summary = workflow_result.get("final_summary", {})
    if summary:
        result_text += f"### ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n"
        result_text += f"â€¢ æ€»æ­¥éª¤: {summary['total_steps']}\n"
        result_text += f"â€¢ æˆåŠŸæ­¥éª¤: {summary['successful_steps']}\n"
        result_text += f"â€¢ æˆåŠŸç‡: {summary['success_rate']:.1f}%\n"
        result_text += f"â€¢ çŠ¶æ€: {summary['status']}\n\n"

        # æˆåŠŸå®Œæˆçš„ä»»åŠ¡
        if summary['completed_tasks']:
            result_text += f"âœ… **å®Œæˆçš„ä»»åŠ¡**:\n"
            for task in summary['completed_tasks']:
                result_text += f"  â€¢ {task}\n"
            result_text += "\n"

        # å¤±è´¥çš„ä»»åŠ¡
        if summary['failed_tasks']:
            result_text += f"âŒ **å¤±è´¥çš„ä»»åŠ¡**:\n"
            for task in summary['failed_tasks']:
                result_text += f"  â€¢ {task}\n"
            result_text += "\n"

    # è¯¦ç»†æ­¥éª¤ç»“æœ
    result_text += f"### ğŸ“ è¯¦ç»†åˆ†æç»“æœ\n"
    for step in workflow_result["steps"]:
        status_icon = "âœ…" if step["success"] else "âŒ"
        result_text += f"{status_icon} **{step['name']}** (Step {step['step']})\n"

        if step["success"] and step.get("data"):
            # ç®€åŒ–æ˜¾ç¤ºæ•°æ®å†…å®¹
            data_preview = str(step["data"])[:200] + "..." if len(str(step["data"])) > 200 else str(step["data"])
            result_text += f"   ç»“æœ: {data_preview}\n"
        elif not step["success"]:
            result_text += f"   å¤±è´¥åŸå› : æœåŠ¡è°ƒç”¨å¤±è´¥\n"

        result_text += f"   æ—¶é—´: {step['timestamp'][:19]}\n\n"

    # ç»¼åˆå»ºè®®
    result_text += f"### ğŸ’¡ æŠ•èµ„å»ºè®®\n"
    if summary.get("success_rate", 0) >= 80:
        result_text += f"âœ… æ•°æ®è·å–å®Œæ•´ï¼Œå¯ä¿¡åº¦é«˜ï¼Œå»ºè®®åŸºäºä»¥ä¸Šåˆ†æè¿›è¡ŒæŠ•èµ„å†³ç­–ã€‚\n"
    elif summary.get("success_rate", 0) >= 50:
        result_text += f"âš ï¸ éƒ¨åˆ†æ•°æ®è·å–æˆåŠŸï¼Œå»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯æºç»¼åˆåˆ¤æ–­ã€‚\n"
    else:
        result_text += f"âŒ æ•°æ®è·å–ä¸è¶³ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–æ£€æŸ¥æœåŠ¡çŠ¶æ€ã€‚\n"

    result_text += f"\nğŸ“‹ å¦‚éœ€æŸ¥çœ‹è¯¦ç»†å·¥ä½œæµçŠ¶æ€ï¼Œè¯·ä½¿ç”¨å·¥ä½œæµID: {workflow_result['workflow_id']}"

    log_mcp_call(logger, "comprehensive_stock_analysis", arguments, {
        "workflow_id": workflow_result['workflow_id'],
        "success_rate": summary.get("success_rate", 0)
    })

    return [TextContent(type="text", text=result_text)]

async def _handle_cross_service_query(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†è·¨æœåŠ¡æŸ¥è¯¢"""

    query_plan = arguments.get("query_plan", [])
    parallel_execution = arguments.get("parallel_execution", True)

    if not query_plan:
        return [TextContent(type="text", text="âŒ æŸ¥è¯¢è®¡åˆ’ä¸èƒ½ä¸ºç©º")]

    result_text = f"ğŸ” è·¨æœåŠ¡æŸ¥è¯¢ç»“æœ\n"
    result_text += f"ğŸ“‹ æŸ¥è¯¢æ•°é‡: {len(query_plan)}\n"
    result_text += f"âš¡ æ‰§è¡Œæ–¹å¼: {'å¹¶è¡Œ' if parallel_execution else 'ä¸²è¡Œ'}\n"
    result_text += f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    results = []

    if parallel_execution:
        # å¹¶è¡Œæ‰§è¡Œ
        tasks = []
        for i, query in enumerate(query_plan):
            task = task_orchestrator.call_mcp_service(
                query["service"],
                query["tool"],
                query["arguments"]
            )
            tasks.append((i, query, task))

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for i, query, task in tasks:
            try:
                result = await task
                results.append({
                    "index": i + 1,
                    "service": query["service"],
                    "tool": query["tool"],
                    "success": result["success"],
                    "result": result
                })
            except Exception as e:
                results.append({
                    "index": i + 1,
                    "service": query["service"],
                    "tool": query["tool"],
                    "success": False,
                    "error": str(e)
                })
    else:
        # ä¸²è¡Œæ‰§è¡Œ
        for i, query in enumerate(query_plan):
            try:
                result = await task_orchestrator.call_mcp_service(
                    query["service"],
                    query["tool"],
                    query["arguments"]
                )
                results.append({
                    "index": i + 1,
                    "service": query["service"],
                    "tool": query["tool"],
                    "success": result["success"],
                    "result": result
                })
            except Exception as e:
                results.append({
                    "index": i + 1,
                    "service": query["service"],
                    "tool": query["tool"],
                    "success": False,
                    "error": str(e)
                })

    # æ ¼å¼åŒ–ç»“æœ
    successful_queries = [r for r in results if r["success"]]
    failed_queries = [r for r in results if not r["success"]]

    result_text += f"### ğŸ“Š æ‰§è¡Œç»Ÿè®¡\n"
    result_text += f"â€¢ æˆåŠŸæŸ¥è¯¢: {len(successful_queries)}/{len(results)}\n"
    result_text += f"â€¢ æˆåŠŸç‡: {len(successful_queries)/len(results)*100:.1f}%\n\n"

    # æˆåŠŸçš„æŸ¥è¯¢
    if successful_queries:
        result_text += f"### âœ… æˆåŠŸæŸ¥è¯¢ç»“æœ\n"
        for r in successful_queries:
            result_text += f"**{r['index']}. {r['service']}.{r['tool']}**\n"
            data_preview = str(r["result"].get("data", ""))[:150] + "..." if len(str(r["result"].get("data", ""))) > 150 else str(r["result"].get("data", ""))
            result_text += f"   ç»“æœ: {data_preview}\n\n"

    # å¤±è´¥çš„æŸ¥è¯¢
    if failed_queries:
        result_text += f"### âŒ å¤±è´¥æŸ¥è¯¢\n"
        for r in failed_queries:
            result_text += f"**{r['index']}. {r['service']}.{r['tool']}**\n"
            result_text += f"   é”™è¯¯: {r.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n"

    log_mcp_call(logger, "cross_service_query", arguments, {
        "total_queries": len(results),
        "successful_queries": len(successful_queries)
    })

    return [TextContent(type="text", text=result_text)]

async def _handle_workflow_status(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†å·¥ä½œæµçŠ¶æ€æŸ¥è¯¢"""

    workflow_id = arguments.get("workflow_id")
    limit = arguments.get("limit", 10)

    if workflow_id:
        # æŸ¥è¯¢ç‰¹å®šå·¥ä½œæµ
        workflow = None
        for w in task_orchestrator.task_history:
            if w["workflow_id"] == workflow_id:
                workflow = w
                break

        if not workflow:
            return [TextContent(type="text", text=f"âŒ æœªæ‰¾åˆ°å·¥ä½œæµID: {workflow_id}")]

        result_text = f"ğŸ” å·¥ä½œæµè¯¦æƒ…\n"
        result_text += f"ğŸ†” å·¥ä½œæµID: {workflow['workflow_id']}\n"
        result_text += f"ğŸ“Š è‚¡ç¥¨ä»£ç : {workflow['stock_code']}\n"
        result_text += f"ğŸ“ˆ åˆ†æç±»å‹: {workflow['analysis_type']}\n"
        result_text += f"ğŸ• å¼€å§‹æ—¶é—´: {workflow['start_time'][:19]}\n"
        result_text += f"ğŸ• ç»“æŸæ—¶é—´: {workflow.get('end_time', 'è¿›è¡Œä¸­')[:19] if workflow.get('end_time') else 'è¿›è¡Œä¸­'}\n"
        result_text += f"â±ï¸ æ‰§è¡Œæ—¶é—´: {workflow.get('duration', 'æœªçŸ¥')}\n\n"

        # æ­¥éª¤è¯¦æƒ…
        result_text += f"### ğŸ“‹ æ‰§è¡Œæ­¥éª¤è¯¦æƒ…\n"
        for step in workflow["steps"]:
            status_icon = "âœ…" if step["success"] else "âŒ"
            result_text += f"{status_icon} **Step {step['step']}: {step['name']}**\n"
            result_text += f"   æ‰§è¡Œæ—¶é—´: {step['timestamp'][:19]}\n"

            if step["success"]:
                result_text += f"   çŠ¶æ€: æˆåŠŸ\n"
            else:
                result_text += f"   çŠ¶æ€: å¤±è´¥\n"
            result_text += "\n"

        # æœ€ç»ˆæ‘˜è¦
        summary = workflow.get("final_summary", {})
        if summary:
            result_text += f"### ğŸ“Š æ‰§è¡Œæ‘˜è¦\n"
            result_text += f"â€¢ æ€»æ­¥éª¤: {summary['total_steps']}\n"
            result_text += f"â€¢ æˆåŠŸæ­¥éª¤: {summary['successful_steps']}\n"
            result_text += f"â€¢ æˆåŠŸç‡: {summary['success_rate']:.1f}%\n"
            result_text += f"â€¢ æœ€ç»ˆçŠ¶æ€: {summary['status']}\n"

    else:
        # æŸ¥è¯¢å·¥ä½œæµå†å²
        history = task_orchestrator.task_history[-limit:] if task_orchestrator.task_history else []

        result_text = f"ğŸ“Š å·¥ä½œæµå†å²è®°å½•\n"
        result_text += f"ğŸ“‹ æ˜¾ç¤ºæœ€è¿‘ {len(history)} æ¡è®°å½•\n\n"

        if not history:
            result_text += "æš‚æ— å·¥ä½œæµæ‰§è¡Œè®°å½•ã€‚\n"
        else:
            for workflow in reversed(history):
                summary = workflow.get("final_summary", {})
                status_icon = "âœ…" if summary.get("status") == "completed" else "âš ï¸" if summary.get("status") == "partial_failure" else "âŒ"

                result_text += f"{status_icon} **{workflow['workflow_id']}** - {workflow['stock_code']}\n"
                result_text += f"   ç±»å‹: {workflow['analysis_type']}\n"
                result_text += f"   æ—¶é—´: {workflow['start_time'][:19]}\n"
                result_text += f"   æ—¶é•¿: {workflow.get('duration', 'æœªçŸ¥')}\n"
                if summary:
                    result_text += f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%\n"
                result_text += "\n"

        # ç»Ÿè®¡ä¿¡æ¯
        if history:
            total_workflows = len(history)
            completed_workflows = len([w for w in history if w.get("final_summary", {}).get("status") == "completed"])
            avg_success_rate = sum([w.get("final_summary", {}).get("success_rate", 0) for w in history]) / len(history)

            result_text += f"### ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n"
            result_text += f"â€¢ æ€»å·¥ä½œæµ: {total_workflows}\n"
            result_text += f"â€¢ å®Œå…¨æˆåŠŸ: {completed_workflows}\n"
            result_text += f"â€¢ å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1f}%\n"

    log_mcp_call(logger, "workflow_status", arguments, {"query_type": "specific" if workflow_id else "history"})

    return [TextContent(type="text", text=result_text)]

async def _handle_service_health_check(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†æœåŠ¡å¥åº·æ£€æŸ¥"""

    detailed = arguments.get("detailed", False)

    # æ‰§è¡Œå¥åº·æ£€æŸ¥
    health_status = await task_orchestrator.health_check_all_services()

    result_text = f"ğŸ¥ MCPæœåŠ¡å¥åº·æ£€æŸ¥æŠ¥å‘Š\n"
    result_text += f"ğŸ• æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    result_text += f"ğŸ“Š æœåŠ¡æ€»æ•°: {len(health_status)}\n\n"

    # ç»Ÿè®¡å¥åº·çŠ¶æ€
    healthy_services = [s for s in health_status.values() if s["status"] == "healthy"]
    unhealthy_services = [s for s in health_status.values() if s["status"] != "healthy"]

    result_text += f"### ğŸ“Š æ€»ä½“çŠ¶æ€\n"
    result_text += f"â€¢ å¥åº·æœåŠ¡: {len(healthy_services)}/{len(health_status)}\n"
    result_text += f"â€¢ æœåŠ¡å¯ç”¨ç‡: {len(healthy_services)/len(health_status)*100:.1f}%\n\n"

    # è¯¦ç»†æœåŠ¡çŠ¶æ€
    result_text += f"### ğŸ“‹ æœåŠ¡è¯¦æƒ…\n"
    for service_name, status_info in health_status.items():
        status_icon = "âœ…" if status_info["status"] == "healthy" else "âŒ"

        result_text += f"{status_icon} **{service_name.replace('_', ' ').title()}**\n"
        result_text += f"   çŠ¶æ€: {status_info['status']}\n"

        if detailed:
            result_text += f"   åœ°å€: {status_info['url']}\n"
            if "response_code" in status_info:
                result_text += f"   å“åº”ç : {status_info['response_code']}\n"
            if "error" in status_info:
                result_text += f"   é”™è¯¯ä¿¡æ¯: {status_info['error']}\n"

        result_text += "\n"

    # å»ºè®®
    if len(unhealthy_services) > 0:
        result_text += f"### âš ï¸ æ³¨æ„äº‹é¡¹\n"
        result_text += f"å‘ç° {len(unhealthy_services)} ä¸ªæœåŠ¡ä¸å¥åº·ï¼Œå»ºè®®:\n"
        result_text += f"â€¢ æ£€æŸ¥ç›¸å…³æœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨\n"
        result_text += f"â€¢ éªŒè¯ç½‘ç»œè¿æ¥å’Œç«¯å£é…ç½®\n"
        result_text += f"â€¢ æŸ¥çœ‹æœåŠ¡æ—¥å¿—æ’æŸ¥é—®é¢˜\n"
    else:
        result_text += f"### ğŸ‰ ç³»ç»ŸçŠ¶æ€è‰¯å¥½\n"
        result_text += f"æ‰€æœ‰MCPæœåŠ¡è¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥æ‰§è¡Œå®Œæ•´çš„åˆ†æå·¥ä½œæµã€‚\n"

    log_mcp_call(logger, "service_health_check", arguments, {
        "total_services": len(health_status),
        "healthy_services": len(healthy_services)
    })

    return [TextContent(type="text", text=result_text)]

async def main():
    """ä¸»å‡½æ•° - å¯åŠ¨MCPæœåŠ¡å™¨"""
    logger.info("å¯åŠ¨ Task Coordination MCP Server (ç«¯å£8009)")

    # è¿è¡ŒMCPæœåŠ¡å™¨
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="coordination-mcp",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={}
                )
            )
        )

if __name__ == "__main__":
    asyncio.run(main())