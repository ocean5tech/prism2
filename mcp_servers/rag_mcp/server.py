#!/usr/bin/env python3
"""
RAG Enhanced MCP Server (Port 8008)

Provides intelligent document retrieval and context enhancement for financial analysis.
Integrates ChromaDB vector database and news data sources.
"""

import asyncio
import json
from typing import Any, Dict, List, Optional, Sequence, Union
from datetime import datetime, timedelta

# MCP SDK imports
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel,
    CallToolRequest,
    ListToolsRequest,
    ListResourcesRequest,
    ReadResourceRequest
)

# Shared modules imports
import sys
import os
sys.path.append('/home/wyatt/prism2/mcp_servers/shared')

from config import config
from logger import get_logger, log_mcp_call
from database import data_manager

logger = get_logger("RAG-MCP")

# Initialize MCP server
server = Server("rag-mcp")

class RAGService:
    """RAGæœåŠ¡æ ¸å¿ƒç±» - å‘é‡æ£€ç´¢å’Œä¸Šä¸‹æ–‡å¢å¼º"""

    def __init__(self):
        self.chroma_client = None
        self.collection = None
        self.embedding_model = "all-MiniLM-L6-v2"  # è½»é‡çº§åµŒå…¥æ¨¡å‹
        self._initialize_vector_db()

    def _initialize_vector_db(self):
        """åˆå§‹åŒ–ChromaDBå‘é‡æ•°æ®åº“"""
        try:
            import chromadb

            # ä½¿ç”¨æ–°çš„PersistentClient
            self.chroma_client = chromadb.PersistentClient(
                path="./chroma_data"
            )

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="financial_documents",
                metadata={"hnsw:space": "cosine"}
            )

            logger.info("ChromaDBå‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            self._populate_sample_data()

        except Exception as e:
            logger.error(f"ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            self.chroma_client = None
            self.collection = None

    def _populate_sample_data(self):
        """å¡«å……ç¤ºä¾‹æ•°æ®åˆ°å‘é‡æ•°æ®åº“"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
            if self.collection.count() > 0:
                logger.info(f"å‘é‡æ•°æ®åº“å·²æœ‰ {self.collection.count()} æ¡è®°å½•")
                return

            # æ·»åŠ ç¤ºä¾‹è´¢ç»æ–°é—»å’Œåˆ†ææ–‡æ¡£
            sample_documents = [
                {
                    "id": "news_001",
                    "content": "ç§‘åˆ›æ¿å…¬å¸688469æœ€æ–°è´¢æŠ¥æ˜¾ç¤ºè¥æ”¶åŒæ¯”å¢é•¿15%ï¼Œå‡€åˆ©æ¶¦å®ç°æ‰­äºä¸ºç›ˆï¼Œä¸»è¦å—ç›Šäºæ–°äº§å“å¸‚åœºè¡¨ç°å¼ºåŠ²",
                    "metadata": {"type": "news", "stock_code": "688469", "date": "2024-09-20", "source": "è´¢ç»æ—¥æŠ¥"},
                },
                {
                    "id": "analysis_001",
                    "content": "ä»æŠ€æœ¯åˆ†æè§’åº¦çœ‹ï¼Œ688469è‚¡ä»·çªç ´é‡è¦é˜»åŠ›ä½ï¼Œæˆäº¤é‡æ”¾å¤§ï¼Œæ˜¾ç¤ºå¤šå¤´åŠ›é‡å¢å¼ºï¼Œå»ºè®®å…³æ³¨å›è°ƒä¹°å…¥æœºä¼š",
                    "metadata": {"type": "analysis", "stock_code": "688469", "analyst": "æŠ€æœ¯åˆ†æå¸ˆ", "date": "2024-09-21"},
                },
                {
                    "id": "industry_001",
                    "content": "æ–°èƒ½æºè¡Œä¸šæ•´ä½“å‘å¥½ï¼Œæ”¿ç­–æ”¯æŒåŠ›åº¦åŠ å¤§ï¼Œç›¸å…³æ¦‚å¿µè‚¡æœ‰æœ›å—ç›Šï¼ŒæŠ•èµ„è€…å¯å…³æ³¨è¡Œä¸šé¾™å¤´ä¼ä¸š",
                    "metadata": {"type": "industry", "industry": "æ–°èƒ½æº", "date": "2024-09-19", "source": "è¡Œä¸šç ”ç©¶"},
                },
                {
                    "id": "market_001",
                    "content": "Aè‚¡å¸‚åœºæ•´ä½“è¡¨ç°å¹³ç¨³ï¼Œç§‘åˆ›æ¿æ´»è·ƒåº¦è¾ƒé«˜ï¼Œèµ„é‡‘æµå‘ç§‘æŠ€è‚¡ï¼Œå»ºè®®å…³æ³¨ä¸šç»©ç¡®å®šæ€§å¼ºçš„ä¼˜è´¨æ ‡çš„",
                    "metadata": {"type": "market", "market": "Aè‚¡", "date": "2024-09-22", "source": "å¸‚åœºåˆ†æ"},
                }
            ]

            # æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
            documents = [doc["content"] for doc in sample_documents]
            metadatas = [doc["metadata"] for doc in sample_documents]
            ids = [doc["id"] for doc in sample_documents]

            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

            logger.info(f"æˆåŠŸæ·»åŠ  {len(sample_documents)} æ¡ç¤ºä¾‹æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")

        except Exception as e:
            logger.error(f"å¡«å……ç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")

    async def search_relevant_documents(self, query: str, n_results: int = 5, stock_code: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            if not self.collection:
                return []

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = {}
            if stock_code:
                where_conditions["stock_code"] = stock_code

            # æ‰§è¡Œå‘é‡æœç´¢
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_conditions if where_conditions else None,
                include=["documents", "metadatas", "distances"]
            )

            # æ ¼å¼åŒ–ç»“æœ
            documents = []
            if results["documents"] and len(results["documents"]) > 0:
                for i, doc in enumerate(results["documents"][0]):
                    documents.append({
                        "content": doc,
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "similarity": 1 - results["distances"][0][i] if results["distances"] else 0
                    })

            logger.info(f"RAGæœç´¢æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
            return documents

        except Exception as e:
            logger.error(f"RAGæ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return []

    async def add_document(self, document_id: str, content: str, metadata: Dict[str, Any]) -> bool:
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            if not self.collection:
                return False

            self.collection.add(
                documents=[content],
                metadatas=[metadata],
                ids=[document_id]
            )

            logger.info(f"æˆåŠŸæ·»åŠ æ–‡æ¡£åˆ°RAGæ•°æ®åº“: {document_id}")
            return True

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False

# åˆå§‹åŒ–RAGæœåŠ¡
rag_service = RAGService()

@server.list_tools()
async def handle_list_tools() -> List[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="search_financial_context",
            description="æœç´¢ä¸è‚¡ç¥¨æˆ–è´¢ç»ä¸»é¢˜ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ã€‚å½“éœ€è¦äº†è§£å¸‚åœºè§‚ç‚¹ã€è¡Œä¸šåŠ¨æ€ã€åˆ†æå¸ˆè§‚ç‚¹æ—¶è°ƒç”¨ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¦‚'688469æœ€æ–°åŠ¨æ€'ã€'æ–°èƒ½æºè¡Œä¸šå‰æ™¯'ç­‰"
                    },
                    "stock_code": {
                        "type": "string",
                        "description": "å¯é€‰çš„è‚¡ç¥¨ä»£ç ï¼Œç”¨äºç­›é€‰ç‰¹å®šè‚¡ç¥¨ç›¸å…³çš„å†…å®¹"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª",
                        "default": 5
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="get_news_sentiment",
            description="è·å–ç‰¹å®šè‚¡ç¥¨çš„æ–°é—»æƒ…æ„Ÿåˆ†æã€‚åˆ†ææœ€è¿‘çš„æ–°é—»æŠ¥é“æƒ…æ„Ÿå€¾å‘ï¼Œä¸ºæŠ•èµ„å†³ç­–æä¾›å‚è€ƒã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "stock_code": {
                        "type": "string",
                        "description": "è‚¡ç¥¨ä»£ç ï¼Œå¦‚'688469'"
                    },
                    "days": {
                        "type": "integer",
                        "description": "åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»ï¼Œé»˜è®¤7å¤©",
                        "default": 7
                    }
                },
                "required": ["stock_code"]
            }
        ),
        Tool(
            name="generate_market_insight",
            description="åŸºäºRAGæ£€ç´¢ç»“æœç”Ÿæˆå¸‚åœºæ´å¯ŸæŠ¥å‘Šã€‚ç»“åˆå¤šä¸ªæ•°æ®æºæä¾›ç»¼åˆæ€§çš„å¸‚åœºåˆ†æå’ŒæŠ•èµ„å»ºè®®ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "åˆ†æä¸»é¢˜ï¼Œå¦‚'ç§‘åˆ›æ¿æŠ•èµ„æœºä¼š'ã€'æ–°èƒ½æºæ¿å—å±•æœ›'ç­‰"
                    },
                    "stock_codes": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ç›¸å…³è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¯é€‰"
                    },
                    "analysis_depth": {
                        "type": "string",
                        "enum": ["brief", "detailed", "comprehensive"],
                        "description": "åˆ†ææ·±åº¦ï¼šç®€è¦ã€è¯¦ç»†ã€å…¨é¢",
                        "default": "detailed"
                    }
                },
                "required": ["topic"]
            }
        ),
        Tool(
            name="add_knowledge_document",
            description="æ·»åŠ æ–°çš„çŸ¥è¯†æ–‡æ¡£åˆ°RAGç³»ç»Ÿã€‚ç”¨äºæ‰©å……çŸ¥è¯†åº“å†…å®¹ï¼Œæ”¯æŒæ–°é—»ã€ç ”æŠ¥ã€åˆ†ææ–‡ç« ç­‰ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "æ–‡æ¡£æ ‡é¢˜"
                    },
                    "content": {
                        "type": "string",
                        "description": "æ–‡æ¡£å†…å®¹"
                    },
                    "doc_type": {
                        "type": "string",
                        "enum": ["news", "analysis", "research", "industry", "market"],
                        "description": "æ–‡æ¡£ç±»å‹"
                    },
                    "metadata": {
                        "type": "object",
                        "description": "æ–‡æ¡£å…ƒæ•°æ®ï¼Œå¦‚è‚¡ç¥¨ä»£ç ã€æ—¥æœŸã€æ¥æºç­‰",
                        "properties": {
                            "stock_code": {"type": "string"},
                            "date": {"type": "string"},
                            "source": {"type": "string"},
                            "industry": {"type": "string"}
                        }
                    }
                },
                "required": ["title", "content", "doc_type"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚"""

    log_mcp_call(logger, name, arguments)

    try:
        if name == "search_financial_context":
            return await _handle_search_financial_context(arguments)
        elif name == "get_news_sentiment":
            return await _handle_get_news_sentiment(arguments)
        elif name == "generate_market_insight":
            return await _handle_generate_market_insight(arguments)
        elif name == "add_knowledge_document":
            return await _handle_add_knowledge_document(arguments)
        else:
            error_msg = f"æœªçŸ¥çš„å·¥å…·: {name}"
            log_mcp_call(logger, name, arguments, error=error_msg)
            return [TextContent(type="text", text=f"âŒ é”™è¯¯: {error_msg}")]

    except Exception as e:
        error_msg = f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {str(e)}"
        log_mcp_call(logger, name, arguments, error=error_msg)
        return [TextContent(type="text", text=f"âŒ é”™è¯¯: {error_msg}")]

async def _handle_search_financial_context(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†è´¢ç»ä¸Šä¸‹æ–‡æœç´¢"""

    query = arguments.get("query", "")
    stock_code = arguments.get("stock_code")
    max_results = arguments.get("max_results", 5)

    if not query.strip():
        return [TextContent(type="text", text="âŒ æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")]

    # æ‰§è¡ŒRAGæœç´¢
    documents = await rag_service.search_relevant_documents(
        query=query,
        n_results=max_results,
        stock_code=stock_code
    )

    if not documents:
        return [TextContent(type="text", text=f"ğŸ” æœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯")]

    # æ ¼å¼åŒ–æœç´¢ç»“æœ
    result_text = f"ğŸ” è´¢ç»èƒŒæ™¯æœç´¢: '{query}'\n"
    if stock_code:
        result_text += f"ğŸ“Š è‚¡ç¥¨ç­›é€‰: {stock_code}\n"
    result_text += f"ğŸ“ æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³ç»“æœ:\n\n"

    for i, doc in enumerate(documents, 1):
        metadata = doc.get("metadata", {})
        similarity = doc.get("similarity", 0)

        result_text += f"### ğŸ“„ ç»“æœ {i} (ç›¸å…³åº¦: {similarity:.3f})\n"
        result_text += f"**å†…å®¹**: {doc['content']}\n"

        if metadata.get("type"):
            result_text += f"**ç±»å‹**: {metadata['type']}\n"
        if metadata.get("date"):
            result_text += f"**æ—¥æœŸ**: {metadata['date']}\n"
        if metadata.get("source"):
            result_text += f"**æ¥æº**: {metadata['source']}\n"
        if metadata.get("stock_code"):
            result_text += f"**ç›¸å…³è‚¡ç¥¨**: {metadata['stock_code']}\n"

        result_text += "\n---\n\n"

    result_text += f"ğŸ’¡ **ä½¿ç”¨å»ºè®®**: ä»¥ä¸Šä¿¡æ¯å¯ä¸ºæŠ•èµ„åˆ†ææä¾›èƒŒæ™¯æ”¯æŒï¼Œè¯·ç»“åˆå®æ—¶æ•°æ®ç»¼åˆåˆ¤æ–­ã€‚"

    log_mcp_call(logger, "search_financial_context", arguments, {"results": len(documents)})
    return [TextContent(type="text", text=result_text)]

async def _handle_get_news_sentiment(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†æ–°é—»æƒ…æ„Ÿåˆ†æ"""

    stock_code = arguments.get("stock_code", "")
    days = arguments.get("days", 7)

    if not stock_code:
        return [TextContent(type="text", text="âŒ è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")]

    # æœç´¢ç›¸å…³æ–°é—»
    news_query = f"{stock_code} æ–°é—» è´¢æŠ¥ ä¸šç»©"
    documents = await rag_service.search_relevant_documents(
        query=news_query,
        n_results=10,
        stock_code=stock_code
    )

    # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æï¼ˆå®é™…åº”ç”¨ä¸­å¯é›†æˆä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æAPIï¼‰
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    sentiment_details = []

    for doc in documents:
        content = doc.get("content", "")
        metadata = doc.get("metadata", {})

        # ç®€å•çš„æƒ…æ„Ÿåˆ†æé€»è¾‘
        positive_words = ["å¢é•¿", "ç›ˆåˆ©", "çªç ´", "å¼ºåŠ²", "åˆ©å¥½", "ä¸Šæ¶¨", "ä¹°å…¥", "æ¨è"]
        negative_words = ["ä¸‹è·Œ", "äºæŸ", "ä¸‹æ»‘", "é£é™©", "è°ƒæ•´", "å–å‡º", "è°¨æ…", "å‹åŠ›"]

        pos_score = sum(1 for word in positive_words if word in content)
        neg_score = sum(1 for word in negative_words if word in content)

        if pos_score > neg_score:
            sentiment = "ç§¯æ"
            positive_count += 1
        elif neg_score > pos_score:
            sentiment = "æ¶ˆæ"
            negative_count += 1
        else:
            sentiment = "ä¸­æ€§"
            neutral_count += 1

        sentiment_details.append({
            "content": content[:100] + "..." if len(content) > 100 else content,
            "sentiment": sentiment,
            "date": metadata.get("date", "æœªçŸ¥"),
            "source": metadata.get("source", "æœªçŸ¥")
        })

    # è®¡ç®—æ€»ä½“æƒ…æ„Ÿ
    total = len(documents)
    if total == 0:
        return [TextContent(type="text", text=f"ğŸ“° {stock_code} æš‚æ— ç›¸å…³æ–°é—»æ•°æ®")]

    positive_pct = (positive_count / total) * 100
    negative_pct = (negative_count / total) * 100
    neutral_pct = (neutral_count / total) * 100

    # ç”Ÿæˆæƒ…æ„Ÿåˆ†ææŠ¥å‘Š
    result_text = f"ğŸ“° {stock_code} æ–°é—»æƒ…æ„Ÿåˆ†ææŠ¥å‘Š\n"
    result_text += f"ğŸ—“ï¸ åˆ†ææœŸé—´: æœ€è¿‘{days}å¤©\n"
    result_text += f"ğŸ“Š åˆ†ææ ·æœ¬: {total}æ¡ç›¸å…³ä¿¡æ¯\n\n"

    result_text += f"### ğŸ“ˆ æ€»ä½“æƒ…æ„Ÿå€¾å‘\n"
    result_text += f"ğŸŸ¢ ç§¯æ: {positive_count}æ¡ ({positive_pct:.1f}%)\n"
    result_text += f"ğŸ”´ æ¶ˆæ: {negative_count}æ¡ ({negative_pct:.1f}%)\n"
    result_text += f"ğŸŸ¡ ä¸­æ€§: {neutral_count}æ¡ ({neutral_pct:.1f}%)\n\n"

    # åˆ¤æ–­æ€»ä½“æƒ…æ„Ÿ
    if positive_pct > 50:
        overall_sentiment = "æ€»ä½“ç§¯æ ğŸ“ˆ"
    elif negative_pct > 50:
        overall_sentiment = "æ€»ä½“æ¶ˆæ ğŸ“‰"
    else:
        overall_sentiment = "æ€»ä½“ä¸­æ€§ â¡ï¸"

    result_text += f"**å¸‚åœºæƒ…æ„Ÿ**: {overall_sentiment}\n\n"

    # è¯¦ç»†æƒ…æ„Ÿåˆ†æ
    result_text += f"### ğŸ“ ä¸»è¦ä¿¡æ¯æ‘˜è¦\n"
    for i, item in enumerate(sentiment_details[:5], 1):
        emoji = "ğŸŸ¢" if item["sentiment"] == "ç§¯æ" else "ğŸ”´" if item["sentiment"] == "æ¶ˆæ" else "ğŸŸ¡"
        result_text += f"{i}. {emoji} **{item['sentiment']}** | {item['date']} | {item['source']}\n"
        result_text += f"   {item['content']}\n\n"

    result_text += f"ğŸ’¡ **æŠ•èµ„å‚è€ƒ**: æƒ…æ„Ÿåˆ†æä»…ä¾›å‚è€ƒï¼Œè¯·ç»“åˆåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢ç»¼åˆåˆ¤æ–­ã€‚"

    log_mcp_call(logger, "get_news_sentiment", arguments, {
        "total_news": total,
        "positive_pct": positive_pct,
        "negative_pct": negative_pct
    })

    return [TextContent(type="text", text=result_text)]

async def _handle_generate_market_insight(arguments: dict) -> Sequence[TextContent]:
    """ç”Ÿæˆå¸‚åœºæ´å¯ŸæŠ¥å‘Š"""

    topic = arguments.get("topic", "")
    stock_codes = arguments.get("stock_codes", [])
    analysis_depth = arguments.get("analysis_depth", "detailed")

    if not topic.strip():
        return [TextContent(type="text", text="âŒ åˆ†æä¸»é¢˜ä¸èƒ½ä¸ºç©º")]

    # åŸºäºä¸»é¢˜æœç´¢ç›¸å…³æ–‡æ¡£
    documents = await rag_service.search_relevant_documents(
        query=topic,
        n_results=10
    )

    # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œä¹Ÿæœç´¢ç›¸å…³ä¿¡æ¯
    stock_documents = []
    if stock_codes:
        for stock_code in stock_codes:
            stock_docs = await rag_service.search_relevant_documents(
                query=f"{stock_code} åˆ†æ æŠ•èµ„",
                n_results=5,
                stock_code=stock_code
            )
            stock_documents.extend(stock_docs)

    # åˆå¹¶æ‰€æœ‰ç›¸å…³æ–‡æ¡£
    all_documents = documents + stock_documents

    if not all_documents:
        return [TextContent(type="text", text=f"ğŸ“Š æœªæ‰¾åˆ°ä¸'{topic}'ç›¸å…³çš„å¸‚åœºä¿¡æ¯")]

    # ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
    result_text = f"ğŸ“Š å¸‚åœºæ´å¯ŸæŠ¥å‘Š: {topic}\n"
    result_text += f"ğŸ—“ï¸ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
    result_text += f"ğŸ“ åˆ†ææ·±åº¦: {analysis_depth}\n"
    if stock_codes:
        result_text += f"ğŸ¯ å…³æ³¨è‚¡ç¥¨: {', '.join(stock_codes)}\n"
    result_text += f"ğŸ“š æ•°æ®æ¥æº: {len(all_documents)}æ¡ç›¸å…³ä¿¡æ¯\n\n"

    # åˆ†æä¸åŒç±»å‹çš„ä¿¡æ¯
    news_docs = [d for d in all_documents if d.get("metadata", {}).get("type") == "news"]
    analysis_docs = [d for d in all_documents if d.get("metadata", {}).get("type") == "analysis"]
    industry_docs = [d for d in all_documents if d.get("metadata", {}).get("type") == "industry"]
    market_docs = [d for d in all_documents if d.get("metadata", {}).get("type") == "market"]

    if analysis_depth in ["detailed", "comprehensive"]:
        # è¯¦ç»†åˆ†æ
        if news_docs:
            result_text += f"### ğŸ“° æ–°é—»åŠ¨æ€ ({len(news_docs)}æ¡)\n"
            for doc in news_docs[:3]:
                result_text += f"â€¢ {doc['content'][:80]}...\n"
            result_text += "\n"

        if analysis_docs:
            result_text += f"### ğŸ“ˆ åˆ†æè§‚ç‚¹ ({len(analysis_docs)}æ¡)\n"
            for doc in analysis_docs[:3]:
                result_text += f"â€¢ {doc['content'][:80]}...\n"
            result_text += "\n"

        if industry_docs:
            result_text += f"### ğŸ­ è¡Œä¸šå‰æ™¯ ({len(industry_docs)}æ¡)\n"
            for doc in industry_docs[:2]:
                result_text += f"â€¢ {doc['content'][:80]}...\n"
            result_text += "\n"

        if market_docs:
            result_text += f"### ğŸŒ å¸‚åœºç¯å¢ƒ ({len(market_docs)}æ¡)\n"
            for doc in market_docs[:2]:
                result_text += f"â€¢ {doc['content'][:80]}...\n"
            result_text += "\n"

    # ç”Ÿæˆç»¼åˆæ´å¯Ÿ
    result_text += f"### ğŸ’¡ ç»¼åˆæ´å¯Ÿ\n"

    # ç®€å•çš„å…³é”®è¯åˆ†æ
    all_content = " ".join([doc.get("content", "") for doc in all_documents])
    positive_indicators = ["å¢é•¿", "åˆ©å¥½", "æœºä¼š", "çªç ´", "å¼ºåŠ²", "ä¸Šæ¶¨"]
    negative_indicators = ["ä¸‹è·Œ", "é£é™©", "å‹åŠ›", "è°ƒæ•´", "è°¨æ…", "æŒ‘æˆ˜"]

    positive_signals = sum(1 for indicator in positive_indicators if indicator in all_content)
    negative_signals = sum(1 for indicator in negative_indicators if indicator in all_content)

    if positive_signals > negative_signals:
        market_outlook = "åå‘ä¹è§‚ ğŸ“ˆ"
        suggestion = "å¯å…³æ³¨ç›¸å…³æŠ•èµ„æœºä¼šï¼Œä½†éœ€æ³¨æ„é£é™©æ§åˆ¶"
    elif negative_signals > positive_signals:
        market_outlook = "åå‘è°¨æ… ğŸ“‰"
        suggestion = "å»ºè®®ä¿æŒè°¨æ…ï¼Œç­‰å¾…æ›´æ˜ç¡®çš„ä¿¡å·"
    else:
        market_outlook = "ä¸­æ€§è§‚ç‚¹ â¡ï¸"
        suggestion = "å»ºè®®æŒç»­å…³æ³¨å¸‚åœºå˜åŒ–ï¼Œçµæ´»åº”å¯¹"

    result_text += f"**å¸‚åœºå±•æœ›**: {market_outlook}\n"
    result_text += f"**æŠ•èµ„å»ºè®®**: {suggestion}\n\n"

    if analysis_depth == "comprehensive":
        # å…¨é¢åˆ†æ - æ·»åŠ æ›´å¤šç»†èŠ‚
        result_text += f"### ğŸ“Š æ•°æ®ç»Ÿè®¡\n"
        result_text += f"â€¢ ç§¯æä¿¡å·æ•°é‡: {positive_signals}\n"
        result_text += f"â€¢ æ¶ˆæä¿¡å·æ•°é‡: {negative_signals}\n"
        result_text += f"â€¢ ä¿¡æ¯è¦†ç›–èŒƒå›´: æ–°é—»({len(news_docs)})ã€åˆ†æ({len(analysis_docs)})ã€è¡Œä¸š({len(industry_docs)})ã€å¸‚åœº({len(market_docs)})\n\n"

    result_text += f"âš ï¸ **å…è´£å£°æ˜**: æœ¬æŠ¥å‘ŠåŸºäºRAGæ£€ç´¢çš„ä¿¡æ¯ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼ŒæŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…ã€‚"

    log_mcp_call(logger, "generate_market_insight", arguments, {
        "documents_analyzed": len(all_documents),
        "market_outlook": market_outlook
    })

    return [TextContent(type="text", text=result_text)]

async def _handle_add_knowledge_document(arguments: dict) -> Sequence[TextContent]:
    """æ·»åŠ çŸ¥è¯†æ–‡æ¡£åˆ°RAGç³»ç»Ÿ"""

    title = arguments.get("title", "")
    content = arguments.get("content", "")
    doc_type = arguments.get("doc_type", "")
    metadata = arguments.get("metadata", {})

    if not all([title, content, doc_type]):
        return [TextContent(type="text", text="âŒ æ ‡é¢˜ã€å†…å®¹å’Œæ–‡æ¡£ç±»å‹éƒ½ä¸èƒ½ä¸ºç©º")]

    # ç”Ÿæˆæ–‡æ¡£ID
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    document_id = f"{doc_type}_{timestamp}"

    # å‡†å¤‡å…ƒæ•°æ®
    doc_metadata = {
        "title": title,
        "type": doc_type,
        "created_at": datetime.now().isoformat(),
        **metadata
    }

    # æ·»åŠ åˆ°RAGç³»ç»Ÿ
    success = await rag_service.add_document(
        document_id=document_id,
        content=content,
        metadata=doc_metadata
    )

    if success:
        result_text = f"âœ… æˆåŠŸæ·»åŠ çŸ¥è¯†æ–‡æ¡£åˆ°RAGç³»ç»Ÿ\n\n"
        result_text += f"ğŸ“„ **æ–‡æ¡£æ ‡é¢˜**: {title}\n"
        result_text += f"ğŸ·ï¸ **æ–‡æ¡£ç±»å‹**: {doc_type}\n"
        result_text += f"ğŸ†” **æ–‡æ¡£ID**: {document_id}\n"
        result_text += f"ğŸ“ **å†…å®¹é•¿åº¦**: {len(content)} å­—ç¬¦\n"

        if metadata:
            result_text += f"ğŸ“‹ **é™„åŠ ä¿¡æ¯**:\n"
            for key, value in metadata.items():
                result_text += f"   â€¢ {key}: {value}\n"

        result_text += f"\nğŸ’¡ æ–‡æ¡£å·²ç´¢å¼•ï¼Œå¯é€šè¿‡æœç´¢åŠŸèƒ½æ£€ç´¢ä½¿ç”¨ã€‚"

        log_mcp_call(logger, "add_knowledge_document", arguments, {"document_id": document_id})
    else:
        result_text = f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€æˆ–ç¨åé‡è¯•ã€‚"
        log_mcp_call(logger, "add_knowledge_document", arguments, error="æ·»åŠ æ–‡æ¡£å¤±è´¥")

    return [TextContent(type="text", text=result_text)]

async def main():
    """ä¸»å‡½æ•° - å¯åŠ¨MCPæœåŠ¡å™¨"""
    logger.info("å¯åŠ¨ RAG Enhanced MCP Server (ç«¯å£8008)")

    # è¿è¡ŒMCPæœåŠ¡å™¨
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="rag-mcp",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={}
                )
            )
        )

if __name__ == "__main__":
    asyncio.run(main())