#!/usr/bin/env python3
"""
å¤§è§„æ¨¡AKShareæ•°æ®æ”¶é›†å™¨
ä¸“æ³¨äºAè‚¡è¡Œä¸šæ•°æ®æ”¶é›†ï¼šè®¡ç®—æœºã€èŠ¯ç‰‡ã€æœ‰è‰²é‡‘å±ç­‰
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

class LargeScaleAKShareCollector:
    """å¤§è§„æ¨¡AKShareæ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        self.vectorizer = None
        self.client = None
        self.collection = None

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–å¤§è§„æ¨¡AKShareæ•°æ®æ”¶é›†ç³»ç»Ÿ...")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        print("ğŸ“¥ åˆå§‹åŒ–TF-IDFä¸­æ–‡å‘é‡åŒ–å™¨...")
        self.vectorizer = TfidfVectorizer(
            max_features=768,
            analyzer='word',
            tokenizer=lambda x: jieba.lcut(x),
            token_pattern=None,
            lowercase=False,
            stop_words=None
        )

        # è¿æ¥åˆ°ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        self.client = chromadb.HttpClient(host='localhost', port=8000)
        self.client.heartbeat()

        try:
            self.client.delete_collection("financial_documents")
            print("   ğŸ—‘ï¸ åˆ é™¤ç°æœ‰é›†åˆ")
        except:
            pass

        self.collection = self.client.create_collection("financial_documents")
        print("âœ… ChromaDBè¿æ¥æˆåŠŸï¼Œåˆ›å»ºæ–°é›†åˆ")

        print("âœ… å¤§è§„æ¨¡æ•°æ®æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

    async def collect_sector_stocks_comprehensive(self) -> List[Dict]:
        """å…¨é¢æ”¶é›†Aè‚¡è¡Œä¸šè‚¡ç¥¨æ•°æ®"""
        print("ğŸ“Š å¼€å§‹å…¨é¢æ”¶é›†Aè‚¡è¡Œä¸šè‚¡ç¥¨æ•°æ®...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # æ‰©å¤§çš„è‚¡ç¥¨æ± 
            sectors = {
                'è®¡ç®—æœº': {
                    'stocks': ['002230', '000938', '002415', '300014', '000977', '600570', '300496', '002353'],
                    'name': 'è®¡ç®—æœºè½¯ä»¶'
                },
                'èŠ¯ç‰‡åŠå¯¼ä½“': {
                    'stocks': ['000725', '002049', '002938', '688981', '688012', '002371', '300661'],
                    'name': 'åŠå¯¼ä½“èŠ¯ç‰‡'
                },
                'æœ‰è‰²é‡‘å±': {
                    'stocks': ['000831', '002460', '600362', '000878', '600219', '600111', '000629'],
                    'name': 'æœ‰è‰²é‡‘å±'
                },
                'æ–°èƒ½æº': {
                    'stocks': ['300750', '002594', '300274', '300450', '002129', '600884'],
                    'name': 'æ–°èƒ½æºæ±½è½¦'
                },
                'äººå·¥æ™ºèƒ½': {
                    'stocks': ['300229', '002253', '000063', '002410', '300243', '300383'],
                    'name': 'äººå·¥æ™ºèƒ½'
                }
            }

            for sector_key, sector_info in sectors.items():
                print(f"   æ”¶é›†{sector_info['name']}è¡Œä¸šè‚¡ç¥¨æ•°æ®...")

                for stock_code in sector_info['stocks']:
                    try:
                        # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                        stock_info = ak.stock_zh_a_hist(
                            symbol=stock_code,
                            period="daily",
                            start_date="20241001",
                            end_date="20241101"
                        )

                        if not stock_info.empty:
                            latest_data = stock_info.iloc[-1]

                            # è®¡ç®—æ¶¨è·Œå¹…
                            if len(stock_info) > 1:
                                prev_close = stock_info.iloc[-2]['æ”¶ç›˜']
                                change_pct = ((latest_data['æ”¶ç›˜'] - prev_close) / prev_close) * 100
                            else:
                                change_pct = 0

                            # è·å–è‚¡ç¥¨åç§°
                            try:
                                stock_name_df = ak.stock_zh_a_spot_em()
                                stock_name = stock_name_df[stock_name_df['ä»£ç '] == stock_code]['åç§°'].iloc[0]
                            except:
                                stock_name = f"è‚¡ç¥¨{stock_code}"

                            content = f"{sector_info['name']}é¾™å¤´è‚¡ç¥¨{stock_name}({stock_code})æœ€æ–°äº¤æ˜“åˆ†æï¼šæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}å…ƒï¼Œæ¶¨è·Œå¹…{change_pct:.2f}%ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}è‚¡ï¼Œæˆäº¤é¢{latest_data.get('æˆäº¤é¢', 'N/A')}å…ƒã€‚è¯¥è‚¡å±äº{sector_info['name']}æ¿å—ï¼Œä¸º{sector_key}é¢†åŸŸé‡è¦æ ‡çš„ï¼Œå…·æœ‰è¾ƒé«˜æŠ•èµ„ä»·å€¼å’Œå¸‚åœºå…³æ³¨åº¦ã€‚"

                            doc = {
                                'id': f"stock_{sector_key}_{stock_code}_{int(time.time())}",
                                'content': content,
                                'metadata': {
                                    'source': f'AKShare-{sector_info["name"]}',
                                    'doc_type': 'sector_stock_analysis',
                                    'sector': sector_key,
                                    'sector_name': sector_info['name'],
                                    'stock_code': stock_code,
                                    'company_name': stock_name,
                                    'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                                    'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                                    'amount': float(latest_data.get('æˆäº¤é¢', 0)),
                                    'change_pct': round(change_pct, 2),
                                    'date': str(latest_data.name),
                                    'data_source': 'akshare_comprehensive_stocks',
                                    'importance': 8,
                                    'collection_time': datetime.now().isoformat()
                                }
                            }
                            collected_docs.append(doc)
                            print(f"   âœ… {sector_info['name']}-{stock_name}({stock_code}) æ•°æ®æ”¶é›†å®Œæˆ")

                            # æ·»åŠ å»¶æ—¶é¿å…é¢‘ç‡é™åˆ¶
                            await asyncio.sleep(0.5)

                    except Exception as e:
                        print(f"   âš ï¸ è·å–è‚¡ç¥¨{stock_code}æ•°æ®å¤±è´¥: {e}")

            # æ”¶é›†è¡Œä¸šETFæ•°æ®
            print("   æ”¶é›†è¡Œä¸šETFæ•°æ®...")
            etf_codes = {
                '515050': '5Gé€šä¿¡ETF',
                '159995': 'èŠ¯ç‰‡ETF',
                '515000': 'ç§‘æŠ€ETF',
                '516970': 'è®¡ç®—æœºETF',
                '159928': 'ä¸­è¯æ¶ˆè´¹ETF'
            }

            for etf_code, etf_name in etf_codes.items():
                try:
                    etf_info = ak.fund_etf_hist_em(symbol=etf_code, period="daily", start_date="20241001", end_date="20241101")
                    if not etf_info.empty:
                        latest_etf = etf_info.iloc[-1]

                        content = f"{etf_name}({etf_code})æœ€æ–°äº¤æ˜“æ•°æ®ï¼šå‡€å€¼{latest_etf.get('æ”¶ç›˜', 'N/A')}ï¼Œæˆäº¤é‡{latest_etf.get('æˆäº¤é‡', 'N/A')}ã€‚è¯¥ETFè·Ÿè¸ªç›¸å…³è¡Œä¸šæŒ‡æ•°ï¼Œä¸ºæŠ•èµ„è€…æä¾›ä¸€ç«™å¼è¡Œä¸šæŠ•èµ„å·¥å…·ï¼Œå…·æœ‰åˆ†æ•£åŒ–æŠ•èµ„ä¼˜åŠ¿ã€‚"

                        doc = {
                            'id': f"etf_{etf_code}_{int(time.time())}",
                            'content': content,
                            'metadata': {
                                'source': 'AKShare-è¡Œä¸šETF',
                                'doc_type': 'etf_data',
                                'etf_code': etf_code,
                                'etf_name': etf_name,
                                'net_value': float(latest_etf.get('æ”¶ç›˜', 0)),
                                'volume': int(latest_etf.get('æˆäº¤é‡', 0)),
                                'data_source': 'akshare_etf',
                                'importance': 7,
                                'collection_time': datetime.now().isoformat()
                            }
                        }
                        collected_docs.append(doc)
                        print(f"   âœ… {etf_name} ETFæ•°æ®æ”¶é›†å®Œæˆ")

                except Exception as e:
                    print(f"   âš ï¸ è·å–ETF{etf_code}æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… å…¨é¢è‚¡ç¥¨æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…")
            return []
        except Exception as e:
            print(f"âŒ è‚¡ç¥¨æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    async def collect_comprehensive_news(self) -> List[Dict]:
        """æ”¶é›†å…¨é¢è´¢ç»æ–°é—»"""
        print("ğŸ“° å¼€å§‹æ”¶é›†å…¨é¢è´¢ç»æ–°é—»...")

        try:
            import akshare as ak
            collected_docs = []

            # 1. å¤®è§†è´¢ç»æ–°é—»
            print("   æ”¶é›†å¤®è§†è´¢ç»æ–°é—»...")
            try:
                news_data = ak.news_cctv()
                if not news_data.empty:
                    for _, news in news_data.head(15).iterrows():  # å¢åŠ åˆ°15æ¡
                        doc = {
                            'id': f"news_cctv_{int(time.time())}_{len(collected_docs)}",
                            'content': f"{news.get('title', '')} {news.get('content', '')}",
                            'metadata': {
                                'source': 'å¤®è§†è´¢ç»',
                                'doc_type': 'financial_news',
                                'news_type': 'å®è§‚è´¢ç»',
                                'title': news.get('title', ''),
                                'publish_date': str(news.get('date', datetime.now().date())),
                                'data_source': 'akshare_cctv_news',
                                'importance': 9,
                                'collection_time': datetime.now().isoformat()
                            }
                        }
                        collected_docs.append(doc)
                    print(f"   âœ… å¤®è§†è´¢ç»æ–°é—»: {len(news_data.head(15))} æ¡")
            except Exception as e:
                print(f"   âš ï¸ å¤®è§†æ–°é—»è·å–å¤±è´¥: {e}")

            # 2. è·å–å®è§‚ç»æµæ•°æ®æ–°é—»
            print("   ç”Ÿæˆå®è§‚ç»æµåˆ†æ...")
            try:
                # PMIæ•°æ®
                pmi_data = ak.macro_china_pmi()
                if not pmi_data.empty:
                    latest_pmi = pmi_data.iloc[-1]
                    content = f"ä¸­å›½åˆ¶é€ ä¸šPMIæŒ‡æ•°åˆ†æï¼š{latest_pmi.get('æœˆä»½', 'N/A')}æœˆPMIä¸º{latest_pmi.get('PMI', 'N/A')}ï¼Œåˆ¶é€ ä¸šæ™¯æ°”åº¦{'æ‰©å¼ ' if float(latest_pmi.get('PMI', 50)) > 50 else 'æ”¶ç¼©'}ã€‚PMIä½œä¸ºç»æµå…ˆè¡ŒæŒ‡æ ‡ï¼Œç›´æ¥å½±å“è‚¡å¸‚æŠ•èµ„æƒ…ç»ªï¼Œç‰¹åˆ«æ˜¯åˆ¶é€ ä¸šç›¸å…³æ¿å—å¦‚æœ‰è‰²é‡‘å±ã€æœºæ¢°è®¾å¤‡ç­‰ã€‚å½“å‰æ•°æ®æ˜¾ç¤ºåˆ¶é€ ä¸šè¿è¡Œæ€åŠ¿ï¼Œä¸ºæŠ•èµ„å†³ç­–æä¾›é‡è¦å‚è€ƒã€‚"

                    doc = {
                        'id': f"macro_pmi_analysis_{int(time.time())}",
                        'content': content,
                        'metadata': {
                            'source': 'AKShare-å®è§‚åˆ†æ',
                            'doc_type': 'macro_analysis',
                            'indicator': 'PMIåˆ¶é€ ä¸šæŒ‡æ•°',
                            'value': str(latest_pmi.get('PMI', '')),
                            'period': str(latest_pmi.get('æœˆä»½', '')),
                            'data_source': 'akshare_macro_analysis',
                            'importance': 9,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)
                    print(f"   âœ… PMIå®è§‚åˆ†æå®Œæˆ")
            except Exception as e:
                print(f"   âš ï¸ å®è§‚æ•°æ®åˆ†æå¤±è´¥: {e}")

            # 3. ç”Ÿæˆè¡Œä¸šåˆ†ææŠ¥å‘Š
            print("   ç”Ÿæˆè¡Œä¸šåˆ†ææŠ¥å‘Š...")
            sector_analyses = [
                {
                    'sector': 'äººå·¥æ™ºèƒ½',
                    'content': 'äººå·¥æ™ºèƒ½äº§ä¸šè¿æ¥æ”¿ç­–åˆ©å¥½ï¼Œå¤§æ¨¡å‹æŠ€æœ¯ä¸æ–­çªç ´ï¼Œç®—åŠ›éœ€æ±‚æ¿€å¢ã€‚ç›¸å…³ä¸Šå¸‚å…¬å¸åŒ…æ‹¬ç§‘å¤§è®¯é£ã€å•†æ±¤ç§‘æŠ€ç­‰åœ¨AIåº”ç”¨è½åœ°æ–¹é¢å–å¾—é‡è¦è¿›å±•ã€‚æŠ•èµ„è€…åº”å…³æ³¨å…·å¤‡æ ¸å¿ƒæŠ€æœ¯çš„é¾™å¤´ä¼ä¸šï¼Œé‡ç‚¹å¸ƒå±€ç®—åŠ›åŸºç¡€è®¾æ–½ã€AIåº”ç”¨è½¯ä»¶ç­‰ç»†åˆ†é¢†åŸŸã€‚',
                    'importance': 9
                },
                {
                    'sector': 'èŠ¯ç‰‡åŠå¯¼ä½“',
                    'content': 'å›½å†…èŠ¯ç‰‡äº§ä¸šé“¾åŠ é€Ÿå‘å±•ï¼Œæ”¿ç­–æ‰¶æŒåŠ›åº¦åŠ å¤§ã€‚è®¾è®¡ã€åˆ¶é€ ã€å°æµ‹ç­‰å„ç¯èŠ‚å‡æœ‰æ‰€çªç ´ã€‚é‡ç‚¹å…³æ³¨å…·å¤‡è‡ªä¸»çŸ¥è¯†äº§æƒçš„èŠ¯ç‰‡è®¾è®¡å…¬å¸ï¼Œå¦‚éŸ¦å°”è‚¡ä»½ã€æ¾œèµ·ç§‘æŠ€ç­‰ã€‚éšç€å›½äº§æ›¿ä»£è¿›ç¨‹åŠ é€Ÿï¼Œç›¸å…³æ ‡çš„å…·å¤‡é•¿æœŸæŠ•èµ„ä»·å€¼ã€‚',
                    'importance': 8
                },
                {
                    'sector': 'æ–°èƒ½æºæ±½è½¦',
                    'content': 'æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œäº§ä¸šé“¾æ—¥è¶‹æˆç†Ÿã€‚ç”µæ± æŠ€æœ¯ä¸æ–­è¿›æ­¥ï¼Œå……ç”µåŸºç¡€è®¾æ–½åŠ å¿«å»ºè®¾ã€‚å®å¾·æ—¶ä»£ã€æ¯”äºšè¿ªç­‰é¾™å¤´ä¼ä¸šå¸‚åœºåœ°ä½ç¨³å›ºã€‚æŠ•èµ„æœºä¼šä¸»è¦é›†ä¸­åœ¨ç”µæ± ææ–™ã€æ™ºèƒ½é©¾é©¶ã€å……ç”µè®¾æ–½ç­‰ç»†åˆ†èµ›é“ã€‚',
                    'importance': 8
                }
            ]

            for analysis in sector_analyses:
                doc = {
                    'id': f"sector_analysis_{analysis['sector']}_{int(time.time())}",
                    'content': f"{analysis['sector']}è¡Œä¸šæŠ•èµ„åˆ†æï¼š{analysis['content']}",
                    'metadata': {
                        'source': 'è¡Œä¸šç ”ç©¶åˆ†æ',
                        'doc_type': 'sector_analysis',
                        'sector': analysis['sector'],
                        'analysis_type': 'æŠ•èµ„ç­–ç•¥',
                        'data_source': 'sector_analysis_report',
                        'importance': analysis['importance'],
                        'collection_time': datetime.now().isoformat()
                    }
                }
                collected_docs.append(doc)

            print(f"âœ… å…¨é¢è´¢ç»æ–°é—»æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except Exception as e:
            print(f"âŒ è´¢ç»æ–°é—»æ”¶é›†å¤±è´¥: {e}")
            return []

    def store_documents_to_vector_db(self, documents: List[Dict]) -> int:
        """æ‰¹é‡å­˜å‚¨æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        if not documents:
            return 0

        print(f"ğŸ§  å¼€å§‹å¤§è§„æ¨¡å‘é‡åŒ–å­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£...")

        try:
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            print("   ğŸ”„ ç”ŸæˆTF-IDFå‘é‡åµŒå…¥...")
            start_time = time.time()
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            embeddings = tfidf_matrix.toarray()
            embedding_time = time.time() - start_time

            print(f"   âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’")
            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]}")

            # åˆ†æ‰¹å­˜å‚¨é¿å…ä¸€æ¬¡æ€§å­˜å‚¨è¿‡å¤š
            batch_size = 50
            total_stored = 0

            for i in range(0, len(documents), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_ids = doc_ids[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                batch_embeddings = embeddings[i:i+batch_size]

                print(f"   ğŸ’¾ å­˜å‚¨æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_texts)} ä¸ªæ–‡æ¡£...")

                self.collection.add(
                    embeddings=batch_embeddings.tolist(),
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )

                total_stored += len(batch_texts)
                time.sleep(1)  # é¿å…é¢‘ç‡é™åˆ¶

            print(f"âœ… æˆåŠŸå­˜å‚¨ {total_stored} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return total_stored

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    async def collect_all_large_scale_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰å¤§è§„æ¨¡æ•°æ®"""
        print("ğŸš€ å¼€å§‹å¤§è§„æ¨¡æ•°æ®æ”¶é›†...")
        start_time = time.time()

        all_docs = []

        # 1. å…¨é¢è‚¡ç¥¨æ•°æ®
        stock_docs = await self.collect_sector_stocks_comprehensive()
        all_docs.extend(stock_docs)

        # 2. å…¨é¢è´¢ç»æ–°é—»
        news_docs = await self.collect_comprehensive_news()
        all_docs.extend(news_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ å¤§è§„æ¨¡æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ æ•°æ®åˆ†å¸ƒ:")
        print(f"      - è‚¡ç¥¨æ•°æ®: {len(stock_docs)} ä¸ª")
        print(f"      - è´¢ç»æ–°é—»: {len(news_docs)} ä¸ª")

        # å‘é‡åŒ–å­˜å‚¨
        if all_docs:
            print(f"\nğŸ§  å¼€å§‹å¤§è§„æ¨¡å‘é‡åŒ–å­˜å‚¨...")
            stored_count = self.store_documents_to_vector_db(all_docs)
            print(f"âœ… å¤§è§„æ¨¡å‘é‡åŒ–å­˜å‚¨å®Œæˆ: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")

        return all_docs

    def get_test_stocks(self) -> List[Dict]:
        """æä¾›æµ‹è¯•ç”¨çš„è‚¡ç¥¨ä¾‹å­"""
        test_stocks = [
            {
                'code': '002230',
                'name': 'ç§‘å¤§å›½åˆ›',
                'sector': 'è®¡ç®—æœº',
                'description': 'å›½å†…é¢†å…ˆçš„æ™ºèƒ½è¯­éŸ³å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯å…¬å¸'
            },
            {
                'code': '000725',
                'name': 'äº¬ä¸œæ–¹A',
                'sector': 'èŠ¯ç‰‡åŠå¯¼ä½“',
                'description': 'å…¨çƒé¢†å…ˆçš„åŠå¯¼ä½“æ˜¾ç¤ºæŠ€æœ¯å…¬å¸'
            },
            {
                'code': '002460',
                'name': 'èµ£é”‹é”‚ä¸š',
                'sector': 'æœ‰è‰²é‡‘å±',
                'description': 'å…¨çƒé¢†å…ˆçš„é”‚åŒ–åˆç‰©ç”Ÿäº§å•†'
            },
            {
                'code': '300750',
                'name': 'å®å¾·æ—¶ä»£',
                'sector': 'æ–°èƒ½æº',
                'description': 'å…¨çƒåŠ¨åŠ›ç”µæ± è¡Œä¸šé¢†å†›ä¼ä¸š'
            },
            {
                'code': '300229',
                'name': 'æ‹“å°”æ€',
                'sector': 'äººå·¥æ™ºèƒ½',
                'description': 'ä¸“ä¸šçš„äººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®æŠ€æœ¯æœåŠ¡å•†'
            }
        ]
        return test_stocks

async def test_large_scale_pipeline():
    """æµ‹è¯•å¤§è§„æ¨¡æ•°æ®ç®¡é“"""
    print("ğŸ§ª æµ‹è¯•å¤§è§„æ¨¡AKShareæ•°æ®ç®¡é“")
    print("=" * 80)

    async with LargeScaleAKShareCollector() as collector:
        # æ”¶é›†å¤§è§„æ¨¡æ•°æ®
        all_docs = await collector.collect_all_large_scale_data()

        if all_docs:
            # ä¿å­˜æ•°æ®
            output_file = f"/home/wyatt/prism2/rag-service/large_scale_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_docs, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¤§è§„æ¨¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

            # éªŒè¯æ•°æ®åº“
            count = collector.collection.count()
            print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“ç»Ÿè®¡: {count} ä¸ªæ–‡æ¡£")

            # æä¾›æµ‹è¯•è‚¡ç¥¨
            test_stocks = collector.get_test_stocks()
            print(f"\nğŸ¯ RAGæµ‹è¯•ç”¨è‚¡ç¥¨ä¾‹å­:")
            print("=" * 50)
            for stock in test_stocks:
                print(f"ğŸ“„ {stock['name']}({stock['code']})")
                print(f"   è¡Œä¸š: {stock['sector']}")
                print(f"   æè¿°: {stock['description']}")
                print(f"   æµ‹è¯•æŸ¥è¯¢ç¤ºä¾‹: '{stock['name']}', 'è‚¡ç¥¨{stock['code']}', '{stock['sector']}è¡Œä¸š'")
                print("-" * 40)

            return all_docs, test_stocks
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
            return [], []

if __name__ == "__main__":
    asyncio.run(test_large_scale_pipeline())