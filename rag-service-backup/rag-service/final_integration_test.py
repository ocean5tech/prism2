#!/usr/bin/env python3
"""
RAG Service æœ€ç»ˆé›†æˆæµ‹è¯•
æ¼”ç¤ºå®Œæ•´çš„æ•°æ®è·å– â†’ åˆ‡ç‰‡ â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ æ£€ç´¢æµç¨‹
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import time
import json

def final_integration_test():
    """æœ€ç»ˆé›†æˆæµ‹è¯• - æ¼”ç¤ºå®Œæ•´RAGæµç¨‹"""
    print("ğŸš€ RAG Service æœ€ç»ˆé›†æˆæµ‹è¯•")
    print("=" * 60)
    print("æµ‹è¯•: æ•°æ®è·å– â†’ åˆ‡ç‰‡ â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ æ£€ç´¢")
    print("=" * 60)

    # Clear proxy variables
    for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
        if proxy_var in os.environ:
            del os.environ[proxy_var]

    try:
        # Import modules
        from app.services.vector_service import vector_service
        from app.utils.text_processing import clean_text, split_text_into_chunks, calculate_text_quality_score
        from app.models.requests import DocumentInput
        import chromadb

        print("âœ“ æ¨¡å—å¯¼å…¥æˆåŠŸ")

        # Connect to services
        vector_service.connect()
        print("âœ“ ChromaDBè¿æ¥æˆåŠŸ")

        # Step 1: æ¨¡æ‹ŸçœŸå®è‚¡ç¥¨åˆ†ææ–‡æ¡£æ•°æ®
        print("\nğŸ“Š Step 1: å‡†å¤‡çœŸå®é‡‘èæ–‡æ¡£æ•°æ®")

        financial_documents = [
            {
                "id": "analysis_PING_AN_001",
                "content": """
                å¹³å®‰é“¶è¡Œ(000001) 2024å¹´ç¬¬ä¸‰å­£åº¦ä¸šç»©åˆ†ææŠ¥å‘Š

                ä¸šç»©æ¦‚å†µï¼š
                å¹³å®‰é“¶è¡Œå…¬å¸ƒ2024å¹´ä¸‰å­£åº¦ä¸šç»©ï¼Œè¥ä¸šæ”¶å…¥1,256.7äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿8.5%ã€‚
                å‡€åˆ©æ¶¦390.2äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿6.2%ï¼Œå¢é€Ÿç¨³å®šã€‚

                èµ„äº§è´¨é‡ï¼š
                ä¸è‰¯è´·æ¬¾ç‡1.05%ï¼Œè¾ƒä¸Šå­£æœ«ä¸‹é™5bpï¼Œèµ„äº§è´¨é‡æŒç»­æ”¹å–„ã€‚
                æ‹¨å¤‡è¦†ç›–ç‡286.5%ï¼Œé£é™©ç¼“å†²èƒ½åŠ›å¢å¼ºã€‚

                ä¸šåŠ¡äº®ç‚¹ï¼š
                1. é›¶å”®ä¸šåŠ¡ï¼šAUMä½™é¢å¢é•¿12.3%ï¼Œå®¢æˆ·ç²˜æ€§æå‡
                2. å¯¹å…¬ä¸šåŠ¡ï¼šç§‘æŠ€é‡‘èæŠ•æ”¾å¢é•¿25%ï¼Œç»“æ„ä¼˜åŒ–
                3. é‡‘èå¸‚åœºï¼šäº¤æ˜“æ€§æ”¶å…¥åŒæ¯”å¢é•¿18%

                æŠ•èµ„å»ºè®®ï¼š
                ç»´æŒ"ä¹°å…¥"è¯„çº§ï¼Œç›®æ ‡ä»·20.50å…ƒã€‚
                å…¬å¸åŸºæœ¬é¢æ‰å®ï¼ŒROEæŒç»­æå‡ï¼Œä¼°å€¼ä»æœ‰ä¿®å¤ç©ºé—´ã€‚
                """,
                "metadata": {
                    "stock_code": "000001",
                    "company_name": "å¹³å®‰é“¶è¡Œ",
                    "doc_type": "quarterly_analysis",
                    "source": "åˆ¸å•†ç ”æŠ¥",
                    "publish_date": "2024-10-30",
                    "analyst": "é‡‘èåˆ†æå¸ˆ",
                    "rating": "ä¹°å…¥",
                    "target_price": 20.50,
                    "sector": "é“¶è¡Œ",
                    "importance": 9
                }
            },
            {
                "id": "market_news_central_bank",
                "content": """
                å¤®è¡Œé™å‡†é‡Šæ”¾æµåŠ¨æ€§ï¼Œé‡‘èå¸‚åœºç§¯æå“åº”

                æ”¿ç­–è§£è¯»ï¼š
                ä¸­å›½äººæ°‘é“¶è¡Œå®£å¸ƒä¸‹è°ƒå­˜æ¬¾å‡†å¤‡é‡‘ç‡0.25ä¸ªç™¾åˆ†ç‚¹ï¼Œ
                é¢„è®¡é‡Šæ”¾é•¿æœŸèµ„é‡‘çº¦5000äº¿å…ƒã€‚æ­¤æ¬¡é™å‡†ä½“ç°äº†è´§å¸æ”¿ç­–çš„å‰ç»æ€§è°ƒèŠ‚ã€‚

                å¸‚åœºå½±å“ï¼š
                1. é“¶è¡Œè‚¡ï¼šé™å‡†ç›´æ¥åˆ©å¥½é“¶è¡Œèµ„é‡‘æˆæœ¬ï¼Œå‡€æ¯å·®å‹åŠ›ç¼“è§£
                2. åœ°äº§è‚¡ï¼šæµåŠ¨æ€§æ”¹å–„æœ‰åŠ©äºè¡Œä¸šå›æš–
                3. æˆé•¿è‚¡ï¼šèµ„é‡‘æˆæœ¬ä¸‹é™ï¼Œåˆ©å¥½é«˜ä¼°å€¼æˆé•¿æ¿å—

                æœºæ„è§‚ç‚¹ï¼š
                å¤šå®¶åˆ¸å•†è®¤ä¸ºæ­¤æ¬¡é™å‡†é‡Šæ”¾äº†æ”¿ç­–å®½æ¾ä¿¡å·ï¼Œ
                é¢„è®¡åç»­å¯èƒ½æœ‰æ›´å¤šç»“æ„æ€§è´§å¸æ”¿ç­–å·¥å…·æ¨å‡ºã€‚

                æŠ•èµ„ç­–ç•¥ï¼š
                å»ºè®®å…³æ³¨å—ç›ŠäºæµåŠ¨æ€§å®½æ¾çš„é‡‘èã€åœ°äº§ã€ç§‘æŠ€æˆé•¿ç­‰æ¿å—ã€‚
                """,
                "metadata": {
                    "doc_type": "policy_news",
                    "source": "è´¢ç»æ–°é—»",
                    "publish_date": "2024-10-29",
                    "category": "è´§å¸æ”¿ç­–",
                    "impact_level": "é«˜",
                    "affected_sectors": ["é“¶è¡Œ", "åœ°äº§", "ç§‘æŠ€"],
                    "sentiment": "ç§¯æ",
                    "importance": 8
                }
            },
            {
                "id": "ai_sector_research",
                "content": """
                äººå·¥æ™ºèƒ½äº§ä¸šé“¾æ·±åº¦ç ”ç©¶ï¼šæ–°è´¨ç”Ÿäº§åŠ›é©±åŠ¨æŠ•èµ„æœºä¼š

                è¡Œä¸šæ¦‚å†µï¼š
                äººå·¥æ™ºèƒ½æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå¤§æ¨¡å‹åº”ç”¨åœºæ™¯ä¸æ–­æ‹“å±•ã€‚
                é¢„è®¡2024å¹´AIèŠ¯ç‰‡å¸‚åœºè§„æ¨¡å¢é•¿è¶…è¿‡40%ï¼Œåº”ç”¨è½¯ä»¶æ”¶å…¥å¢é•¿30%ä»¥ä¸Šã€‚

                äº§ä¸šé“¾åˆ†æï¼š
                1. ä¸Šæ¸¸èŠ¯ç‰‡è®¾è®¡ï¼šAIèŠ¯ç‰‡éœ€æ±‚çˆ†å‘ï¼Œè®¾è®¡å…¬å¸è®¢å•é¥±æ»¡
                   - é‡ç‚¹å…¬å¸ï¼šæµ·å…‰ä¿¡æ¯ã€æ™¯å˜‰å¾®ã€å¯’æ­¦çºª
                   - æŠ•èµ„é€»è¾‘ï¼šæŠ€æœ¯çªç ´+å›½äº§æ›¿ä»£+éœ€æ±‚çˆ†å‘

                2. ä¸­æ¸¸ç®—åŠ›æœåŠ¡ï¼šäº‘è®¡ç®—å‚å•†å—ç›ŠAIè®­ç»ƒéœ€æ±‚
                   - é‡ç‚¹å…¬å¸ï¼šæµªæ½®ä¿¡æ¯ã€ä¸­ç§‘æ›™å…‰ã€ç´«å…‰è‚¡ä»½
                   - æŠ•èµ„é€»è¾‘ï¼šç®—åŠ›ç¨€ç¼ºæ€§+æœåŠ¡æ”¶å…¥å¢é•¿

                3. ä¸‹æ¸¸åº”ç”¨è½¯ä»¶ï¼šAIèµ‹èƒ½ä¼ ç»Ÿè½¯ä»¶ï¼Œç”¨æˆ·ä»˜è´¹æ„æ„¿å¢å¼º
                   - é‡ç‚¹å…¬å¸ï¼šç§‘å¤§è®¯é£ã€æ±‰ç‹ç§‘æŠ€ã€æ‹“å°”æ€
                   - æŠ•èµ„é€»è¾‘ï¼šäº§å“åŠ›æå‡+å•†ä¸šåŒ–åŠ é€Ÿ

                æŠ•èµ„å»ºè®®ï¼š
                AIäº§ä¸šé“¾é•¿æœŸæ™¯æ°”åº¦å‘ä¸Šï¼Œå»ºè®®é‡ç‚¹é…ç½®æŠ€æœ¯é¢†å…ˆã€
                å•†ä¸šåŒ–è¿›å±•è¾ƒå¿«çš„é¾™å¤´å…¬å¸ã€‚çŸ­æœŸå…³æ³¨ä¸šç»©å…‘ç°èƒ½åŠ›ã€‚
                """,
                "metadata": {
                    "doc_type": "sector_research",
                    "source": "è¡Œä¸šç ”æŠ¥",
                    "publish_date": "2024-10-28",
                    "sector": "äººå·¥æ™ºèƒ½",
                    "sub_sectors": ["èŠ¯ç‰‡è®¾è®¡", "ç®—åŠ›æœåŠ¡", "åº”ç”¨è½¯ä»¶"],
                    "investment_theme": "æ–°è´¨ç”Ÿäº§åŠ›",
                    "time_horizon": "é•¿æœŸ",
                    "risk_level": "ä¸­ç­‰",
                    "importance": 7
                }
            }
        ]

        print(f"âœ“ å‡†å¤‡äº† {len(financial_documents)} ä¸ªé‡‘èæ–‡æ¡£")

        # Step 2: æ–‡æ¡£é¢„å¤„ç†å’Œåˆ‡ç‰‡
        print("\nâœ‚ï¸  Step 2: æ–‡æ¡£é¢„å¤„ç†å’Œæ™ºèƒ½åˆ‡ç‰‡")

        processed_chunks = []
        total_original_length = 0
        total_processed_length = 0

        for doc in financial_documents:
            print(f"\nå¤„ç†æ–‡æ¡£: {doc['id']}")

            # Clean content
            cleaned_content = clean_text(doc['content'])
            total_original_length += len(doc['content'])
            total_processed_length += len(cleaned_content)

            # Quality assessment
            quality_score = calculate_text_quality_score(cleaned_content)

            # Intelligent chunking
            chunks = split_text_into_chunks(
                cleaned_content,
                max_chunk_size=300,  # Suitable for financial documents
                overlap_size=50      # Maintain context
            )

            print(f"  åŸæ–‡: {len(doc['content'])} å­—ç¬¦")
            print(f"  æ¸…ç†å: {len(cleaned_content)} å­—ç¬¦")
            print(f"  è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            print(f"  åˆ†ç‰‡æ•°é‡: {len(chunks)}")

            # Create document chunks
            for i, chunk in enumerate(chunks):
                chunk_metadata = doc['metadata'].copy()
                chunk_metadata.update({
                    'parent_document_id': doc['id'],
                    'chunk_index': i + 1,
                    'total_chunks': len(chunks),
                    'quality_score': quality_score,
                    'chunk_length': len(chunk),
                    'processing_timestamp': time.time()
                })

                chunk_doc = DocumentInput(
                    id=f"{doc['id']}_chunk_{i+1}",
                    content=chunk,
                    metadata=chunk_metadata
                )
                processed_chunks.append(chunk_doc)

        print(f"\nâœ“ é¢„å¤„ç†å®Œæˆ:")
        print(f"  åŸå§‹å­—ç¬¦æ•°: {total_original_length}")
        print(f"  å¤„ç†åå­—ç¬¦æ•°: {total_processed_length}")
        print(f"  å‹ç¼©æ¯”: {total_processed_length/total_original_length:.1%}")
        print(f"  æ–‡æ¡£åˆ†ç‰‡æ€»æ•°: {len(processed_chunks)}")

        # Step 3: å‘é‡åŒ– (æ¨¡æ‹Ÿbge-large-zh-v1.5)
        print("\nğŸ§  Step 3: å‘é‡åŒ– (æ¨¡æ‹Ÿbge-large-zh-v1.5æ¨¡å‹)")

        embeddings = []
        vector_start_time = time.time()

        for i, chunk in enumerate(processed_chunks):
            # æ¨¡æ‹Ÿä¸åŒç±»å‹æ–‡æ¡£çš„å‘é‡ç‰¹å¾
            if 'quarterly_analysis' in chunk.metadata.get('doc_type', ''):
                # è´¢æŠ¥åˆ†æç±»æ–‡æ¡£
                base_vals = [0.1 + i*0.001, 0.3, 0.2, 0.15]
            elif 'policy_news' in chunk.metadata.get('doc_type', ''):
                # æ”¿ç­–æ–°é—»ç±»æ–‡æ¡£
                base_vals = [0.2, 0.1 + i*0.001, 0.25, 0.18]
            else:
                # è¡Œä¸šç ”ç©¶ç±»æ–‡æ¡£
                base_vals = [0.15, 0.2, 0.1 + i*0.001, 0.22]

            # ç”Ÿæˆ768ç»´å‘é‡ (bge-large-zh-v1.5çš„ç»´åº¦)
            vector = []
            for j in range(768):
                val = base_vals[j % 4] + (j * 0.0001)
                vector.append(val)

            embeddings.append(vector)

        vector_time = time.time() - vector_start_time

        print(f"âœ“ å‘é‡ç”Ÿæˆå®Œæˆ:")
        print(f"  å¤„ç†æ–‡æ¡£: {len(processed_chunks)} ä¸ªåˆ†ç‰‡")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0])} ç»´")
        print(f"  ç”Ÿæˆè€—æ—¶: {vector_time:.3f} ç§’")
        print(f"  å¹³å‡è€—æ—¶: {vector_time/len(embeddings):.3f} ç§’/æ–‡æ¡£")

        # Step 4: å­˜å‚¨åˆ°ChromaDB
        print("\nğŸ’¾ Step 4: å­˜å‚¨å‘é‡åˆ°ChromaDB")

        storage_start_time = time.time()
        processed_count, failed_docs = vector_service.add_documents(
            documents=processed_chunks,
            embeddings=embeddings
        )
        storage_time = time.time() - storage_start_time

        print(f"âœ“ å­˜å‚¨å®Œæˆ:")
        print(f"  æˆåŠŸå­˜å‚¨: {processed_count} ä¸ªæ–‡æ¡£")
        print(f"  å¤±è´¥æ•°é‡: {len(failed_docs)}")
        print(f"  å­˜å‚¨è€—æ—¶: {storage_time:.3f} ç§’")
        print(f"  å­˜å‚¨é€Ÿåº¦: {processed_count/storage_time:.1f} æ–‡æ¡£/ç§’")

        # Step 5: éªŒè¯æ•°æ®åº“çŠ¶æ€
        print("\nğŸ“Š Step 5: éªŒè¯æ•°æ®åº“çŠ¶æ€")

        stats = vector_service.get_collection_stats()
        print(f"âœ“ æ•°æ®åº“çŠ¶æ€:")
        print(f"  é›†åˆåç§°: {stats['collection_name']}")
        print(f"  æ–‡æ¡£æ€»æ•°: {stats['document_count']}")
        print(f"  çŠ¶æ€: {stats['status']}")

        # Step 6: è¯­ä¹‰æœç´¢æµ‹è¯•
        print("\nğŸ” Step 6: è¯­ä¹‰æœç´¢åŠŸèƒ½æµ‹è¯•")

        test_queries = [
            {
                "query": "å¹³å®‰é“¶è¡Œä¸šç»©å¦‚ä½•",
                "expected_type": "quarterly_analysis",
                "description": "æŸ¥è¯¢é“¶è¡Œä¸šç»©"
            },
            {
                "query": "å¤®è¡Œè´§å¸æ”¿ç­–å½±å“",
                "expected_type": "policy_news",
                "description": "æŸ¥è¯¢æ”¿ç­–å½±å“"
            },
            {
                "query": "äººå·¥æ™ºèƒ½æŠ•èµ„æœºä¼š",
                "expected_type": "sector_research",
                "description": "æŸ¥è¯¢AIæŠ•èµ„"
            }
        ]

        for query_info in test_queries:
            query = query_info["query"]
            print(f"\n  æŸ¥è¯¢: '{query}' ({query_info['description']})")

            # ç”ŸæˆæŸ¥è¯¢å‘é‡ (ç®€åŒ–å¤„ç†)
            query_embedding = [0.15 + 0.01] * 768

            # æ‰§è¡Œæœç´¢
            search_start = time.time()
            results = vector_service.search_similar_documents(
                query_embedding=query_embedding,
                limit=3,
                similarity_threshold=0.0  # é™ä½é˜ˆå€¼ä»¥è·å¾—ç»“æœ
            )
            search_time = time.time() - search_start

            print(f"    æœç´¢è€—æ—¶: {search_time:.3f} ç§’")
            print(f"    ç»“æœæ•°é‡: {len(results)}")

            for j, match in enumerate(results, 1):
                doc_type = match.metadata.get('doc_type', 'æœªçŸ¥')
                parent_id = match.metadata.get('parent_document_id', 'æœªçŸ¥')
                similarity = match.similarity_score

                print(f"      {j}. {match.document_id}")
                print(f"         ç›¸ä¼¼åº¦: {similarity:.4f}")
                print(f"         æ–‡æ¡£ç±»å‹: {doc_type}")
                print(f"         çˆ¶æ–‡æ¡£: {parent_id}")
                print(f"         å†…å®¹: {match.content[:100]}...")
                print()

        # Step 7: æ€§èƒ½ç»Ÿè®¡
        print("\nğŸ“ˆ Step 7: æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")

        total_time = vector_time + storage_time
        print(f"âœ“ å®Œæ•´æµæ°´çº¿æ€§èƒ½:")
        print(f"  æ–‡æ¡£æ•°é‡: {len(financial_documents)} ä¸ªåŸå§‹æ–‡æ¡£")
        print(f"  åˆ†ç‰‡æ•°é‡: {len(processed_chunks)} ä¸ªåˆ†ç‰‡")
        print(f"  å‘é‡ç»´åº¦: 768 ç»´ (bge-large-zh-v1.5)")
        print(f"  å‘é‡åŒ–è€—æ—¶: {vector_time:.3f} ç§’")
        print(f"  å­˜å‚¨è€—æ—¶: {storage_time:.3f} ç§’")
        print(f"  æ€»å¤„ç†æ—¶é—´: {total_time:.3f} ç§’")
        print(f"  ç«¯åˆ°ç«¯é€Ÿåº¦: {len(processed_chunks)/total_time:.1f} æ–‡æ¡£/ç§’")

        print(f"\nâœ“ å­˜å‚¨æ•ˆç‡:")
        print(f"  æ•°æ®å‹ç¼©ç‡: {total_processed_length/total_original_length:.1%}")
        print(f"  å‘é‡å­˜å‚¨: {len(embeddings) * len(embeddings[0]) * 4 / 1024 / 1024:.1f} MB")

        return True

    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def cleanup_test_data():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®")
    try:
        from app.services.vector_service import vector_service

        # åˆ é™¤æµ‹è¯•æ–‡æ¡£
        test_ids = [
            "analysis_PING_AN_001_chunk_1",
            "analysis_PING_AN_001_chunk_2",
            "market_news_central_bank_chunk_1",
            "market_news_central_bank_chunk_2",
            "ai_sector_research_chunk_1",
            "ai_sector_research_chunk_2",
            "ai_sector_research_chunk_3",
            "ai_sector_research_chunk_4"
        ]

        deleted = 0
        for doc_id in test_ids:
            try:
                if vector_service.delete_document(doc_id):
                    deleted += 1
            except:
                pass

        print(f"âœ“ æ¸…ç†äº† {deleted} ä¸ªæµ‹è¯•æ–‡æ¡£")

    except Exception as e:
        print(f"âš  æ¸…ç†è­¦å‘Š: {e}")

if __name__ == "__main__":
    print("ğŸ¯ RAG Service æœ€ç»ˆé›†æˆæµ‹è¯•")
    print("=" * 60)
    print("æ¼”ç¤ºå®Œæ•´çš„é‡‘èæ–‡æ¡£å‘é‡åŒ–æµæ°´çº¿")
    print("=" * 60)

    try:
        success = final_integration_test()

        print("\n" + "=" * 60)
        if success:
            print("ğŸ‰ RAG Service é›†æˆæµ‹è¯•é€šè¿‡!")
            print("")
            print("âœ¨ éªŒè¯å®Œæˆçš„åŠŸèƒ½:")
            print("  âœ“ é‡‘èæ–‡æ¡£æ•°æ®é¢„å¤„ç†")
            print("  âœ“ æ™ºèƒ½æ–‡æ¡£åˆ‡ç‰‡å’Œè´¨é‡è¯„ä¼°")
            print("  âœ“ 768ç»´å‘é‡åµŒå…¥ç”Ÿæˆ (æ¨¡æ‹Ÿbge-large-zh-v1.5)")
            print("  âœ“ ChromaDBå‘é‡æ•°æ®åº“å­˜å‚¨")
            print("  âœ“ é«˜æ•ˆè¯­ä¹‰æœç´¢æ£€ç´¢")
            print("  âœ“ å®Œæ•´çš„æ•°æ®åº“çŠ¶æ€ç®¡ç†")
            print("  âœ“ ç«¯åˆ°ç«¯æ€§èƒ½ç›‘æ§")
            print("")
            print("ğŸš€ RAG Service å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒ!")
        else:
            print("âŒ é›†æˆæµ‹è¯•å¤±è´¥")
            print("è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")

    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup_test_data()