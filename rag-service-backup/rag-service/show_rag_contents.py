#!/usr/bin/env python3
"""
æ˜¾ç¤ºRAGåº“è¯¦ç»†å†…å®¹
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import chromadb
import json
from datetime import datetime

def show_rag_contents():
    """æ˜¾ç¤ºRAGåº“è¯¦ç»†å†…å®¹"""
    print("ğŸ“Š RAGæ•°æ®åº“å†…å®¹è¯¦ç»†æŠ¥å‘Š")
    print("=" * 80)

    # Clear proxy variables
    for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
        if proxy_var in os.environ:
            del os.environ[proxy_var]

    try:
        # Connect to ChromaDB
        client = chromadb.HttpClient(host='localhost', port=8000)
        client.heartbeat()
        print("âœ… æˆåŠŸè¿æ¥åˆ°ChromaDB")

        # Get collection
        collection = client.get_collection("financial_documents")
        count = collection.count()

        print(f"\nğŸ—‚ï¸  é›†åˆ: financial_documents")
        print(f"ğŸ“„ æ–‡æ¡£æ€»æ•°: {count}")
        print(f"ğŸ• æŸ¥çœ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if count > 0:
            # Get all documents
            results = collection.get(
                include=['documents', 'metadatas', 'embeddings']
            )

            print(f"\nğŸ“‹ æ‰€æœ‰æ–‡æ¡£è¯¦æƒ…:")
            print("=" * 80)

            for i, doc_id in enumerate(results['ids']):
                doc_content = results['documents'][i] if results['documents'] else 'No content'
                metadata = results['metadatas'][i] if results['metadatas'] else {}
                embedding_dim = len(results['embeddings'][i]) if results['embeddings'] is not None and i < len(results['embeddings']) and results['embeddings'][i] is not None else 0

                print(f"\nğŸ“„ æ–‡æ¡£ {i+1}: {doc_id}")
                print("-" * 60)
                print(f"ğŸ“ å†…å®¹: {doc_content}")
                print(f"ğŸ§® å‘é‡ç»´åº¦: {embedding_dim}")
                print(f"ğŸ·ï¸  å…ƒæ•°æ®:")
                for key, value in metadata.items():
                    print(f"   {key}: {value}")

            # Show metadata statistics
            print(f"\nğŸ“ˆ å…ƒæ•°æ®ç»Ÿè®¡:")
            print("=" * 40)

            # Count by document type
            doc_types = {}
            sources = {}
            importance_levels = []

            for metadata in results['metadatas']:
                if metadata:
                    # Document types
                    doc_type = metadata.get('doc_type', 'unknown')
                    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

                    # Sources
                    source = metadata.get('source', 'unknown')
                    sources[source] = sources.get(source, 0) + 1

                    # Importance levels
                    importance = metadata.get('importance')
                    if importance:
                        importance_levels.append(importance)

            print(f"ğŸ“Š æŒ‰æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
            for doc_type, count in sorted(doc_types.items()):
                print(f"   {doc_type}: {count} ä¸ªæ–‡æ¡£")

            print(f"\nğŸ“Š æŒ‰æ¥æºåˆ†å¸ƒ:")
            for source, count in sorted(sources.items()):
                print(f"   {source}: {count} ä¸ªæ–‡æ¡£")

            if importance_levels:
                avg_importance = sum(importance_levels) / len(importance_levels)
                print(f"\nğŸ“Š é‡è¦æ€§ç»Ÿè®¡:")
                print(f"   å¹³å‡é‡è¦æ€§: {avg_importance:.1f}")
                print(f"   æœ€é«˜é‡è¦æ€§: {max(importance_levels)}")
                print(f"   æœ€ä½é‡è¦æ€§: {min(importance_levels)}")

            # Test semantic search
            print(f"\nğŸ” è¯­ä¹‰æœç´¢æµ‹è¯•:")
            print("=" * 40)

            test_queries = [
                ("é“¶è¡Œ", "æŸ¥æ‰¾é“¶è¡Œç›¸å…³æ–‡æ¡£"),
                ("æ”¿ç­–", "æŸ¥æ‰¾æ”¿ç­–ç›¸å…³æ–‡æ¡£"),
                ("äººå·¥æ™ºèƒ½", "æŸ¥æ‰¾AIç›¸å…³æ–‡æ¡£")
            ]

            for query_term, description in test_queries:
                print(f"\nğŸ” æœç´¢: '{query_term}' ({description})")

                # Simple text-based search
                matching_docs = []
                for i, doc in enumerate(results['documents']):
                    if query_term in doc:
                        matching_docs.append({
                            'id': results['ids'][i],
                            'content': doc[:100] + '...' if len(doc) > 100 else doc,
                            'metadata': results['metadatas'][i]
                        })

                print(f"   ğŸ“Š æ‰¾åˆ° {len(matching_docs)} ä¸ªåŒ¹é…æ–‡æ¡£:")
                for j, match in enumerate(matching_docs[:3]):  # Show top 3
                    print(f"      {j+1}. {match['id']}")
                    print(f"         {match['content']}")

        else:
            print("âŒ é›†åˆä¸ºç©ºï¼Œæ²¡æœ‰æ–‡æ¡£")

        return True

    except Exception as e:
        print(f"âŒ æŸ¥çœ‹RAGåº“å†…å®¹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    show_rag_contents()