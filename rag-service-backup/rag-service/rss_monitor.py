#!/usr/bin/env python3
"""
RSSç›‘æ§ç³»ç»Ÿ
å®šæœŸæŠ“å–è´¢ç»RSSæºï¼Œè·å–æœ€æ–°æ–°é—»å¹¶æ›´æ–°RAGæ•°æ®åº“
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import chromadb
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import hashlib
import time
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
import schedule
import threading

class RSSMonitor:
    def __init__(self):
        # Clear proxy variables
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        self.client = None
        self.collection = None
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))

        # RSSæºé…ç½®
        self.rss_feeds = {
            'å¤®è§†è´¢ç»': 'http://app.cctv.com/data/api/GetNewsHtml5?app=cctvnews&imageFlag=1&page=1&pageSize=20&serviceId=caijing',
            'äººæ°‘ç½‘è´¢ç»': 'http://finance.people.com.cn/rss/channel.xml',
            'æ–°åè´¢ç»': 'http://www.xinhuanet.com/finance/news_finance.xml',
            'ä¸­å›½æ–°é—»ç½‘è´¢ç»': 'http://finance.chinanews.com/rss/finance.xml',
            'é‡‘èç•Œ': 'http://rss.jrj.com.cn/rss/focus.xml',
            'è¯åˆ¸æ—¶æŠ¥': 'http://news.stcn.com/rss/news.xml',
            'å’Œè®¯è´¢ç»': 'http://rss.hexun.com/rss_main.aspx'
        }

        # ç›‘æ§é…ç½®
        self.update_interval = 30  # 30åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
        self.max_articles_per_feed = 20  # æ¯ä¸ªæºæœ€å¤šè·å–20ç¯‡æ–‡ç« 
        self.processed_urls = set()  # å·²å¤„ç†çš„URL

        # æ•°æ®è´¨é‡è¿‡æ»¤é…ç½®
        self.min_content_length = 100  # æœ€å°å†…å®¹é•¿åº¦
        self.blacklist_keywords = ['å¹¿å‘Š', 'æ¨å¹¿', 'å‹æƒ…é“¾æ¥', 'ç‰ˆæƒå£°æ˜']

    def connect_to_chromadb(self):
        """è¿æ¥åˆ°ChromaDB"""
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection("financial_documents")
            except Exception:
                # åˆ›å»ºæ–°é›†åˆ
                self.collection = self.client.create_collection(
                    name="financial_documents",
                    metadata={"description": "Financial news and analysis documents"}
                )

            print("âœ… è¿æ¥åˆ°ChromaDBæˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ è¿æ¥ChromaDBå¤±è´¥: {e}")
            return False

    def chinese_tokenize(self, text: str) -> str:
        """ä¸­æ–‡åˆ†è¯"""
        return ' '.join(jieba.cut(text))

    def create_tfidf_vector(self, text: str) -> List[float]:
        """åˆ›å»ºTF-IDFå‘é‡"""
        try:
            processed_text = self.chinese_tokenize(text)
            # ä½¿ç”¨å·²è®­ç»ƒçš„å‘é‡åŒ–å™¨æˆ–åˆ›å»ºæ–°çš„
            try:
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
            except:
                # é‡æ–°è®­ç»ƒå‘é‡åŒ–å™¨
                self.vectorizer.fit([processed_text])
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡
            return [0.0] * 1000

    def extract_keywords(self, title: str, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        combined_text = f"{title} {content}"
        words = jieba.cut(combined_text)

        # è¿‡æ»¤å¸¸ç”¨è¯å’Œåœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        keywords = [word for word in words if len(word) > 1 and word not in stop_words]

        # è¿”å›å‰10ä¸ªå…³é”®è¯
        return list(set(keywords))[:10]

    def calculate_importance_score(self, article: Dict) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§åˆ†æ•° (1-10)"""
        score = 5  # åŸºç¡€åˆ†æ•°

        title = article.get('title', '').lower()
        content = article.get('content', '').lower()

        # é‡è¦å…³é”®è¯åŠ åˆ†
        important_keywords = {
            'å¤®è¡Œ': 2, 'åˆ©ç‡': 2, 'GDP': 2, 'PMI': 2,
            'è‚¡å¸‚': 1, 'åŸºé‡‘': 1, 'å€ºåˆ¸': 1, 'å¤–æ±‡': 1,
            'æ”¿ç­–': 1, 'ç›‘ç®¡': 1, 'æ”¹é©': 1, 'åˆ›æ–°': 1,
            'é£é™©': 1, 'æœºä¼š': 1, 'æŠ•èµ„': 1, 'æ”¶ç›Š': 1
        }

        for keyword, points in important_keywords.items():
            if keyword in title:
                score += points * 2  # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
            elif keyword in content:
                score += points

        # å†…å®¹é•¿åº¦è°ƒæ•´
        content_length = len(content)
        if content_length > 1000:
            score += 1
        elif content_length < 200:
            score -= 1

        # æ—¶æ•ˆæ€§è°ƒæ•´
        pub_time = article.get('published_time')
        if pub_time:
            try:
                pub_datetime = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                hours_ago = (datetime.now() - pub_datetime.replace(tzinfo=None)).total_seconds() / 3600

                if hours_ago < 1:
                    score += 2  # 1å°æ—¶å†…çš„æ–°é—»
                elif hours_ago < 6:
                    score += 1  # 6å°æ—¶å†…çš„æ–°é—»
            except:
                pass

        return max(1, min(10, score))  # ç¡®ä¿åˆ†æ•°åœ¨1-10èŒƒå›´å†…

    def is_high_quality_content(self, article: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜è´¨é‡å†…å®¹"""
        title = article.get('title', '')
        content = article.get('content', '')

        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(content) < self.min_content_length:
            return False

        # é»‘åå•è¯æ±‡æ£€æŸ¥
        combined_text = f"{title} {content}".lower()
        for keyword in self.blacklist_keywords:
            if keyword in combined_text:
                return False

        # é‡å¤å†…å®¹æ£€æŸ¥
        content_hash = hashlib.md5(content.encode()).hexdigest()
        if content_hash in self.processed_urls:
            return False

        self.processed_urls.add(content_hash)
        return True

    async def fetch_rss_feed(self, session: aiohttp.ClientSession, feed_name: str, feed_url: str) -> List[Dict]:
        """è·å–RSSæºå†…å®¹"""
        articles = []

        try:
            print(f"ğŸ” è·å–RSSæº: {feed_name}")

            async with session.get(feed_url, timeout=30) as response:
                if response.status == 200:
                    rss_content = await response.text()
                    feed = feedparser.parse(rss_content)

                    if feed.entries:
                        print(f"   ğŸ“° æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")

                        for entry in feed.entries[:self.max_articles_per_feed]:
                            try:
                                # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                                article_content = await self.fetch_article_content(session, entry.link)

                                article = {
                                    'title': entry.title,
                                    'content': article_content or entry.summary,
                                    'url': entry.link,
                                    'source': feed_name,
                                    'published_time': getattr(entry, 'published', datetime.now().isoformat()),
                                    'summary': getattr(entry, 'summary', ''),
                                }

                                # è´¨é‡æ£€æŸ¥
                                if self.is_high_quality_content(article):
                                    article['importance'] = self.calculate_importance_score(article)
                                    article['keywords'] = self.extract_keywords(article['title'], article['content'])
                                    articles.append(article)

                            except Exception as e:
                                print(f"   âš ï¸ å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                                continue
                    else:
                        print(f"   âŒ RSSæºæ— æœ‰æ•ˆå†…å®¹")
                else:
                    print(f"   âŒ è·å–å¤±è´¥: HTTP {response.status}")

        except Exception as e:
            print(f"   âŒ RSSæºè·å–å¤±è´¥: {e}")

        return articles

    async def fetch_article_content(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """è·å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        try:
            async with session.get(url, timeout=15) as response:
                if response.status == 200:
                    html_content = await response.text()
                    # è¿™é‡Œå¯ä»¥æ·»åŠ HTMLè§£æé€»è¾‘æå–æ­£æ–‡
                    # ä¸ºç®€åŒ–ï¼Œç›´æ¥è¿”å›éƒ¨åˆ†HTMLå†…å®¹
                    return html_content[:2000]  # å‰2000å­—ç¬¦

        except Exception as e:
            print(f"   âš ï¸ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")

        return None

    async def collect_rss_articles(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰RSSæºçš„æ–‡ç« """
        all_articles = []

        print(f"ğŸš€ å¼€å§‹RSSç›‘æ§ä»»åŠ¡")
        print(f"ğŸ“¡ ç›‘æ§ {len(self.rss_feeds)} ä¸ªRSSæº")

        async with aiohttp.ClientSession() as session:
            tasks = []
            for feed_name, feed_url in self.rss_feeds.items():
                task = self.fetch_rss_feed(session, feed_name, feed_url)
                tasks.append(task)

            # å¹¶å‘è·å–æ‰€æœ‰RSSæº
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for i, result in enumerate(results):
                if isinstance(result, list):
                    all_articles.extend(result)
                    print(f"   âœ… {list(self.rss_feeds.keys())[i]}: {len(result)} ç¯‡æ–‡ç« ")
                else:
                    print(f"   âŒ {list(self.rss_feeds.keys())[i]}: {result}")

        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        all_articles.sort(key=lambda x: (x['importance'], x['published_time']), reverse=True)

        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡é«˜è´¨é‡æ–‡ç« ")
        return all_articles

    def store_articles_to_rag(self, articles: List[Dict]):
        """å°†æ–‡ç« å­˜å‚¨åˆ°RAGæ•°æ®åº“"""
        if not self.collection:
            print("âŒ RAGæ•°æ®åº“æœªè¿æ¥")
            return

        stored_count = 0

        for article in articles:
            try:
                # åˆ›å»ºæ–‡æ¡£ID
                doc_id = f"rss_{hashlib.md5(article['url'].encode()).hexdigest()[:12]}"

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                try:
                    existing = self.collection.get(ids=[doc_id])
                    if existing['ids']:
                        continue  # è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£
                except:
                    pass

                # å‡†å¤‡æ–‡æ¡£å†…å®¹
                document_content = f"""æ ‡é¢˜: {article['title']}

å†…å®¹: {article['content']}

æ‘˜è¦: {article['summary']}

å…³é”®è¯: {', '.join(article['keywords'])}"""

                # åˆ›å»ºå‘é‡
                embedding = self.create_tfidf_vector(document_content)

                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'source_type': 'rss_news',
                    'source_name': article['source'],
                    'url': article['url'],
                    'timestamp': datetime.now().isoformat(),
                    'published_time': article['published_time'],
                    'importance': article['importance'],
                    'keywords': json.dumps(article['keywords'], ensure_ascii=False),
                    'category': 'financial_news'
                }

                # å­˜å‚¨åˆ°ChromaDB
                self.collection.add(
                    documents=[document_content],
                    embeddings=[embedding],
                    metadatas=[metadata],
                    ids=[doc_id]
                )

                stored_count += 1

            except Exception as e:
                print(f"âš ï¸ å­˜å‚¨æ–‡ç« å¤±è´¥: {e}")
                continue

        print(f"âœ… æˆåŠŸå­˜å‚¨ {stored_count} ç¯‡æ–°æ–‡ç« åˆ°RAGæ•°æ®åº“")

    async def run_monitoring_cycle(self):
        """æ‰§è¡Œä¸€æ¬¡ç›‘æ§å‘¨æœŸ"""
        start_time = time.time()

        # æ”¶é›†RSSæ–‡ç« 
        articles = await self.collect_rss_articles()

        # å­˜å‚¨åˆ°RAGæ•°æ®åº“
        if articles:
            self.store_articles_to_rag(articles)

        end_time = time.time()
        duration = end_time - start_time

        print(f"â±ï¸ ç›‘æ§å‘¨æœŸå®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ”„ ä¸‹æ¬¡æ›´æ–°æ—¶é—´: {(datetime.now() + timedelta(minutes=self.update_interval)).strftime('%H:%M:%S')}")
        print("-" * 80)

    def start_monitoring(self):
        """å¯åŠ¨RSSç›‘æ§"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ¯ RSSç›‘æ§ç³»ç»Ÿå¯åŠ¨")
        print(f"â° æ›´æ–°é—´éš”: {self.update_interval} åˆ†é’Ÿ")
        print(f"ğŸ“¡ ç›‘æ§æºæ•°é‡: {len(self.rss_feeds)}")
        print("="*80)

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        asyncio.run(self.run_monitoring_cycle())

        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(self.update_interval).minutes.do(
            lambda: asyncio.run(self.run_monitoring_cycle())
        )

        print(f"âš¡ å®šæ—¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ RSSç›‘æ§å·²åœæ­¢")

    def run_once(self):
        """è¿è¡Œä¸€æ¬¡ç›‘æ§ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ§ª æ‰§è¡Œä¸€æ¬¡RSSç›‘æ§æµ‹è¯•")
        asyncio.run(self.run_monitoring_cycle())

def main():
    """ä¸»å‡½æ•°"""
    monitor = RSSMonitor()

    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        # æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œä¸€æ¬¡
        monitor.run_once()
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šæŒç»­ç›‘æ§
        monitor.start_monitoring()

if __name__ == "__main__":
    main()