#!/usr/bin/env python3
"""
éš”ç¦»çš„RSSç›‘æ§æœåŠ¡
- åªè´Ÿè´£RSSæ•°æ®æ”¶é›†
- ä½¿ç”¨ç‹¬ç«‹ç¿»è¯‘æœåŠ¡
- ç”ŸæˆJSONæ ¼å¼æ•°æ®ä¾›RAGæ¶ˆè´¹
- ä¸ç›´æ¥ä¾èµ–ChromaDB
"""

import asyncio
import aiohttp
import feedparser
import json
import os
import hashlib
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import re
from translation_service import TranslationService

class IsolatedRSSMonitor:
    def __init__(self):
        # åˆå§‹åŒ–ç¿»è¯‘æœåŠ¡
        self.translation_service = TranslationService()

        # RSSæºé…ç½® - ä½¿ç”¨å¯ç”¨çš„æºè¿›è¡Œæµ‹è¯•
        self.rss_feeds = {
            'BBC Business': 'http://feeds.bbci.co.uk/news/business/rss.xml',
            'CNN Business': 'http://rss.cnn.com/rss/money_latest.rss',
            'Reuters Business': 'https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best',
            'AP Business': 'https://apnews.com/apf-BusinessNews',
            'Financial Times': 'https://www.ft.com/rss/home',
            # å¤‡ç”¨ç®€å•æµ‹è¯•æº
            'Test News': 'https://rss-feed.com/feeds/all.rss.xml'
        }

        # æ•°æ®ç›®å½•
        self.data_dir = "/home/wyatt/prism2/rag-service/rss_data"
        os.makedirs(self.data_dir, exist_ok=True)

        self.stats = {
            'total_articles': 0,
            'successful_translations': 0,
            'failed_translations': 0,
            'cache_hits': 0,
            'rss_errors': 0
        }

    async def fetch_rss_content(self, source_name: str, url: str) -> List[Dict]:
        """è·å–RSSå†…å®¹"""
        articles = []

        try:
            print(f"ğŸ” è·å–RSSæº: {source_name}")

            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),
                headers={'User-Agent': 'Mozilla/5.0 (compatible; RSSBot/1.0)'}
            ) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        feed = feedparser.parse(content)

                        print(f"   ğŸ“° æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")

                        for entry in feed.entries:
                            article = self._process_entry(entry, source_name)
                            if article:
                                articles.append(article)
                                self.stats['total_articles'] += 1

                    else:
                        print(f"   âŒ HTTP {response.status}")
                        self.stats['rss_errors'] += 1

        except Exception as e:
            print(f"   âŒ RSSæºè·å–å¤±è´¥: {e}")
            self.stats['rss_errors'] += 1

        return articles

    def _process_entry(self, entry, source_name: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªRSSæ¡ç›®"""
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title = getattr(entry, 'title', '').strip()
            link = getattr(entry, 'link', '')
            published = getattr(entry, 'published', '')

            # æå–å†…å®¹
            content = ''
            if hasattr(entry, 'summary'):
                content = entry.summary
            elif hasattr(entry, 'description'):
                content = entry.description

            # æ¸…ç†HTMLæ ‡ç­¾
            if content:
                soup = BeautifulSoup(content, 'html.parser')
                content = soup.get_text().strip()

            # åŸºæœ¬éªŒè¯
            if not title or len(title) < 10:
                return None

            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(f"{link}{title}".encode()).hexdigest()

            # è®¡ç®—é‡è¦æ€§è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            importance_score = self._calculate_importance_score(title, content)

            return {
                'id': doc_id,
                'title': title,
                'content': content,
                'link': link,
                'source': source_name,
                'published': published,
                'importance_score': importance_score,
                'collected_at': datetime.now().isoformat(),
                'translated': False
            }

        except Exception as e:
            print(f"   âš ï¸ å¤„ç†æ¡ç›®å¤±è´¥: {e}")
            return None

    def _calculate_importance_score(self, title: str, content: str) -> float:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§è¯„åˆ†ï¼ˆ1-10ï¼‰"""
        score = 5.0  # åŸºç¡€åˆ†

        # æ ‡é¢˜é•¿åº¦æƒé‡
        if len(title) > 50:
            score += 0.5
        elif len(title) < 20:
            score -= 0.5

        # å†…å®¹é•¿åº¦æƒé‡
        if len(content) > 200:
            score += 1.0
        elif len(content) < 50:
            score -= 1.0

        # å…³é”®è¯æƒé‡
        important_keywords = [
            'federal reserve', 'å¤®è¡Œ', 'åˆ©ç‡', 'interest rate',
            'earnings', 'è´¢æŠ¥', 'revenue', 'æ”¶ç›Š',
            'stock', 'è‚¡ç¥¨', 'market', 'å¸‚åœº',
            'economy', 'ç»æµ', 'gdp', 'inflation', 'é€šèƒ€'
        ]

        text_lower = (title + ' ' + content).lower()
        keyword_count = sum(1 for keyword in important_keywords if keyword in text_lower)
        score += keyword_count * 0.5

        # é™åˆ¶åœ¨1-10èŒƒå›´å†…
        return max(1.0, min(10.0, score))

    def translate_articles(self, articles: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ç¿»è¯‘æ–‡ç« """
        translated_articles = []

        for article in articles:
            try:
                # ç¿»è¯‘æ ‡é¢˜
                title_result = self.translation_service.translate_text(article['title'])
                content_result = self.translation_service.translate_text(article['content'])

                # æ›´æ–°æ–‡ç« ä¿¡æ¯
                article['translated_title'] = title_result['translated_text']
                article['translated_content'] = content_result['translated_text']
                article['original_language'] = title_result['detected_language']
                article['translated'] = title_result['translation_needed'] or content_result['translation_needed']

                if title_result['success'] and content_result['success']:
                    self.stats['successful_translations'] += 1
                else:
                    self.stats['failed_translations'] += 1
                    # è®°å½•é”™è¯¯ä¿¡æ¯
                    if 'error' in title_result:
                        article['translation_error'] = title_result['error']
                    elif 'error' in content_result:
                        article['translation_error'] = content_result['error']

                translated_articles.append(article)

                # é¿å…ç¿»è¯‘APIé¢‘ç‡é™åˆ¶
                time.sleep(0.1)

            except Exception as e:
                print(f"   âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
                self.stats['failed_translations'] += 1
                article['translation_error'] = str(e)
                translated_articles.append(article)

        return translated_articles

    def save_to_file(self, articles: List[Dict], filename: str = None) -> str:
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rss_data_{timestamp}.json"

        filepath = os.path.join(self.data_dir, filename)

        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metadata': {
                'collection_time': datetime.now().isoformat(),
                'total_articles': len(articles),
                'sources_count': len(set(article['source'] for article in articles)),
                'translated_articles': sum(1 for article in articles if article.get('translated', False)),
                'stats': self.stats.copy()
            },
            'articles': articles
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        return filepath

    async def monitor_cycle(self) -> str:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ç›‘æ§å‘¨æœŸ"""
        print(f"ğŸš€ å¼€å§‹RSSç›‘æ§ä»»åŠ¡ - {datetime.now()}")

        all_articles = []

        # æ”¶é›†æ‰€æœ‰RSSæº
        for source_name, url in self.rss_feeds.items():
            articles = await self.fetch_rss_content(source_name, url)
            all_articles.extend(articles)

        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")

        if all_articles:
            # ç¿»è¯‘æ–‡ç« 
            print("ğŸ”„ å¼€å§‹ç¿»è¯‘...")
            translated_articles = self.translate_articles(all_articles)

            # ä¿å­˜æ•°æ®
            saved_file = self.save_to_file(translated_articles)
            print(f"ğŸ’¾ æ•°æ®ä¿å­˜åˆ°: {saved_file}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            translation_stats = self.translation_service.get_stats()
            print(f"ğŸ“ˆ ç¿»è¯‘ç»Ÿè®¡: æˆåŠŸ{self.stats['successful_translations']}, å¤±è´¥{self.stats['failed_translations']}")
            print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {translation_stats['cache_hit_rate']}")

            return saved_file
        else:
            print("âš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•æ–‡ç« ")
            return ""

    async def run_continuous_monitoring(self, interval_minutes: int = 15):
        """è¿è¡Œè¿ç»­ç›‘æ§"""
        print(f"â° å¯åŠ¨è¿ç»­RSSç›‘æ§ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")

        while True:
            try:
                await self.monitor_cycle()
                print(f"ğŸ˜´ ç­‰å¾… {interval_minutes} åˆ†é’Ÿ...")
                await asyncio.sleep(interval_minutes * 60)
            except KeyboardInterrupt:
                print("ğŸ›‘ ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"âŒ ç›‘æ§å‘¨æœŸé”™è¯¯: {e}")
                await asyncio.sleep(60)  # é”™è¯¯åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

async def main():
    """ä¸»å‡½æ•°"""
    import sys

    monitor = IsolatedRSSMonitor()

    if len(sys.argv) > 1:
        if sys.argv[1] == "once":
            # è¿è¡Œä¸€æ¬¡
            result = await monitor.monitor_cycle()
            print(f"âœ… å®Œæˆï¼Œä¿å­˜åœ¨: {result}")
        elif sys.argv[1] == "continuous":
            # è¿ç»­è¿è¡Œ
            interval = int(sys.argv[2]) if len(sys.argv) > 2 else 15
            await monitor.run_continuous_monitoring(interval)
        else:
            print("ç”¨æ³•: python3 isolated_rss_monitor.py [once|continuous [interval_minutes]]")
    else:
        # é»˜è®¤è¿è¡Œä¸€æ¬¡
        result = await monitor.monitor_cycle()
        print(f"âœ… å®Œæˆï¼Œä¿å­˜åœ¨: {result}")

if __name__ == "__main__":
    asyncio.run(main())