#!/usr/bin/env python3
"""
é«˜ä»·å€¼RAGæ„å»ºå™¨
ä¸“æ³¨æ”¶é›†çœŸæ­£æœ‰æŠ•èµ„ä»·å€¼çš„æ·±åº¦å†…å®¹
é¿å…å™ªéŸ³æ•°æ®ï¼Œæ„å»ºé«˜è´¨é‡çš„æŠ•èµ„å†³ç­–æ”¯æŒç³»ç»Ÿ
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

class HighValueRAGBuilder:
    """é«˜ä»·å€¼RAGæ„å»ºå™¨"""

    def __init__(self):
        self.vectorizer = None
        self.client = None
        self.collection = None
        self.high_value_docs = []

    async def __aenter__(self):
        print("ğŸ¯ åˆå§‹åŒ–é«˜ä»·å€¼RAGæ„å»ºç³»ç»Ÿ...")
        print("ç›®æ ‡ï¼šæ„å»ºçœŸæ­£æœ‰æŠ•èµ„ä»·å€¼çš„çŸ¥è¯†åº“")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–å‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=768,
            analyzer='word',
            tokenizer=lambda x: jieba.lcut(x),
            token_pattern=None,
            lowercase=False,
            stop_words=None
        )

        # è¿æ¥ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDB...")
        self.client = chromadb.HttpClient(host='localhost', port=8000)
        self.client.heartbeat()

        try:
            self.client.delete_collection("financial_documents")
            print("   ğŸ—‘ï¸ æ¸…ç†æ—§æ•°æ®")
        except:
            pass

        self.collection = self.client.create_collection("financial_documents")
        print("âœ… é«˜ä»·å€¼RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

    async def collect_company_fundamentals(self) -> List[Dict]:
        """æ”¶é›†å…¬å¸åŸºæœ¬é¢æ·±åº¦åˆ†æ"""
        print("ğŸ¢ æ”¶é›†å…¬å¸åŸºæœ¬é¢æ·±åº¦åˆ†æ...")

        try:
            import akshare as ak
            fundamental_docs = []

            # é‡ç‚¹å…³æ³¨çš„ä¼˜è´¨å…¬å¸
            key_companies = {
                '002230': {'name': 'ç§‘å¤§è®¯é£', 'industry': 'AIè¯­éŸ³', 'theme': 'äººå·¥æ™ºèƒ½é¾™å¤´'},
                '000725': {'name': 'äº¬ä¸œæ–¹A', 'industry': 'æ˜¾ç¤ºé¢æ¿', 'theme': 'åŠå¯¼ä½“æ˜¾ç¤º'},
                '002460': {'name': 'èµ£é”‹é”‚ä¸š', 'industry': 'é”‚ç”µææ–™', 'theme': 'æ–°èƒ½æºææ–™'},
                '300750': {'name': 'å®å¾·æ—¶ä»£', 'industry': 'åŠ¨åŠ›ç”µæ± ', 'theme': 'æ–°èƒ½æºé¾™å¤´'},
                '300229': {'name': 'æ‹“å°”æ€', 'industry': 'å¤§æ•°æ®', 'theme': 'AI+å¤§æ•°æ®'}
            }

            for stock_code, info in key_companies.items():
                try:
                    print(f"   åˆ†æ {info['name']} åŸºæœ¬é¢...")

                    # è·å–è´¢åŠ¡æ•°æ®
                    financial_data = ak.stock_financial_em(symbol=stock_code)

                    if not financial_data.empty:
                        latest_data = financial_data.iloc[0]

                        # æ„å»ºæœ‰ä»·å€¼çš„åŸºæœ¬é¢åˆ†æ
                        content = f"""
{info['name']}({stock_code}) åŸºæœ¬é¢æ·±åº¦åˆ†æ

ã€å…¬å¸æ¦‚å†µã€‘
{info['name']}æ˜¯{info['industry']}é¢†åŸŸçš„{info['theme']}ä¼ä¸šï¼Œåœ¨è¡Œä¸šä¸­å…·æœ‰é‡è¦åœ°ä½ã€‚

ã€è´¢åŠ¡å¥åº·åº¦åˆ†æã€‘
è¥ä¸šæ”¶å…¥ï¼š{latest_data.get('è¥ä¸šæ”¶å…¥', 'N/A')}ä¸‡å…ƒ
å‡€åˆ©æ¶¦ï¼š{latest_data.get('å‡€åˆ©æ¶¦', 'N/A')}ä¸‡å…ƒ
å‡€èµ„äº§æ”¶ç›Šç‡ï¼š{latest_data.get('å‡€èµ„äº§æ”¶ç›Šç‡', 'N/A')}%
èµ„äº§è´Ÿå€ºç‡ï¼š{latest_data.get('èµ„äº§è´Ÿå€ºç‡', 'N/A')}%
é”€å”®å‡€åˆ©ç‡ï¼š{latest_data.get('é”€å”®å‡€åˆ©ç‡', 'N/A')}%

ã€è´¢åŠ¡è´¨é‡è¯„ä¼°ã€‘
1. ç›ˆåˆ©èƒ½åŠ›ï¼š{self.assess_profitability(latest_data)}
2. å¿å€ºèƒ½åŠ›ï¼š{self.assess_solvency(latest_data)}
3. æˆé•¿æ€§ï¼š{self.assess_growth_potential(info['industry'])}
4. è¿è¥æ•ˆç‡ï¼š{self.assess_operational_efficiency(latest_data)}

ã€æŠ•èµ„äº®ç‚¹ã€‘
{self.generate_investment_highlights(info)}

ã€é£é™©å› ç´ ã€‘
{self.generate_risk_factors(info)}

ã€æŠ•èµ„å»ºè®®ã€‘
åŸºäºåŸºæœ¬é¢åˆ†æï¼Œè¯¥å…¬å¸{self.generate_investment_recommendation(info)}
                        """.strip()

                        doc = {
                            'id': f"fundamental_analysis_{stock_code}_{int(time.time())}",
                            'content': content,
                            'metadata': {
                                'source': f'åŸºæœ¬é¢ç ”ç©¶-{info["name"]}',
                                'doc_type': 'fundamental_analysis',
                                'stock_code': stock_code,
                                'company_name': info['name'],
                                'industry': info['industry'],
                                'analysis_type': 'æ·±åº¦åŸºæœ¬é¢åˆ†æ',
                                'data_source': 'akshare_financial_em',
                                'importance': 9,
                                'content_type': 'investment_analysis'
                            }
                        }
                        fundamental_docs.append(doc)
                        print(f"   âœ… {info['name']} åŸºæœ¬é¢åˆ†æå®Œæˆ")

                except Exception as e:
                    print(f"   âš ï¸ {info['name']} åŸºæœ¬é¢åˆ†æå¤±è´¥: {e}")

                await asyncio.sleep(2)  # é¿å…APIé¢‘ç‡é™åˆ¶

            print(f"âœ… å…¬å¸åŸºæœ¬é¢åˆ†æå®Œæˆ: {len(fundamental_docs)} ç¯‡")
            return fundamental_docs

        except Exception as e:
            print(f"âŒ åŸºæœ¬é¢åˆ†ææ”¶é›†å¤±è´¥: {e}")
            return []

    def assess_profitability(self, data) -> str:
        """è¯„ä¼°ç›ˆåˆ©èƒ½åŠ›"""
        try:
            roe = float(data.get('å‡€èµ„äº§æ”¶ç›Šç‡', 0))
            margin = float(data.get('é”€å”®å‡€åˆ©ç‡', 0))

            if roe > 15 and margin > 10:
                return "ä¼˜ç§€ï¼ŒROEå’Œå‡€åˆ©ç‡å‡å¤„äºè¡Œä¸šé¢†å…ˆæ°´å¹³"
            elif roe > 10 and margin > 5:
                return "è‰¯å¥½ï¼Œç›ˆåˆ©èƒ½åŠ›ç¨³å¥"
            else:
                return "ä¸€èˆ¬ï¼Œéœ€å…³æ³¨ç›ˆåˆ©èƒ½åŠ›æ”¹å–„"
        except:
            return "æ•°æ®ä¸è¶³ï¼Œéœ€è¿›ä¸€æ­¥åˆ†æ"

    def assess_solvency(self, data) -> str:
        """è¯„ä¼°å¿å€ºèƒ½åŠ›"""
        try:
            debt_ratio = float(data.get('èµ„äº§è´Ÿå€ºç‡', 0))

            if debt_ratio < 40:
                return "ä¼˜ç§€ï¼Œè´¢åŠ¡æ æ†è¾ƒä½ï¼Œå¿å€ºå‹åŠ›å°"
            elif debt_ratio < 60:
                return "è‰¯å¥½ï¼Œå€ºåŠ¡ç»“æ„åˆç†"
            else:
                return "éœ€å…³æ³¨ï¼Œå€ºåŠ¡æ°´å¹³è¾ƒé«˜"
        except:
            return "æ•°æ®ä¸è¶³ï¼Œéœ€è¿›ä¸€æ­¥åˆ†æ"

    def assess_growth_potential(self, industry) -> str:
        """è¯„ä¼°æˆé•¿æ½œåŠ›"""
        growth_sectors = {
            'AIè¯­éŸ³': 'äººå·¥æ™ºèƒ½è¡Œä¸šå¤„äºå¿«é€Ÿå‘å±•æœŸï¼Œè¯­éŸ³æŠ€æœ¯åº”ç”¨åœºæ™¯ä¸æ–­æ‹“å±•',
            'æ˜¾ç¤ºé¢æ¿': 'æ–°å‹æ˜¾ç¤ºæŠ€æœ¯å‡çº§ï¼ŒOLEDã€Mini LEDç­‰é«˜ç«¯äº§å“éœ€æ±‚å¢é•¿',
            'é”‚ç”µææ–™': 'æ–°èƒ½æºæ±½è½¦çˆ†å‘å¼å¢é•¿ï¼Œé”‚ç”µææ–™éœ€æ±‚å¼ºåŠ²',
            'åŠ¨åŠ›ç”µæ± ': 'å…¨çƒç”µåŠ¨åŒ–è¶‹åŠ¿ç¡®å®šï¼ŒåŠ¨åŠ›ç”µæ± å¸‚åœºç©ºé—´å·¨å¤§',
            'å¤§æ•°æ®': 'æ•°å­—ç»æµå‘å±•ï¼Œå¤§æ•°æ®å’ŒAIåº”ç”¨éœ€æ±‚æŒç»­å¢é•¿'
        }
        return growth_sectors.get(industry, 'è¡Œä¸šå‰æ™¯éœ€è¿›ä¸€æ­¥åˆ†æ')

    def assess_operational_efficiency(self, data) -> str:
        """è¯„ä¼°è¿è¥æ•ˆç‡"""
        return "éœ€ç»“åˆæ›´å¤šè¿è¥æŒ‡æ ‡ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬å­˜è´§å‘¨è½¬ç‡ã€åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡ç­‰"

    def generate_investment_highlights(self, info) -> str:
        """ç”ŸæˆæŠ•èµ„äº®ç‚¹"""
        highlights = {
            'AIè¯­éŸ³': '1.æŠ€æœ¯å£å’æ·±åš 2.ç”Ÿæ€å¸ƒå±€å®Œå–„ 3.æ”¿ç­–æ”¯æŒæ˜ç¡®',
            'æ˜¾ç¤ºé¢æ¿': '1.äº§èƒ½è§„æ¨¡é¢†å…ˆ 2.æŠ€æœ¯æŒç»­å‡çº§ 3.ä¸‹æ¸¸éœ€æ±‚ç¨³å®š',
            'é”‚ç”µææ–™': '1.èµ„æºå‚¨å¤‡ä¸°å¯Œ 2.äº§ä¸šé“¾ä¸€ä½“åŒ– 3.å…¨çƒåŒ–å¸ƒå±€',
            'åŠ¨åŠ›ç”µæ± ': '1.æŠ€æœ¯é¢†å…ˆä¼˜åŠ¿ 2.å®¢æˆ·èµ„æºä¼˜è´¨ 3.è§„æ¨¡æ•ˆåº”æ˜¾è‘—',
            'å¤§æ•°æ®': '1.æŠ€æœ¯ç§¯ç´¯æ·±åš 2.è¡Œä¸šåº”ç”¨å¹¿æ³› 3.AIèµ‹èƒ½æ•ˆæœæ˜æ˜¾'
        }
        return highlights.get(info['industry'], 'éœ€æ·±å…¥ç ”ç©¶å…·ä½“æŠ•èµ„äº®ç‚¹')

    def generate_risk_factors(self, info) -> str:
        """ç”Ÿæˆé£é™©å› ç´ """
        risks = {
            'AIè¯­éŸ³': 'æŠ€æœ¯è¿­ä»£é£é™©ã€ç«äº‰åŠ å‰§ã€æ”¿ç­–å˜åŒ–',
            'æ˜¾ç¤ºé¢æ¿': 'å‘¨æœŸæ€§æ³¢åŠ¨ã€æŠ€æœ¯æ›¿ä»£ã€äº§èƒ½è¿‡å‰©',
            'é”‚ç”µææ–™': 'åŸææ–™ä»·æ ¼æ³¢åŠ¨ã€æŠ€æœ¯è·¯çº¿å˜åŒ–ã€ç¯ä¿æ”¿ç­–',
            'åŠ¨åŠ›ç”µæ± ': 'æŠ€æœ¯è¿­ä»£ã€å®¢æˆ·é›†ä¸­ã€å®‰å…¨æ€§è¦æ±‚',
            'å¤§æ•°æ®': 'æ•°æ®å®‰å…¨ã€éšç§ä¿æŠ¤ã€æŠ€æœ¯æ›´æ–°'
        }
        return risks.get(info['industry'], 'éœ€è¯†åˆ«è¡Œä¸šç‰¹å®šé£é™©')

    def generate_investment_recommendation(self, info) -> str:
        """ç”ŸæˆæŠ•èµ„å»ºè®®"""
        recommendations = {
            'AIè¯­éŸ³': 'å»ºè®®é•¿æœŸå…³æ³¨ï¼Œåœ¨AIäº§ä¸šçˆ†å‘æœŸå…·å¤‡è¾ƒå¼ºçš„æŠ•èµ„ä»·å€¼',
            'æ˜¾ç¤ºé¢æ¿': 'å»ºè®®å…³æ³¨è¡Œä¸šå‘¨æœŸåº•éƒ¨çš„æŠ•èµ„æœºä¼š',
            'é”‚ç”µææ–™': 'å»ºè®®é‡ç‚¹é…ç½®ï¼Œå—ç›Šäºæ–°èƒ½æºäº§ä¸šé•¿æœŸå‘å±•',
            'åŠ¨åŠ›ç”µæ± ': 'å»ºè®®æ ¸å¿ƒæŒæœ‰ï¼Œå…¨çƒç”µåŠ¨åŒ–é¾™å¤´æ ‡çš„',
            'å¤§æ•°æ®': 'å»ºè®®å…³æ³¨AIåº”ç”¨è½åœ°å¸¦æ¥çš„ä¸šç»©å¢é•¿'
        }
        return recommendations.get(info['industry'], 'éœ€ç»“åˆå¸‚åœºç¯å¢ƒåˆ¶å®šæŠ•èµ„ç­–ç•¥')

    async def collect_industry_research(self) -> List[Dict]:
        """æ”¶é›†è¡Œä¸šç ”ç©¶æŠ¥å‘Š"""
        print("ğŸ­ æ”¶é›†æ·±åº¦è¡Œä¸šç ”ç©¶...")

        try:
            import akshare as ak
            research_docs = []

            # è·å–è¡Œä¸šèµ„é‡‘æµå‘åˆ†æ
            try:
                print("   åˆ†æè¡Œä¸šèµ„é‡‘æµå‘...")
                industry_flow = ak.stock_sector_fund_flow_rank(indicator="ä»Šæ—¥")

                if not industry_flow.empty:
                    # å–èµ„é‡‘å‡€æµå…¥å‰5çš„è¡Œä¸š
                    top_industries = industry_flow.head(5)

                    content = f"""
è¡Œä¸šèµ„é‡‘æµå‘æ·±åº¦åˆ†ææŠ¥å‘Š

ã€æ•°æ®æ—¶é—´ã€‘{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}

ã€æ ¸å¿ƒå‘ç°ã€‘
ä»Šæ—¥èµ„é‡‘å‡€æµå…¥å‰äº”å¤§è¡Œä¸šæ˜¾ç¤ºå¸‚åœºåå¥½è¶‹åŠ¿ï¼š

"""
                    for idx, row in top_industries.iterrows():
                        sector = row.get('è¡Œä¸š', 'æœªçŸ¥è¡Œä¸š')
                        net_inflow = row.get('å‡€æµå…¥', 0)
                        content += f"""
{idx+1}. {sector}
   å‡€æµå…¥èµ„é‡‘ï¼š{net_inflow}ä¸‡å…ƒ
   å¸‚åœºåˆ¤æ–­ï¼š{self.analyze_sector_flow(sector, net_inflow)}
"""

                    content += f"""

ã€æŠ•èµ„ç­–ç•¥åˆ†æã€‘
1. èµ„é‡‘æµå‘åæ˜ ï¼šå½“å‰å¸‚åœºå¯¹{top_industries.iloc[0]['è¡Œä¸š']}ç­‰è¡Œä¸šä¿¡å¿ƒè¾ƒå¼º
2. é…ç½®å»ºè®®ï¼šå…³æ³¨èµ„é‡‘æŒç»­æµå…¥çš„ä¼˜è´¨èµ›é“
3. é£é™©æç¤ºï¼šçŸ­æœŸèµ„é‡‘æµå‘å­˜åœ¨æ³¢åŠ¨ï¼Œéœ€ç»“åˆåŸºæœ¬é¢åˆ†æ

ã€åå¸‚å±•æœ›ã€‘
ç»“åˆè¡Œä¸šåŸºæœ¬é¢å’Œèµ„é‡‘æµå‘ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨å…·å¤‡é•¿æœŸæˆé•¿é€»è¾‘ä¸”è·å¾—èµ„é‡‘é’ççš„è¡Œä¸šé¾™å¤´æ ‡çš„ã€‚
                    """.strip()

                    doc = {
                        'id': f"industry_flow_analysis_{int(time.time())}",
                        'content': content,
                        'metadata': {
                            'source': 'è¡Œä¸šèµ„é‡‘æµå‘ç ”ç©¶',
                            'doc_type': 'industry_flow_analysis',
                            'analysis_date': datetime.now().strftime('%Y%m%d'),
                            'top_sector': top_industries.iloc[0]['è¡Œä¸š'],
                            'data_source': 'akshare_sector_fund_flow',
                            'importance': 8,
                            'content_type': 'market_analysis'
                        }
                    }
                    research_docs.append(doc)
                    print("   âœ… è¡Œä¸šèµ„é‡‘æµå‘åˆ†æå®Œæˆ")

            except Exception as e:
                print(f"   âš ï¸ è¡Œä¸šèµ„é‡‘æµå‘åˆ†æå¤±è´¥: {e}")

            # è·å–æ¦‚å¿µæ¿å—è¡¨ç°åˆ†æ
            try:
                print("   åˆ†æçƒ­ç‚¹æ¦‚å¿µæ¿å—...")
                concept_rank = ak.stock_board_concept_name_em()

                if not concept_rank.empty:
                    hot_concepts = concept_rank.head(10)

                    content = f"""
çƒ­ç‚¹æ¦‚å¿µæ¿å—æ·±åº¦è¿½è¸ªæŠ¥å‘Š

ã€å¸‚åœºçƒ­ç‚¹æ¦‚å¿µåˆ†æã€‘
å½“å‰å¸‚åœºå…³æ³¨çš„å‰åå¤§æ¦‚å¿µæ¿å—ï¼š

"""
                    for idx, row in hot_concepts.iterrows():
                        concept = row.get('æ¿å—åç§°', 'æœªçŸ¥æ¦‚å¿µ')
                        content += f"{idx+1}. {concept} - {self.analyze_concept_potential(concept)}\n"

                    content += f"""

ã€æŠ•èµ„é€»è¾‘åˆ†æã€‘
1. æ¦‚å¿µè½®åŠ¨ç‰¹å¾ï¼šå½“å‰å¸‚åœºå‘ˆç°æ˜æ˜¾çš„ä¸»é¢˜æŠ•èµ„ç‰¹å¾
2. æŒç»­æ€§åˆ¤æ–­ï¼šéœ€å…³æ³¨æ¦‚å¿µæ˜¯å¦å…·å¤‡åŸºæœ¬é¢æ”¯æ’‘
3. å‚ä¸ç­–ç•¥ï¼šå»ºè®®å…³æ³¨æœ‰å®è´¨ä¸šåŠ¡çš„æ¦‚å¿µé¾™å¤´

ã€é£é™©æç¤ºã€‘
æ¦‚å¿µæ¿å—æ³¢åŠ¨è¾ƒå¤§ï¼ŒæŠ•èµ„éœ€è°¨æ…ï¼Œå»ºè®®ä»¥åŸºæœ¬é¢ä¸ºæœ¬ï¼Œæ¦‚å¿µä¸ºè¾…çš„æŠ•èµ„ç­–ç•¥ã€‚
                    """.strip()

                    doc = {
                        'id': f"concept_analysis_{int(time.time())}",
                        'content': content,
                        'metadata': {
                            'source': 'æ¦‚å¿µæ¿å—ç ”ç©¶',
                            'doc_type': 'concept_analysis',
                            'hot_concepts': [row['æ¿å—åç§°'] for _, row in hot_concepts.iterrows()],
                            'data_source': 'akshare_concept_board',
                            'importance': 7,
                            'content_type': 'theme_analysis'
                        }
                    }
                    research_docs.append(doc)
                    print("   âœ… æ¦‚å¿µæ¿å—åˆ†æå®Œæˆ")

            except Exception as e:
                print(f"   âš ï¸ æ¦‚å¿µæ¿å—åˆ†æå¤±è´¥: {e}")

            print(f"âœ… è¡Œä¸šç ”ç©¶å®Œæˆ: {len(research_docs)} ç¯‡")
            return research_docs

        except Exception as e:
            print(f"âŒ è¡Œä¸šç ”ç©¶æ”¶é›†å¤±è´¥: {e}")
            return []

    def analyze_sector_flow(self, sector: str, net_inflow: float) -> str:
        """åˆ†æè¡Œä¸šèµ„é‡‘æµå‘"""
        if net_inflow > 100000:  # 10äº¿ä»¥ä¸Š
            return "å¼ºçƒˆçœ‹å¥½ï¼Œå¤§èµ„é‡‘æŒç»­æµå…¥"
        elif net_inflow > 50000:  # 5äº¿ä»¥ä¸Š
            return "ç§¯æå…³æ³¨ï¼Œèµ„é‡‘æµå…¥æ˜æ˜¾"
        elif net_inflow > 0:
            return "æ¸©å’Œçœ‹å¥½ï¼Œå°å¹…èµ„é‡‘æµå…¥"
        else:
            return "è°¨æ…è§‚æœ›ï¼Œå­˜åœ¨èµ„é‡‘æµå‡º"

    def analyze_concept_potential(self, concept: str) -> str:
        """åˆ†ææ¦‚å¿µæ½œåŠ›"""
        high_value_concepts = ['äººå·¥æ™ºèƒ½', 'æ–°èƒ½æº', 'åŠå¯¼ä½“', 'ç”Ÿç‰©åŒ»è¯', '5G']
        medium_value_concepts = ['äº‘è®¡ç®—', 'å¤§æ•°æ®', 'ç‰©è”ç½‘', 'æ–°ææ–™']

        if any(keyword in concept for keyword in high_value_concepts):
            return "é«˜ä»·å€¼æ¦‚å¿µï¼Œå…·å¤‡é•¿æœŸæŠ•èµ„é€»è¾‘"
        elif any(keyword in concept for keyword in medium_value_concepts):
            return "ä¸­ç­‰ä»·å€¼æ¦‚å¿µï¼Œéœ€å…³æ³¨è½åœ°æƒ…å†µ"
        else:
            return "éœ€æ·±å…¥åˆ†ææ¦‚å¿µçš„å•†ä¸šåŒ–å‰æ™¯"

    async def collect_macro_insights(self) -> List[Dict]:
        """æ”¶é›†å®è§‚ç»æµæ´å¯Ÿ"""
        print("ğŸ“Š æ”¶é›†å®è§‚ç»æµæ·±åº¦æ´å¯Ÿ...")

        try:
            import akshare as ak
            macro_docs = []

            # è·å–å®è§‚ç»æµæ•°æ®å¹¶æ·±åº¦åˆ†æ
            try:
                print("   åˆ†æå®è§‚ç»æµæŒ‡æ ‡...")

                # PMIæ•°æ®åˆ†æ
                pmi_data = ak.macro_china_pmi()
                if not pmi_data.empty:
                    latest_pmi = pmi_data.iloc[-1]
                    pmi_value = latest_pmi.get('PMI', 50)

                    content = f"""
ä¸­å›½å®è§‚ç»æµæ™¯æ°”åº¦æ·±åº¦åˆ†æ

ã€PMIæŒ‡æ ‡è§£è¯»ã€‘
æœ€æ–°PMIæ•°æ®ï¼š{pmi_value}
æ•°æ®æ—¶é—´ï¼š{latest_pmi.get('æœˆä»½', 'æœªçŸ¥æœˆä»½')}

ã€ç»æµæ™¯æ°”åº¦åˆ¤æ–­ã€‘
{self.interpret_pmi(pmi_value)}

ã€å¯¹è‚¡å¸‚å½±å“åˆ†æã€‘
1. åˆ¶é€ ä¸šå½±å“ï¼šPMIæ•°æ®ç›´æ¥åæ˜ åˆ¶é€ ä¸šæ™¯æ°”ç¨‹åº¦ï¼Œå½±å“å‘¨æœŸè‚¡è¡¨ç°
2. è´§å¸æ”¿ç­–é¢„æœŸï¼šPMIæ•°æ®å½±å“å¤®è¡Œè´§å¸æ”¿ç­–é¢„æœŸï¼Œè¿›è€Œå½±å“å¸‚åœºæµåŠ¨æ€§
3. è¡Œä¸šè½®åŠ¨ï¼šä¸åŒPMIæ°´å¹³ä¸‹ï¼Œå„è¡Œä¸šè¡¨ç°åˆ†åŒ–æ˜æ˜¾

ã€æŠ•èµ„ç­–ç•¥å»ºè®®ã€‘
åŸºäºå½“å‰PMIæ°´å¹³ï¼Œå»ºè®®ï¼š
{self.generate_pmi_strategy(pmi_value)}

ã€é£é™©æç¤ºã€‘
å®è§‚æ•°æ®å­˜åœ¨æ»åæ€§ï¼Œéœ€ç»“åˆå…¶ä»–å…ˆè¡ŒæŒ‡æ ‡ç»¼åˆåˆ¤æ–­ç»æµèµ°åŠ¿ã€‚
                    """.strip()

                    doc = {
                        'id': f"macro_pmi_analysis_{int(time.time())}",
                        'content': content,
                        'metadata': {
                            'source': 'å®è§‚ç»æµç ”ç©¶',
                            'doc_type': 'macro_analysis',
                            'indicator': 'PMI',
                            'pmi_value': float(pmi_value),
                            'analysis_period': str(latest_pmi.get('æœˆä»½', '')),
                            'data_source': 'akshare_macro_pmi',
                            'importance': 9,
                            'content_type': 'macro_insight'
                        }
                    }
                    macro_docs.append(doc)
                    print("   âœ… PMIæ·±åº¦åˆ†æå®Œæˆ")

            except Exception as e:
                print(f"   âš ï¸ PMIåˆ†æå¤±è´¥: {e}")

            print(f"âœ… å®è§‚æ´å¯Ÿå®Œæˆ: {len(macro_docs)} ç¯‡")
            return macro_docs

        except Exception as e:
            print(f"âŒ å®è§‚æ´å¯Ÿæ”¶é›†å¤±è´¥: {e}")
            return []

    def interpret_pmi(self, pmi_value: float) -> str:
        """è§£è¯»PMIæ•°æ®"""
        try:
            pmi = float(pmi_value)
            if pmi > 52:
                return "ç»æµæ‰©å¼ åŠ¿å¤´å¼ºåŠ²ï¼Œåˆ¶é€ ä¸šæ´»åŠ¨æ˜æ˜¾å›å‡ï¼Œåˆ©å¥½å‘¨æœŸè‚¡å’Œä»·å€¼è‚¡"
            elif pmi > 50:
                return "ç»æµæ¸©å’Œæ‰©å¼ ï¼Œåˆ¶é€ ä¸šä¿æŒå¢é•¿æ€åŠ¿ï¼Œå¸‚åœºæƒ…ç»ªç›¸å¯¹ç§¯æ"
            elif pmi > 48:
                return "ç»æµæ¥è¿‘è£æ¯çº¿ï¼Œåˆ¶é€ ä¸šæ´»åŠ¨è¶‹äºå¹³ç¨³ï¼Œéœ€å…³æ³¨æ”¿ç­–æ”¯æŒ"
            else:
                return "ç»æµæ”¶ç¼©å‹åŠ›è¾ƒå¤§ï¼Œåˆ¶é€ ä¸šæ´»åŠ¨ä½è¿·ï¼Œéœ€è­¦æƒ•ä¸‹è¡Œé£é™©"
        except:
            return "PMIæ•°æ®å¼‚å¸¸ï¼Œéœ€è¿›ä¸€æ­¥æ ¸å®"

    def generate_pmi_strategy(self, pmi_value: float) -> str:
        """åŸºäºPMIç”ŸæˆæŠ•èµ„ç­–ç•¥"""
        try:
            pmi = float(pmi_value)
            if pmi > 52:
                return "1.åŠ å¤§å‘¨æœŸè‚¡é…ç½® 2.å…³æ³¨æœ‰è‰²é‡‘å±ã€é’¢é“ç­‰è¡Œä¸š 3.é€‚åº¦å¢åŠ é£é™©åå¥½"
            elif pmi > 50:
                return "1.å‡è¡¡é…ç½®å‘¨æœŸå’Œæˆé•¿ 2.å…³æ³¨ä¸šç»©ç¡®å®šæ€§è¾ƒå¼ºçš„æ ‡çš„ 3.ä¿æŒè°¨æ…ä¹è§‚"
            else:
                return "1.é˜²å¾¡æ€§é…ç½®ä¸ºä¸» 2.å…³æ³¨æ¶ˆè´¹ã€åŒ»è¯ç­‰é˜²å¾¡å“ç§ 3.é™ä½é£é™©æ•å£"
        except:
            return "éœ€ç»“åˆå…¶ä»–æŒ‡æ ‡åˆ¶å®šæŠ•èµ„ç­–ç•¥"

    def store_to_vector_db(self, documents: List[Dict]) -> int:
        """å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        if not documents:
            return 0

        print(f"ğŸ§  å‘é‡åŒ–å­˜å‚¨ {len(documents)} ä¸ªé«˜ä»·å€¼æ–‡æ¡£...")

        try:
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            # å‘é‡åŒ–
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            embeddings = tfidf_matrix.toarray()

            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]}")

            # å­˜å‚¨
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=doc_ids
            )

            print(f"âœ… é«˜ä»·å€¼æ–‡æ¡£å­˜å‚¨å®Œæˆ: {len(documents)} ç¯‡")
            return len(documents)

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            return 0

    async def build_high_value_rag(self) -> List[Dict]:
        """æ„å»ºé«˜ä»·å€¼RAGç³»ç»Ÿ"""
        print("ğŸš€ å¼€å§‹æ„å»ºé«˜ä»·å€¼RAGç³»ç»Ÿ...")
        start_time = time.time()

        all_docs = []

        # 1. å…¬å¸åŸºæœ¬é¢æ·±åº¦åˆ†æ
        fundamental_docs = await self.collect_company_fundamentals()
        all_docs.extend(fundamental_docs)

        # 2. è¡Œä¸šç ”ç©¶æŠ¥å‘Š
        industry_docs = await self.collect_industry_research()
        all_docs.extend(industry_docs)

        # 3. å®è§‚ç»æµæ´å¯Ÿ
        macro_docs = await self.collect_macro_insights()
        all_docs.extend(macro_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ é«˜ä»·å€¼å†…å®¹æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ å†…å®¹åˆ†å¸ƒ:")
        print(f"      - åŸºæœ¬é¢åˆ†æ: {len(fundamental_docs)} ç¯‡")
        print(f"      - è¡Œä¸šç ”ç©¶: {len(industry_docs)} ç¯‡")
        print(f"      - å®è§‚æ´å¯Ÿ: {len(macro_docs)} ç¯‡")

        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if all_docs:
            stored_count = self.store_to_vector_db(all_docs)
            print(f"âœ… å‘é‡åŒ–å­˜å‚¨: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")

        # ä¿å­˜é«˜ä»·å€¼å†…å®¹
        output_file = f"/home/wyatt/prism2/rag-service/high_value_rag_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_docs, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ é«˜ä»·å€¼å†…å®¹å·²ä¿å­˜: {output_file}")

        return all_docs

async def test_high_value_rag():
    """æµ‹è¯•é«˜ä»·å€¼RAGæ„å»º"""
    print("ğŸ§ª æµ‹è¯•é«˜ä»·å€¼RAGæ„å»ºç³»ç»Ÿ")
    print("=" * 80)

    async with HighValueRAGBuilder() as builder:
        high_value_docs = await builder.build_high_value_rag()

        if high_value_docs:
            # éªŒè¯æ•°æ®åº“
            count = builder.collection.count()
            print(f"\nğŸ“Š æœ€ç»ˆRAGæ•°æ®åº“: {count} ä¸ªé«˜ä»·å€¼æ–‡æ¡£")

            print(f"\nâœ… é«˜ä»·å€¼RAGç‰¹æ€§:")
            print(f"   - æ·±åº¦åŸºæœ¬é¢åˆ†æï¼šå…¬å¸è´¢åŠ¡å¥åº·åº¦ã€æŠ•èµ„äº®ç‚¹ã€é£é™©å› ç´ ")
            print(f"   - è¡Œä¸šèµ„é‡‘æµå‘ï¼šå¸‚åœºåå¥½ã€æŠ•èµ„æœºä¼šã€é…ç½®å»ºè®®")
            print(f"   - å®è§‚ç»æµæ´å¯Ÿï¼šç»æµæ™¯æ°”åº¦ã€æ”¿ç­–å½±å“ã€æŠ•èµ„ç­–ç•¥")
            print(f"   - ä¸“ä¸šæŠ•èµ„é€»è¾‘ï¼šè€Œéç®€å•çš„ä»·æ ¼æ•°æ®å †ç Œ")

            return high_value_docs
        else:
            print("âŒ æ²¡æœ‰æ„å»ºæˆåŠŸ")
            return []

if __name__ == "__main__":
    asyncio.run(test_high_value_rag())