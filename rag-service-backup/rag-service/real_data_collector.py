#!/usr/bin/env python3
"""
çœŸå®æ•°æ®æ”¶é›†å™¨ + å‘é‡åŒ–å¤„ç†å™¨
å®ç°çœŸå®çš„é‡‘èæ•°æ®è·å–ï¼šAKShare + RSS + ç½‘é¡µçˆ¬å–
å¹¶ä½¿ç”¨çœŸå®çš„bge-large-zh-v1.5æ¨¡å‹è¿›è¡Œå‘é‡åŒ–å­˜å‚¨
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import re
import chromadb
from sentence_transformers import SentenceTransformer

class RealDataCollector:
    """çœŸå®æ•°æ®æ”¶é›†å™¨ + å‘é‡åŒ–å¤„ç†å™¨"""

    def __init__(self):
        self.session = None
        self.collected_data = []
        self.model = None
        self.client = None
        self.collection = None

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–çœŸå®æ•°æ®æ”¶é›†å’Œå‘é‡åŒ–ç³»ç»Ÿ...")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–çœŸå®çš„bge-large-zh-v1.5æ¨¡å‹
        print("ğŸ“¥ åŠ è½½çœŸå®çš„bge-large-zh-v1.5æ¨¡å‹...")
        try:
            self.model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
            print("âœ… bge-large-zh-v1.5æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½: {e}")
            try:
                # å¼ºåˆ¶ä¸‹è½½æ¨¡å‹
                self.model = SentenceTransformer('BAAI/bge-large-zh-v1.5', cache_folder='./models')
                print("âœ… bge-large-zh-v1.5æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: {e2}")
                raise e2

        # è¿æ¥åˆ°ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()
            self.collection = self.client.get_collection("financial_documents")
            print("âœ… ChromaDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ChromaDBè¿æ¥å¤±è´¥: {e}")
            raise e

        # è®¾ç½®HTTPä¼šè¯
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Linux; U; Android 4.0.3; ko-kr; LG-L160L Build/IML74K) AppleWebkit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'
            }
        )

        print("âœ… çœŸå®æ•°æ®æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def collect_akshare_data(self) -> List[Dict]:
        """ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®"""
        print("ğŸ“Š å¼€å§‹ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # 1. è·å–Aè‚¡å®æ—¶æ–°é—»
            print("   è·å–Aè‚¡æ–°é—»...")
            try:
                news_data = ak.news_cctv()  # å¤®è§†è´¢ç»æ–°é—»
                print(f"   âœ… è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                for _, news in news_data.head(5).iterrows():  # å–å‰5æ¡
                    doc = {
                        'id': f"akshare_news_{int(time.time())}_{len(collected_docs)}",
                        'content': f"{news.get('title', '')} {news.get('content', '')}",
                        'metadata': {
                            'source': 'AKShare-CCTVè´¢ç»',
                            'doc_type': 'financial_news',
                            'title': news.get('title', ''),
                            'publish_date': str(news.get('date', datetime.now().date())),
                            'data_source': 'akshare_cctv_news',
                            'importance': 7,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)

            except Exception as e:
                print(f"   âš ï¸ è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")

            # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            print("   è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            try:
                # è·å–æ²ªæ·±300æˆåˆ†è‚¡
                stock_info = ak.stock_zh_a_hist(symbol="000001", period="daily", start_date="20241101", end_date="20241101")
                if not stock_info.empty:
                    latest_data = stock_info.iloc[-1]
                    doc = {
                        'id': f"akshare_stock_000001_{int(time.time())}",
                        'content': f"å¹³å®‰é“¶è¡Œ(000001)æœ€æ–°äº¤æ˜“æ•°æ®ï¼šå¼€ç›˜ä»·{latest_data.get('å¼€ç›˜', 'N/A')}ï¼Œæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}ã€‚",
                        'metadata': {
                            'source': 'AKShare-è‚¡ç¥¨æ•°æ®',
                            'doc_type': 'stock_data',
                            'stock_code': '000001',
                            'company_name': 'å¹³å®‰é“¶è¡Œ',
                            'open_price': float(latest_data.get('å¼€ç›˜', 0)),
                            'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                            'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                            'date': str(latest_data.name),
                            'data_source': 'akshare_stock_hist',
                            'importance': 8,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)
                    print(f"   âœ… è·å–åˆ°å¹³å®‰é“¶è¡Œè‚¡ç¥¨æ•°æ®")

            except Exception as e:
                print(f"   âš ï¸ è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")

            # 3. è·å–å®è§‚ç»æµæ•°æ®
            print("   è·å–å®è§‚ç»æµæ•°æ®...")
            try:
                # è·å–è´§å¸ä¾›åº”é‡æ•°æ®
                macro_data = ak.macro_china_m2_yearly()
                if not macro_data.empty:
                    latest_m2 = macro_data.iloc[-1]
                    doc = {
                        'id': f"akshare_macro_m2_{int(time.time())}",
                        'content': f"ä¸­å›½è´§å¸ä¾›åº”é‡M2æ•°æ®ï¼š{latest_m2.get('å¹´ä»½', 'N/A')}å¹´M2åŒæ¯”å¢é•¿{latest_m2.get('M2åŒæ¯”å¢é•¿', 'N/A')}%ï¼Œåæ˜ è´§å¸æ”¿ç­–è¶‹åŠ¿ã€‚",
                        'metadata': {
                            'source': 'AKShare-å®è§‚æ•°æ®',
                            'doc_type': 'macro_economic',
                            'indicator': 'M2è´§å¸ä¾›åº”é‡',
                            'year': str(latest_m2.get('å¹´ä»½', '')),
                            'growth_rate': str(latest_m2.get('M2åŒæ¯”å¢é•¿', '')),
                            'data_source': 'akshare_macro_m2',
                            'importance': 9,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)
                    print(f"   âœ… è·å–åˆ°å®è§‚ç»æµæ•°æ®")

            except Exception as e:
                print(f"   âš ï¸ è·å–å®è§‚æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… AKShareæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…ï¼Œè·³è¿‡AKShareæ•°æ®æ”¶é›†")
            return []
        except Exception as e:
            print(f"âŒ AKShareæ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    async def collect_rss_data(self) -> List[Dict]:
        """æ”¶é›†RSSæ–°é—»æ•°æ®"""
        print("ğŸ“¡ å¼€å§‹æ”¶é›†RSSæ–°é—»æ•°æ®...")

        rss_feeds = [
            {
                'url': 'http://www.cs.com.cn/xwzx/hwxx/rss.xml',
                'name': 'ä¸­è¯ç½‘-æµ·å¤–æ–°é—»',
                'category': 'overseas_news'
            },
            {
                'url': 'http://finance.sina.com.cn/roll/index.d.html?cid=56588&page=1',
                'name': 'æ–°æµªè´¢ç»',
                'category': 'financial_news'
            }
        ]

        collected_docs = []

        for feed_info in rss_feeds:
            try:
                print(f"   å¤„ç†RSSæº: {feed_info['name']}")

                # å°è¯•è§£æRSS
                feed = feedparser.parse(feed_info['url'])

                if feed.entries:
                    print(f"   âœ… è·å–åˆ° {len(feed.entries)} ä¸ªRSSæ¡ç›®")

                    for i, entry in enumerate(feed.entries[:3]):  # å–å‰3æ¡
                        # æ¸…ç†HTMLæ ‡ç­¾
                        content = BeautifulSoup(entry.get('summary', entry.get('description', '')), 'html.parser').get_text()

                        doc = {
                            'id': f"rss_{feed_info['category']}_{int(time.time())}_{i}",
                            'content': f"{entry.get('title', '')} {content}",
                            'metadata': {
                                'source': feed_info['name'],
                                'doc_type': 'rss_news',
                                'category': feed_info['category'],
                                'title': entry.get('title', ''),
                                'link': entry.get('link', ''),
                                'published': entry.get('published', ''),
                                'rss_url': feed_info['url'],
                                'importance': 6,
                                'collection_time': datetime.now().isoformat()
                            }
                        }
                        collected_docs.append(doc)

                else:
                    print(f"   âš ï¸ RSSæºæ— æ•°æ®: {feed_info['name']}")

            except Exception as e:
                print(f"   âŒ RSSæºå¤„ç†å¤±è´¥ {feed_info['name']}: {e}")

        print(f"âœ… RSSæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
        return collected_docs

    async def collect_web_scraping_data(self) -> List[Dict]:
        """ç½‘é¡µçˆ¬å–æ•°æ®"""
        print("ğŸ•·ï¸ å¼€å§‹ç½‘é¡µçˆ¬å–æ•°æ®...")

        web_sources = [
            {
                'url': 'https://www.eastmoney.com/',
                'name': 'ä¸œæ–¹è´¢å¯Œç½‘',
                'type': 'financial_portal'
            },
            {
                'url': 'https://finance.sina.com.cn/',
                'name': 'æ–°æµªè´¢ç»',
                'type': 'financial_news'
            }
        ]

        collected_docs = []

        if not self.session:
            print("âŒ HTTPä¼šè¯æœªåˆå§‹åŒ–")
            return collected_docs

        for source in web_sources:
            try:
                print(f"   çˆ¬å–ç½‘ç«™: {source['name']}")

                async with self.session.get(source['url']) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        soup = BeautifulSoup(html_content, 'html.parser')

                        # æå–æ ‡é¢˜å’Œæ‘˜è¦
                        titles = soup.find_all(['h1', 'h2', 'h3'], limit=5)
                        for i, title in enumerate(titles):
                            title_text = title.get_text(strip=True)
                            if len(title_text) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ ‡é¢˜

                                # å°è¯•æ‰¾åˆ°ç›¸å…³æ®µè½
                                content = title_text
                                next_elem = title.find_next(['p', 'div'], class_=lambda x: x and 'content' in str(x).lower())
                                if next_elem:
                                    content += " " + next_elem.get_text(strip=True)[:200]

                                doc = {
                                    'id': f"webscrape_{source['type']}_{int(time.time())}_{i}",
                                    'content': content,
                                    'metadata': {
                                        'source': source['name'],
                                        'doc_type': 'web_scraped',
                                        'website_type': source['type'],
                                        'url': source['url'],
                                        'title': title_text,
                                        'scrape_time': datetime.now().isoformat(),
                                        'importance': 5,
                                        'collection_time': datetime.now().isoformat()
                                    }
                                }
                                collected_docs.append(doc)

                        print(f"   âœ… ä» {source['name']} æå–äº† {len([t for t in titles if len(t.get_text(strip=True)) > 10])} ä¸ªå†…å®¹")

                    else:
                        print(f"   âš ï¸ ç½‘ç«™è®¿é—®å¤±è´¥ {source['name']}: HTTP {response.status}")

            except Exception as e:
                print(f"   âŒ ç½‘ç«™çˆ¬å–å¤±è´¥ {source['name']}: {e}")

        print(f"âœ… ç½‘é¡µçˆ¬å–å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
        return collected_docs

    def store_documents_to_vector_db(self, documents: List[Dict]) -> int:
        """å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨çœŸå®çš„bge-large-zh-v1.5æ¨¡å‹"""
        if not documents:
            print("âš ï¸ æ²¡æœ‰æ–‡æ¡£éœ€è¦å­˜å‚¨")
            return 0

        print(f"ğŸ§  å¼€å§‹ä½¿ç”¨çœŸå®bge-large-zh-v1.5æ¨¡å‹å‘é‡åŒ– {len(documents)} ä¸ªæ–‡æ¡£...")

        try:
            # æå–æ–‡æ¡£å†…å®¹ç”¨äºå‘é‡åŒ–
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            # ä½¿ç”¨çœŸå®çš„bge-large-zh-v1.5æ¨¡å‹ç”Ÿæˆå‘é‡
            print("   ğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...")
            start_time = time.time()
            embeddings = self.model.encode(texts, normalize_embeddings=True)
            embedding_time = time.time() - start_time

            print(f"   âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’")
            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]} (åº”è¯¥æ˜¯1024ç»´)")

            # å­˜å‚¨åˆ°ChromaDB
            print("   ğŸ’¾ å­˜å‚¨åˆ°ChromaDB...")
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=doc_ids
            )

            print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªçœŸå®æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return len(documents)

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    async def collect_all_real_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®æº...")
        start_time = time.time()

        all_docs = []

        # 1. AKShareæ•°æ® (æ— éœ€å¼‚æ­¥)
        akshare_docs = await self.collect_akshare_data()
        all_docs.extend(akshare_docs)

        # 2. RSSæ•°æ®
        rss_docs = await self.collect_rss_data()
        all_docs.extend(rss_docs)

        # 3. ç½‘é¡µçˆ¬å–æ•°æ®
        web_docs = await self.collect_web_scraping_data()
        all_docs.extend(web_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ çœŸå®æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ æ•°æ®æºåˆ†å¸ƒ:")
        print(f"      - AKShare: {len(akshare_docs)} ä¸ª")
        print(f"      - RSS: {len(rss_docs)} ä¸ª")
        print(f"      - ç½‘é¡µçˆ¬å–: {len(web_docs)} ä¸ª")

        # ç«‹å³è¿›è¡Œå‘é‡åŒ–å­˜å‚¨
        if all_docs:
            print(f"\nğŸ§  å¼€å§‹å‘é‡åŒ–å­˜å‚¨æµç¨‹...")
            stored_count = self.store_documents_to_vector_db(all_docs)
            print(f"âœ… å‘é‡åŒ–å­˜å‚¨å®Œæˆ: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")
        else:
            print("âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡å‘é‡åŒ–å­˜å‚¨")

        return all_docs

async def test_real_data_collection():
    """æµ‹è¯•çœŸå®æ•°æ®æ”¶é›† + å‘é‡åŒ–å­˜å‚¨å®Œæ•´æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•çœŸå®æ•°æ®æ”¶é›† + å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿ")
    print("=" * 80)

    async with RealDataCollector() as collector:
        # æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®å¹¶è‡ªåŠ¨å‘é‡åŒ–å­˜å‚¨
        real_docs = await collector.collect_all_real_data()

        if real_docs:
            print(f"\nğŸ“‹ æ”¶é›†åˆ°çš„çœŸå®æ•°æ®é¢„è§ˆ:")
            print("=" * 60)

            for i, doc in enumerate(real_docs[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"\nğŸ“„ æ–‡æ¡£ {i}: {doc['id']}")
                print(f"   æ¥æº: {doc['metadata']['source']}")
                print(f"   ç±»å‹: {doc['metadata']['doc_type']}")
                print(f"   å†…å®¹: {doc['content'][:150]}...")
                print("-" * 50)

            # ä¿å­˜åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
            output_file = f"/home/wyatt/prism2/rag-service/real_data_collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(real_docs, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

            # éªŒè¯æ•°æ®åº“ä¸­çš„å†…å®¹
            print(f"\nğŸ” éªŒè¯ChromaDBä¸­çš„çœŸå®å‘é‡åŒ–æ•°æ®:")
            print("=" * 60)

            try:
                count = collector.collection.count()
                print(f"   ğŸ“Š æ•°æ®åº“ä¸­æ€»æ–‡æ¡£æ•°: {count}")

                # è·å–æœ€æ–°å­˜å‚¨çš„æ–‡æ¡£
                recent_results = collector.collection.get(
                    include=['documents', 'metadatas', 'embeddings'],
                    limit=3
                )

                for i, doc_id in enumerate(recent_results['ids']):
                    metadata = recent_results['metadatas'][i] if recent_results['metadatas'] else {}
                    embedding_dim = len(recent_results['embeddings'][i]) if recent_results['embeddings'] and recent_results['embeddings'][i] else 0

                    print(f"   ğŸ“„ å‘é‡åŒ–æ–‡æ¡£ {i+1}: {doc_id}")
                    print(f"      å‘é‡ç»´åº¦: {embedding_dim}")
                    print(f"      æ¥æº: {metadata.get('source', 'æœªçŸ¥')}")
                    print(f"      ç±»å‹: {metadata.get('doc_type', 'æœªçŸ¥')}")

            except Exception as e:
                print(f"   âŒ éªŒè¯æ•°æ®åº“å¤±è´¥: {e}")

            return real_docs
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•çœŸå®æ•°æ®")
            return []

if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„ä¾èµ–
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…...")
    try:
        import akshare
        print("âœ… AKShare å·²å®‰è£…")
    except ImportError:
        print("âš ï¸ AKShare æœªå®‰è£…ï¼Œå°†è·³è¿‡AKShareæ•°æ®æ”¶é›†")

    try:
        import feedparser
        print("âœ… feedparser å·²å®‰è£…")
    except ImportError:
        print("âŒ feedparser æœªå®‰è£…ï¼Œå®‰è£…ä¸­...")
        os.system("pip install feedparser")

    try:
        import aiohttp
        print("âœ… aiohttp å·²å®‰è£…")
    except ImportError:
        print("âŒ aiohttp æœªå®‰è£…ï¼Œå®‰è£…ä¸­...")
        os.system("pip install aiohttp")

    try:
        from bs4 import BeautifulSoup
        print("âœ… BeautifulSoup å·²å®‰è£…")
    except ImportError:
        print("âŒ BeautifulSoup æœªå®‰è£…ï¼Œå®‰è£…ä¸­...")
        os.system("pip install beautifulsoup4")

    # è¿è¡Œæ•°æ®æ”¶é›†æµ‹è¯•
    asyncio.run(test_real_data_collection())