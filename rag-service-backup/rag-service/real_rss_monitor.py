#!/usr/bin/env python3
"""
çœŸå®RSSç›‘æ§ç³»ç»Ÿ
ä½¿ç”¨çœŸå®çš„Bloombergã€å•†åŠ¡éƒ¨ã€è·¯é€ç¤¾ç­‰æ•°æ®æº
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import chromadb
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import hashlib
import time
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
import schedule
import threading
from bs4 import BeautifulSoup
import re
from archive_manager import ArchiveManager

class RealRSSMonitor:
    def __init__(self):
        # Clear proxy variables
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        self.client = None
        self.collection = None
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.archive_manager = ArchiveManager()

        # çœŸå®RSSæºé…ç½®
        self.rss_feeds = {
            # Bloomberg è´¢ç»æ–°é—»
            'Bloomberg Markets': 'https://feeds.bloomberg.com/markets/news.rss',
            'Bloomberg Economics': 'https://feeds.bloomberg.com/economics/news.rss',
            'Bloomberg Industries': 'https://feeds.bloomberg.com/industries/news.rss',
            'Bloomberg Green': 'https://feeds.bloomberg.com/green/news.rss',

            # ä¸­å›½å®˜æ–¹æ•°æ®æº
            'å•†åŠ¡éƒ¨': 'https://www.mofcom.gov.cn/zfxxgk/index.html',

            # è·¯é€ç¤¾
            'Thomson Reuters': 'https://ir.thomsonreuters.com/rss/news-releases.xml?items=15',

            # å’Œè®¯è´¢ç»
            'å’Œè®¯è´¢ç»': 'https://news.hexun.com/rss/',

            # å¤‡ç”¨æº
            'æ–°æµªè´¢ç»RSS': 'https://finance.sina.com.cn/roll/index.d.html?cid=56588',
            'ç½‘æ˜“è´¢ç»RSS': 'http://money.163.com/special/002557S6/rss_newstop.xml'
        }

        # ç›‘æ§é…ç½®
        self.update_interval = 15  # 15åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
        self.max_articles_per_feed = 25  # æ¯ä¸ªæºæœ€å¤šè·å–25ç¯‡æ–‡ç« 
        self.processed_urls = set()  # å·²å¤„ç†çš„URL
        self.session_timeout = 30  # è¯·æ±‚è¶…æ—¶30ç§’

        # æ•°æ®è´¨é‡è¿‡æ»¤é…ç½®
        self.min_content_length = 80  # æœ€å°å†…å®¹é•¿åº¦
        self.blacklist_keywords = ['å¹¿å‘Š', 'æ¨å¹¿', 'å‹æƒ…é“¾æ¥', 'ç‰ˆæƒå£°æ˜', 'advertisement']

    def connect_to_chromadb(self):
        """è¿æ¥åˆ°ChromaDB"""
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()

            collection_name = "real_financial_news"
            try:
                self.collection = self.client.get_collection(collection_name)
                print(f"ğŸ“ ä½¿ç”¨ç°æœ‰é›†åˆ: {collection_name}")
            except Exception:
                self.collection = self.client.create_collection(
                    name=collection_name,
                    metadata={"description": "Real financial news from RSS feeds"}
                )
                print(f"ğŸ“ åˆ›å»ºæ–°é›†åˆ: {collection_name}")

            print("âœ… è¿æ¥åˆ°ChromaDBæˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ è¿æ¥ChromaDBå¤±è´¥: {e}")
            return False

    def chinese_tokenize(self, text: str) -> str:
        """ä¸­æ–‡åˆ†è¯"""
        return ' '.join(jieba.cut(text))

    def create_tfidf_vector(self, text: str) -> List[float]:
        """åˆ›å»ºTF-IDFå‘é‡"""
        try:
            processed_text = self.chinese_tokenize(text)
            try:
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
            except:
                # é‡æ–°è®­ç»ƒå‘é‡åŒ–å™¨
                self.vectorizer.fit([processed_text])
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            return [0.0] * 1000

    def clean_html_content(self, html_content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
        if not html_content:
            return ""

        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()

            # è·å–çº¯æ–‡æœ¬
            text = soup.get_text()

            # æ¸…ç†ç©ºç™½å­—ç¬¦
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            return text

        except Exception as e:
            print(f"âš ï¸ HTMLæ¸…ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çš„æ­£åˆ™æ¸…ç†
            text = re.sub(r'<[^>]+>', '', html_content)
            text = re.sub(r'\s+', ' ', text).strip()
            return text

    def extract_keywords(self, title: str, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        combined_text = f"{title} {content}"

        # è‹±æ–‡å…³é”®è¯æå–
        english_keywords = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', combined_text)

        # ä¸­æ–‡å…³é”®è¯æå–
        chinese_words = jieba.cut(combined_text)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        chinese_keywords = [word for word in chinese_words if len(word) > 1 and word not in stop_words]

        # åˆå¹¶å¹¶å»é‡
        all_keywords = list(set(english_keywords + chinese_keywords))
        return all_keywords[:15]  # è¿”å›å‰15ä¸ªå…³é”®è¯

    def calculate_importance_score(self, article: Dict) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§åˆ†æ•° (1-10)"""
        score = 5  # åŸºç¡€åˆ†æ•°

        title = article.get('title', '').lower()
        content = article.get('content', '').lower()
        source = article.get('source', '').lower()

        # é‡è¦å…³é”®è¯åŠ åˆ†
        important_keywords = {
            # è‹±æ–‡å…³é”®è¯
            'fed': 3, 'federal reserve': 3, 'interest rate': 2, 'inflation': 2,
            'gdp': 2, 'unemployment': 2, 'recession': 2, 'market': 1,
            'stock': 1, 'bond': 1, 'currency': 1, 'oil': 1, 'gold': 1,
            'china': 2, 'us': 1, 'europe': 1, 'asia': 1,
            'earnings': 2, 'revenue': 1, 'profit': 1, 'loss': 1,
            'merger': 2, 'acquisition': 2, 'ipo': 2, 'dividend': 1,

            # ä¸­æ–‡å…³é”®è¯
            'å¤®è¡Œ': 3, 'åˆ©ç‡': 2, 'GDP': 2, 'PMI': 2, 'é€šèƒ€': 2,
            'è‚¡å¸‚': 2, 'åŸºé‡‘': 1, 'å€ºåˆ¸': 1, 'å¤–æ±‡': 1,
            'æ”¿ç­–': 2, 'ç›‘ç®¡': 2, 'æ”¹é©': 2, 'åˆ›æ–°': 1,
            'é£é™©': 1, 'æœºä¼š': 1, 'æŠ•èµ„': 2, 'æ”¶ç›Š': 1,
            'ä¸šç»©': 2, 'è´¢æŠ¥': 2, 'å¹¶è´­': 2, 'ä¸Šå¸‚': 2
        }

        for keyword, points in important_keywords.items():
            if keyword in title:
                score += points * 2  # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
            elif keyword in content:
                score += points

        # æ•°æ®æºæƒé‡
        source_weights = {
            'bloomberg': 2,
            'å•†åŠ¡éƒ¨': 3,
            'thomson reuters': 2,
            'å’Œè®¯': 1
        }

        for src, weight in source_weights.items():
            if src in source:
                score += weight

        # å†…å®¹é•¿åº¦è°ƒæ•´
        content_length = len(content)
        if content_length > 1000:
            score += 2
        elif content_length > 500:
            score += 1
        elif content_length < 100:
            score -= 2

        # æ—¶æ•ˆæ€§è°ƒæ•´
        pub_time = article.get('published_time')
        if pub_time:
            try:
                if isinstance(pub_time, str):
                    pub_datetime = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                else:
                    pub_datetime = pub_time

                hours_ago = (datetime.now() - pub_datetime.replace(tzinfo=None)).total_seconds() / 3600

                if hours_ago < 1:
                    score += 3  # 1å°æ—¶å†…çš„æ–°é—»
                elif hours_ago < 6:
                    score += 2  # 6å°æ—¶å†…çš„æ–°é—»
                elif hours_ago < 24:
                    score += 1  # 24å°æ—¶å†…çš„æ–°é—»
            except:
                pass

        return max(1, min(10, score))  # ç¡®ä¿åˆ†æ•°åœ¨1-10èŒƒå›´å†…

    def is_high_quality_content(self, article: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜è´¨é‡å†…å®¹"""
        title = article.get('title', '')
        content = article.get('content', '')
        url = article.get('url', '')

        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(content) < self.min_content_length:
            return False

        # æ ‡é¢˜ä¸èƒ½ä¸ºç©º
        if not title.strip():
            return False

        # é»‘åå•è¯æ±‡æ£€æŸ¥
        combined_text = f"{title} {content}".lower()
        for keyword in self.blacklist_keywords:
            if keyword in combined_text:
                return False

        # é‡å¤å†…å®¹æ£€æŸ¥
        content_hash = hashlib.md5(f"{title}{content}".encode()).hexdigest()
        if content_hash in self.processed_urls:
            return False

        # URLé‡å¤æ£€æŸ¥
        if url in self.processed_urls:
            return False

        self.processed_urls.add(content_hash)
        self.processed_urls.add(url)
        return True

    async def fetch_rss_feed(self, session: aiohttp.ClientSession, feed_name: str, feed_url: str) -> List[Dict]:
        """è·å–RSSæºå†…å®¹"""
        articles = []

        try:
            print(f"ğŸ” è·å–RSSæº: {feed_name}")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Cache-Control': 'no-cache'
            }

            async with session.get(feed_url, timeout=self.session_timeout, headers=headers) as response:
                if response.status == 200:
                    rss_content = await response.text()

                    # å°è¯•è§£æRSS
                    feed = feedparser.parse(rss_content)

                    if feed.entries:
                        print(f"   ğŸ“° æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")

                        for entry in feed.entries[:self.max_articles_per_feed]:
                            try:
                                # è·å–å‘å¸ƒæ—¶é—´
                                pub_time = None
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6])
                                elif hasattr(entry, 'published'):
                                    try:
                                        pub_time = datetime.fromisoformat(entry.published.replace('Z', '+00:00'))
                                    except:
                                        pub_time = datetime.now()
                                else:
                                    pub_time = datetime.now()

                                # è·å–å†…å®¹
                                content = ""
                                if hasattr(entry, 'content') and entry.content:
                                    content = self.clean_html_content(entry.content[0].value)
                                elif hasattr(entry, 'summary'):
                                    content = self.clean_html_content(entry.summary)
                                elif hasattr(entry, 'description'):
                                    content = self.clean_html_content(entry.description)

                                # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•è·å–å…¨æ–‡
                                if len(content) < 200 and hasattr(entry, 'link'):
                                    try:
                                        full_content = await self.fetch_article_content(session, entry.link)
                                        if full_content and len(full_content) > len(content):
                                            content = full_content
                                    except:
                                        pass

                                article = {
                                    'title': getattr(entry, 'title', 'No Title'),
                                    'content': content,
                                    'url': getattr(entry, 'link', ''),
                                    'source': feed_name,
                                    'published_time': pub_time.isoformat() if pub_time else datetime.now().isoformat(),
                                    'summary': getattr(entry, 'summary', '')[:200] + '...' if hasattr(entry, 'summary') else '',
                                }

                                # è´¨é‡æ£€æŸ¥
                                if self.is_high_quality_content(article):
                                    article['importance'] = self.calculate_importance_score(article)
                                    article['keywords'] = self.extract_keywords(article['title'], article['content'])
                                    articles.append(article)
                                    print(f"   âœ… æ”¶é›†: {article['title'][:50]}...")

                            except Exception as e:
                                print(f"   âš ï¸ å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                                continue
                    else:
                        print(f"   âŒ RSSæºæ— æœ‰æ•ˆå†…å®¹")
                else:
                    print(f"   âŒ è·å–å¤±è´¥: HTTP {response.status}")

        except asyncio.TimeoutError:
            print(f"   â° RSSæºè¶…æ—¶: {feed_name}")
        except Exception as e:
            print(f"   âŒ RSSæºè·å–å¤±è´¥: {e}")

        return articles

    async def fetch_article_content(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """è·å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            async with session.get(url, timeout=15, headers=headers) as response:
                if response.status == 200:
                    html_content = await response.text()
                    clean_content = self.clean_html_content(html_content)
                    return clean_content[:2000]  # é™åˆ¶é•¿åº¦

        except Exception as e:
            print(f"   âš ï¸ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")

        return None

    async def collect_real_rss_articles(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰RSSæºçš„çœŸå®æ–‡ç« """
        all_articles = []

        print(f"ğŸš€ å¼€å§‹çœŸå®RSSç›‘æ§ä»»åŠ¡")
        print(f"ğŸ“¡ ç›‘æ§ {len(self.rss_feeds)} ä¸ªçœŸå®RSSæº")
        print("-" * 70)

        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=60)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            for feed_name, feed_url in self.rss_feeds.items():
                task = self.fetch_rss_feed(session, feed_name, feed_url)
                tasks.append(task)

            # å¹¶å‘è·å–æ‰€æœ‰RSSæºï¼Œä½†é™åˆ¶å¹¶å‘æ•°
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for i, result in enumerate(results):
                feed_name = list(self.rss_feeds.keys())[i]
                if isinstance(result, list):
                    all_articles.extend(result)
                    print(f"   âœ… {feed_name}: {len(result)} ç¯‡æ–‡ç« ")
                else:
                    print(f"   âŒ {feed_name}: {result}")

        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        all_articles.sort(key=lambda x: (x['importance'], x['published_time']), reverse=True)

        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡çœŸå®è´¢ç»æ–‡ç« ")
        return all_articles

    def store_articles_to_rag(self, articles: List[Dict]):
        """å°†æ–‡ç« å­˜å‚¨åˆ°RAGæ•°æ®åº“"""
        if not self.collection:
            print("âŒ RAGæ•°æ®åº“æœªè¿æ¥")
            return

        stored_count = 0

        for article in articles:
            try:
                # åˆ›å»ºæ–‡æ¡£ID
                doc_id = f"real_rss_{hashlib.md5(article['url'].encode()).hexdigest()[:12]}"

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                try:
                    existing = self.collection.get(ids=[doc_id])
                    if existing['ids']:
                        continue  # è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£
                except:
                    pass

                # å‡†å¤‡æ–‡æ¡£å†…å®¹
                document_content = f"""æ ‡é¢˜: {article['title']}

å†…å®¹: {article['content']}

æ‘˜è¦: {article['summary']}

å…³é”®è¯: {', '.join(article['keywords'])}

æ¥æº: {article['source']}
å‘å¸ƒæ—¶é—´: {article['published_time']}"""

                # åˆ›å»ºå‘é‡
                embedding = self.create_tfidf_vector(document_content)

                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'source_type': 'real_rss_news',
                    'source_name': article['source'],
                    'url': article['url'],
                    'timestamp': datetime.now().isoformat(),
                    'published_time': article['published_time'],
                    'importance': article['importance'],
                    'keywords': json.dumps(article['keywords'], ensure_ascii=False),
                    'category': 'real_financial_news',
                    'content_length': len(article['content'])
                }

                # å­˜å‚¨åˆ°ChromaDB
                self.collection.add(
                    documents=[document_content],
                    embeddings=[embedding],
                    metadatas=[metadata],
                    ids=[doc_id]
                )

                stored_count += 1
                print(f"   âœ… å­˜å‚¨: {article['title'][:40]}...")

            except Exception as e:
                print(f"âš ï¸ å­˜å‚¨æ–‡ç« å¤±è´¥: {e}")
                continue

        print(f"âœ… æˆåŠŸå­˜å‚¨ {stored_count} ç¯‡çœŸå®æ–°é—»åˆ°RAGæ•°æ®åº“")

        # ä¿å­˜åˆ°å½’æ¡£
        if articles:
            archive_metadata = {
                'source': 'real_rss_monitor',
                'collection_name': 'real_financial_news',
                'stored_count': stored_count,
                'total_articles': len(articles),
                'feed_sources': list(self.rss_feeds.keys())
            }

            self.archive_manager.save_archive(
                data=articles,
                data_type='rss_news',
                metadata=archive_metadata,
                custom_suffix='real_feeds'
            )

    async def run_monitoring_cycle(self):
        """æ‰§è¡Œä¸€æ¬¡ç›‘æ§å‘¨æœŸ"""
        start_time = time.time()

        # æ”¶é›†çœŸå®RSSæ–‡ç« 
        articles = await self.collect_real_rss_articles()

        # å­˜å‚¨åˆ°RAGæ•°æ®åº“
        if articles:
            self.store_articles_to_rag(articles)

        end_time = time.time()
        duration = end_time - start_time

        print(f"â±ï¸ ç›‘æ§å‘¨æœŸå®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ”„ ä¸‹æ¬¡æ›´æ–°æ—¶é—´: {(datetime.now() + timedelta(minutes=self.update_interval)).strftime('%H:%M:%S')}")
        print("=" * 80)

    def start_monitoring(self):
        """å¯åŠ¨çœŸå®RSSç›‘æ§"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ¯ çœŸå®RSSç›‘æ§ç³»ç»Ÿå¯åŠ¨")
        print(f"â° æ›´æ–°é—´éš”: {self.update_interval} åˆ†é’Ÿ")
        print(f"ğŸ“¡ ç›‘æ§æºæ•°é‡: {len(self.rss_feeds)}")
        print("="*80)

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        asyncio.run(self.run_monitoring_cycle())

        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(self.update_interval).minutes.do(
            lambda: asyncio.run(self.run_monitoring_cycle())
        )

        print(f"âš¡ çœŸå®RSSå®šæ—¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ çœŸå®RSSç›‘æ§å·²åœæ­¢")

    def run_once(self):
        """è¿è¡Œä¸€æ¬¡ç›‘æ§ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ§ª æ‰§è¡Œä¸€æ¬¡çœŸå®RSSç›‘æ§æµ‹è¯•")
        asyncio.run(self.run_monitoring_cycle())

def main():
    """ä¸»å‡½æ•°"""
    monitor = RealRSSMonitor()

    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        # æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œä¸€æ¬¡
        monitor.run_once()
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šæŒç»­ç›‘æ§
        monitor.start_monitoring()

if __name__ == "__main__":
    main()