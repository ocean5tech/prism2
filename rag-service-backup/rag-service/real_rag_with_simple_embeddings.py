#!/usr/bin/env python3
"""
çœŸå®RAGç³»ç»Ÿï¼Œä½¿ç”¨ç®€å•çš„å‘é‡åŒ–æ–¹æ³•
é¿å…HuggingFaceä¸‹è½½é—®é¢˜ï¼Œä½¿ç”¨æœ¬åœ°TF-IDFå‘é‡åŒ–
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import re
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

class SimpleRealDataCollector:
    """çœŸå®æ•°æ®æ”¶é›†å™¨ + ç®€å•å‘é‡åŒ–å¤„ç†å™¨"""

    def __init__(self):
        self.session = None
        self.collected_data = []
        self.vectorizer = None
        self.client = None
        self.collection = None

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–çœŸå®æ•°æ®æ”¶é›†å’Œç®€å•å‘é‡åŒ–ç³»ç»Ÿ...")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–ç®€å•çš„TF-IDFå‘é‡åŒ–å™¨
        print("ğŸ“¥ åˆå§‹åŒ–TF-IDFä¸­æ–‡å‘é‡åŒ–å™¨...")
        try:
            self.vectorizer = TfidfVectorizer(
                max_features=768,  # ä¸bge-large-zh-v1.5çš„ç»´åº¦ä¿æŒä¸€è‡´
                analyzer='word',
                tokenizer=lambda x: jieba.lcut(x),  # ä½¿ç”¨jiebaä¸­æ–‡åˆ†è¯
                token_pattern=None,
                lowercase=False,
                stop_words=None
            )
            print("âœ… TF-IDFä¸­æ–‡å‘é‡åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

        # è¿æ¥åˆ°ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()

            # å°è¯•åˆ é™¤ç°æœ‰é›†åˆä»¥é¿å…ç»´åº¦å†²çª
            try:
                self.client.delete_collection("financial_documents")
                print("   ğŸ—‘ï¸ åˆ é™¤ç°æœ‰é›†åˆä»¥é¿å…ç»´åº¦å†²çª")
            except:
                pass

            # åˆ›å»ºæ–°é›†åˆ
            self.collection = self.client.create_collection("financial_documents")
            print("âœ… ChromaDBè¿æ¥æˆåŠŸï¼Œåˆ›å»ºæ–°é›†åˆ")
        except Exception as e:
            print(f"âŒ ChromaDBè¿æ¥å¤±è´¥: {e}")
            raise e

        # è®¾ç½®HTTPä¼šè¯
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Linux; U; Android 4.0.3; ko-kr; LG-L160L Build/IML74K) AppleWebkit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'
            }
        )

        print("âœ… çœŸå®æ•°æ®æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def collect_akshare_data(self) -> List[Dict]:
        """ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®"""
        print("ğŸ“Š å¼€å§‹ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # 1. è·å–Aè‚¡å®æ—¶æ–°é—»
            print("   è·å–Aè‚¡æ–°é—»...")
            try:
                news_data = ak.news_cctv()  # å¤®è§†è´¢ç»æ–°é—»
                print(f"   âœ… è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                for _, news in news_data.head(3).iterrows():  # å–å‰3æ¡
                    doc = {
                        'id': f"akshare_news_{int(time.time())}_{len(collected_docs)}",
                        'content': f"{news.get('title', '')} {news.get('content', '')}",
                        'metadata': {
                            'source': 'AKShare-CCTVè´¢ç»',
                            'doc_type': 'financial_news',
                            'title': news.get('title', ''),
                            'publish_date': str(news.get('date', datetime.now().date())),
                            'data_source': 'akshare_cctv_news',
                            'importance': 7,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)

            except Exception as e:
                print(f"   âš ï¸ è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")

            # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            print("   è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            try:
                # è·å–å¹³å®‰é“¶è¡Œç®€å•è‚¡ç¥¨æ•°æ®
                stock_info = ak.stock_zh_a_hist(symbol="000001", period="daily", start_date="20241101", end_date="20241101")
                if not stock_info.empty:
                    latest_data = stock_info.iloc[-1]
                    doc = {
                        'id': f"akshare_stock_000001_{int(time.time())}",
                        'content': f"å¹³å®‰é“¶è¡Œ(000001)æœ€æ–°äº¤æ˜“æ•°æ®ï¼šå¼€ç›˜ä»·{latest_data.get('å¼€ç›˜', 'N/A')}ï¼Œæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}ã€‚è¯¥è‚¡ç¥¨è¡¨ç°ç¨³å®šï¼Œé“¶è¡Œæ¿å—æ•´ä½“èµ°åŠ¿è‰¯å¥½ã€‚",
                        'metadata': {
                            'source': 'AKShare-è‚¡ç¥¨æ•°æ®',
                            'doc_type': 'stock_data',
                            'stock_code': '000001',
                            'company_name': 'å¹³å®‰é“¶è¡Œ',
                            'open_price': float(latest_data.get('å¼€ç›˜', 0)),
                            'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                            'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                            'date': str(latest_data.name),
                            'data_source': 'akshare_stock_hist',
                            'importance': 8,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)
                    print(f"   âœ… è·å–åˆ°å¹³å®‰é“¶è¡Œè‚¡ç¥¨æ•°æ®")

            except Exception as e:
                print(f"   âš ï¸ è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… AKShareæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…ï¼Œè·³è¿‡AKShareæ•°æ®æ”¶é›†")
            return []
        except Exception as e:
            print(f"âŒ AKShareæ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    def store_documents_to_vector_db(self, documents: List[Dict]) -> int:
        """å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨TF-IDFå‘é‡åŒ–"""
        if not documents:
            print("âš ï¸ æ²¡æœ‰æ–‡æ¡£éœ€è¦å­˜å‚¨")
            return 0

        print(f"ğŸ§  å¼€å§‹ä½¿ç”¨TF-IDFä¸­æ–‡å‘é‡åŒ– {len(documents)} ä¸ªæ–‡æ¡£...")

        try:
            # æå–æ–‡æ¡£å†…å®¹ç”¨äºå‘é‡åŒ–
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            # ä½¿ç”¨TF-IDFå‘é‡åŒ–å™¨ç”Ÿæˆå‘é‡
            print("   ğŸ”„ ç”ŸæˆTF-IDFå‘é‡åµŒå…¥...")
            start_time = time.time()

            # è®­ç»ƒå‘é‡åŒ–å™¨å¹¶è½¬æ¢æ–‡æ¡£
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            embeddings = tfidf_matrix.toarray()

            embedding_time = time.time() - start_time

            print(f"   âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’")
            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]} (TF-IDFå‘é‡)")

            # å­˜å‚¨åˆ°ChromaDB
            print("   ğŸ’¾ å­˜å‚¨åˆ°ChromaDB...")
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=doc_ids
            )

            print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªçœŸå®æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return len(documents)

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    async def collect_all_real_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®æº...")
        start_time = time.time()

        all_docs = []

        # 1. AKShareæ•°æ®
        akshare_docs = await self.collect_akshare_data()
        all_docs.extend(akshare_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ çœŸå®æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ æ•°æ®æºåˆ†å¸ƒ:")
        print(f"      - AKShare: {len(akshare_docs)} ä¸ª")

        # ç«‹å³è¿›è¡Œå‘é‡åŒ–å­˜å‚¨
        if all_docs:
            print(f"\nğŸ§  å¼€å§‹å‘é‡åŒ–å­˜å‚¨æµç¨‹...")
            stored_count = self.store_documents_to_vector_db(all_docs)
            print(f"âœ… å‘é‡åŒ–å­˜å‚¨å®Œæˆ: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")
        else:
            print("âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡å‘é‡åŒ–å­˜å‚¨")

        return all_docs

    def test_semantic_search(self, query: str, n_results: int = 3):
        """æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½"""
        print(f"ğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢: '{query}'")

        try:
            # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
            query_vector = self.vectorizer.transform([query]).toarray()[0]

            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            results = self.collection.query(
                query_embeddings=[query_vector.tolist()],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )

            print(f"   ğŸ“Š æ‰¾åˆ° {len(results['ids'][0])} ä¸ªç›¸å…³æ–‡æ¡£:")

            for i, doc_id in enumerate(results['ids'][0]):
                doc = results['documents'][0][i]
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]

                print(f"   ğŸ“„ æ–‡æ¡£ {i+1}: {doc_id}")
                print(f"      ç›¸ä¼¼åº¦: {1-distance:.3f}")
                print(f"      æ¥æº: {metadata.get('source', 'æœªçŸ¥')}")
                print(f"      å†…å®¹: {doc[:100]}...")
                print(f"      ---")

            return results

        except Exception as e:
            print(f"   âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return None

async def test_complete_real_pipeline():
    """æµ‹è¯•å®Œæ•´çš„çœŸå®æ•°æ®ç®¡é“"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´çœŸå®æ•°æ®ç®¡é“ (æ”¶é›†â†’å‘é‡åŒ–â†’å­˜å‚¨â†’æœç´¢)")
    print("=" * 80)

    async with SimpleRealDataCollector() as collector:
        # æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®å¹¶è‡ªåŠ¨å‘é‡åŒ–å­˜å‚¨
        real_docs = await collector.collect_all_real_data()

        if real_docs:
            print(f"\nğŸ“‹ æ”¶é›†åˆ°çš„çœŸå®æ•°æ®é¢„è§ˆ:")
            print("=" * 60)

            for i, doc in enumerate(real_docs[:2], 1):  # æ˜¾ç¤ºå‰2ä¸ª
                print(f"\nğŸ“„ æ–‡æ¡£ {i}: {doc['id']}")
                print(f"   æ¥æº: {doc['metadata']['source']}")
                print(f"   ç±»å‹: {doc['metadata']['doc_type']}")
                print(f"   å†…å®¹: {doc['content'][:100]}...")
                print("-" * 50)

            # éªŒè¯æ•°æ®åº“ä¸­çš„å†…å®¹
            print(f"\nğŸ” éªŒè¯ChromaDBä¸­çš„çœŸå®å‘é‡åŒ–æ•°æ®:")
            print("=" * 60)

            try:
                count = collector.collection.count()
                print(f"   ğŸ“Š æ•°æ®åº“ä¸­æ€»æ–‡æ¡£æ•°: {count}")

                # æµ‹è¯•è¯­ä¹‰æœç´¢
                print(f"\nğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½:")
                print("=" * 40)

                test_queries = [
                    "é“¶è¡Œè‚¡ç¥¨æ•°æ®",
                    "å¹³å®‰é“¶è¡Œ",
                    "è´¢ç»æ–°é—»"
                ]

                for query in test_queries:
                    collector.test_semantic_search(query, n_results=2)
                    print()

            except Exception as e:
                print(f"   âŒ éªŒè¯æ•°æ®åº“å¤±è´¥: {e}")

            # ä¿å­˜åˆ°æ–‡ä»¶
            output_file = f"/home/wyatt/prism2/rag-service/real_pipeline_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(real_docs, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

            return real_docs
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•çœŸå®æ•°æ®")
            return []

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…...")
    required_packages = ['akshare', 'feedparser', 'aiohttp', 'beautifulsoup4', 'scikit-learn', 'jieba']

    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package} æœªå®‰è£…")

    # è¿è¡Œå®Œæ•´æµ‹è¯•
    asyncio.run(test_complete_real_pipeline())