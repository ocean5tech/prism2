#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prism2 Backendå…¨é¢æµ‹è¯•æ‰§è¡Œå™¨
åŸºäºBackend-Comprehensive-Test-Plan.mdçš„å¯æ‰§è¡Œæµ‹è¯•è„šæœ¬
"""
import asyncio
import json
import time
import sys
import logging
import requests
import redis
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
from typing import Dict, List, Any, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveTestRunner:
    """Backendå…¨é¢æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.api_base_url = "http://localhost:8081"
        self.test_stocks = {
            "online": "600549",  # å¦é—¨é’¨ä¸š
            "batch": ["688660", "600629", "600619"]  # æ·±åº¦å­¦ä¹ ã€åå»ºé›†å›¢ã€æµ·ç«‹è‚¡ä»½
        }

        # è¿æ¥é…ç½®
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        self.pg_config = {
            'host': 'localhost',
            'database': 'prism2',
            'user': 'prism2',
            'password': 'prism2_secure_password'
        }

        # æµ‹è¯•ç»“æœ
        self.test_results = {
            "start_time": datetime.now(),
            "online_api_tests": {},
            "batch_processing_tests": {},
            "data_consistency_tests": {},
            "performance_tests": {},
            "integration_tests": {},
            "summary": {}
        }

    def get_pg_connection(self):
        """è·å–PostgreSQLè¿æ¥"""
        try:
            return psycopg2.connect(**self.pg_config, cursor_factory=RealDictCursor)
        except psycopg2.Error as e:
            logger.error(f"PostgreSQLè¿æ¥å¤±è´¥: {e}")
            raise Exception(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¼‚å¸¸: {e}")
            raise Exception(f"æ•°æ®åº“è¿æ¥å¼‚å¸¸: {e}")

    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Backendå…¨é¢æµ‹è¯•")
        logger.info(f"æµ‹è¯•è‚¡ç¥¨ - Online: {self.test_stocks['online']}, Batch: {self.test_stocks['batch']}")

        try:
            # 1. Online APIæµ‹è¯•
            await self.run_online_api_tests()

            # 2. æ‰¹å¤„ç†ç³»ç»Ÿæµ‹è¯•
            await self.run_batch_processing_tests()

            # 3. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
            await self.run_data_consistency_tests()

            # 4. æ€§èƒ½æµ‹è¯•
            await self.run_performance_tests()

            # 5. é›†æˆæµ‹è¯•
            await self.run_integration_tests()

            # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            await self.generate_test_report()

        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def run_online_api_tests(self):
        """Online APIç³»ç»Ÿæµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸŒ Part 1: Online APIç³»ç»Ÿæµ‹è¯•")
        logger.info("="*60)

        online_results = {}

        # æµ‹è¯•ç”¨ä¾‹ API-001: ä¸»è¦ä»ªè¡¨æ¿æ¥å£
        logger.info("æµ‹è¯•ç”¨ä¾‹ API-001: ä¸»è¦ä»ªè¡¨æ¿æ¥å£")

        # æ¸…ç©ºç¼“å­˜
        self.redis_client.flushdb()
        logger.info("âœ“ ç¼“å­˜å·²æ¸…ç©º")

        # å‘é€APIè¯·æ±‚
        payload = {
            "stock_code": self.test_stocks["online"],
            "data_types": ["realtime", "kline", "basic_info", "financial", "announcements", "shareholders", "longhubang"],
            "kline_period": "daily",
            "kline_days": 60,
            "news_days": 7
        }

        start_time = time.time()
        try:
            response = requests.post(
                f"{self.api_base_url}/api/v1/stocks/dashboard",
                json=payload,
                timeout=30
            )
            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                online_results["api_001"] = {
                    "success": True,
                    "response_time": response_time,
                    "stock_code": data.get("stock_code"),
                    "data_sources": data.get("data_sources", {}),
                    "cache_info": data.get("cache_info", {}),
                    "data_types_count": len(data.get("data", {}))
                }
                logger.info(f"âœ“ APIè¯·æ±‚æˆåŠŸ: {response_time:.2f}s")
                logger.info(f"  æ•°æ®ç±»å‹: {len(data.get('data', {}))} ç§")
                logger.info(f"  ç¼“å­˜ä¿¡æ¯: {data.get('cache_info', {})}")
            else:
                online_results["api_001"] = {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                logger.error(f"âœ— APIè¯·æ±‚å¤±è´¥: {response.status_code}")

        except Exception as e:
            online_results["api_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— APIè¯·æ±‚å¼‚å¸¸: {e}")

        # éªŒè¯æ•°æ®åº“å­˜å‚¨
        try:
            conn = self.get_pg_connection()
            cursor = conn.cursor()

            tables = ["stock_financial", "stock_announcements", "stock_shareholders", "stock_longhubang"]
            db_counts = {}

            for table in tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE stock_code = %s", (self.test_stocks["online"],))
                result = cursor.fetchone()
                count = result['count'] if result else 0
                db_counts[table] = count
                logger.info(f"  {table}: {count} æ¡è®°å½•")

            online_results["database_storage"] = db_counts
            conn.close()

        except Exception as e:
            error_msg = str(e) if str(e) and str(e) != "0" else "æ•°æ®åº“è¿æ¥æˆ–æŸ¥è¯¢å¤±è´¥"
            logger.error(f"âœ— æ•°æ®åº“éªŒè¯å¤±è´¥: {error_msg}")
            online_results["database_storage"] = {"error": error_msg}

        # éªŒè¯Redisç¼“å­˜
        try:
            cache_keys = self.redis_client.keys(f"*{self.test_stocks['online']}*")
            online_results["redis_cache"] = {
                "keys_count": len(cache_keys),
                "keys": cache_keys[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªé”®
            }
            logger.info(f"  Redisç¼“å­˜: {len(cache_keys)} ä¸ªé”®")

        except Exception as e:
            logger.error(f"âœ— RediséªŒè¯å¤±è´¥: {e}")
            online_results["redis_cache"] = {"error": str(e)}

        # æµ‹è¯•ç”¨ä¾‹ API-002: ç¼“å­˜å‘½ä¸­æµ‹è¯•
        logger.info("\næµ‹è¯•ç”¨ä¾‹ API-002: ç¼“å­˜å‘½ä¸­æµ‹è¯•")

        payload_cache = {
            "stock_code": self.test_stocks["online"],
            "data_types": ["realtime", "kline", "basic_info"],
            "kline_period": "daily",
            "kline_days": 60
        }

        start_time = time.time()
        try:
            response = requests.post(
                f"{self.api_base_url}/api/v1/stocks/dashboard",
                json=payload_cache,
                timeout=10
            )
            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                online_results["api_002"] = {
                    "success": True,
                    "response_time": response_time,
                    "cache_info": data.get("cache_info", {})
                }
                logger.info(f"âœ“ ç¼“å­˜æµ‹è¯•æˆåŠŸ: {response_time:.2f}s")
                logger.info(f"  ç¼“å­˜å‘½ä¸­: {data.get('cache_info', {})}")
            else:
                online_results["api_002"] = {
                    "success": False,
                    "error": f"HTTP {response.status_code}"
                }
                logger.error(f"âœ— ç¼“å­˜æµ‹è¯•å¤±è´¥: {response.status_code}")

        except Exception as e:
            online_results["api_002"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— ç¼“å­˜æµ‹è¯•å¼‚å¸¸: {e}")

        # æµ‹è¯•ç”¨ä¾‹ API-003: æ ¹è·¯å¾„æµ‹è¯•
        logger.info("\næµ‹è¯•ç”¨ä¾‹ API-003: æ ¹è·¯å¾„æµ‹è¯•")

        try:
            response = requests.get(f"{self.api_base_url}/", timeout=5)
            if response.status_code == 200:
                data = response.json()
                online_results["api_003"] = {
                    "success": True,
                    "message": data.get("message"),
                    "status": data.get("status"),
                    "version": data.get("version")
                }
                logger.info(f"âœ“ æ ¹è·¯å¾„æµ‹è¯•æˆåŠŸ: {data.get('message')}")
            else:
                online_results["api_003"] = {
                    "success": False,
                    "error": f"HTTP {response.status_code}"
                }
                logger.error(f"âœ— æ ¹è·¯å¾„æµ‹è¯•å¤±è´¥: {response.status_code}")

        except Exception as e:
            online_results["api_003"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— æ ¹è·¯å¾„æµ‹è¯•å¼‚å¸¸: {e}")

        self.test_results["online_api_tests"] = online_results

    async def run_batch_processing_tests(self):
        """æ‰¹å¤„ç†ç³»ç»Ÿæµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("âš¡ Part 2: æ‰¹å¤„ç†ç³»ç»Ÿæµ‹è¯•")
        logger.info("="*60)

        batch_results = {}

        # æµ‹è¯•ç”¨ä¾‹ BATCH-001: åˆ›å»ºæµ‹è¯•è‡ªé€‰è‚¡åˆ—è¡¨
        logger.info("æµ‹è¯•ç”¨ä¾‹ BATCH-001: åˆ›å»ºæµ‹è¯•è‡ªé€‰è‚¡åˆ—è¡¨")

        try:
            from batch_processor.services.watchlist_service import WatchlistService
            from batch_processor.models.watchlist import WatchlistCreate

            service = WatchlistService()

            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„é‡å¤æµ‹è¯•æ•°æ®
            try:
                existing_watchlists = await service.get_user_watchlists("test_user_comprehensive")
                for watchlist in existing_watchlists:
                    if watchlist.watchlist_name == "ç»¼åˆæµ‹è¯•è‚¡ç¥¨æ± ":
                        await service.delete_watchlist(watchlist.id)
                        logger.info(f"å·²æ¸…ç†é‡å¤çš„æµ‹è¯•è‡ªé€‰è‚¡åˆ—è¡¨: {watchlist.id}")
            except Exception as cleanup_error:
                logger.warning(f"æ¸…ç†é‡å¤æ•°æ®æ—¶å‡ºé”™: {cleanup_error}")

            watchlist_data = WatchlistCreate(
                user_id='test_user_comprehensive',
                watchlist_name='ç»¼åˆæµ‹è¯•è‚¡ç¥¨æ± ',
                description='ç”¨äºBackendå…¨é¢æµ‹è¯•çš„è‚¡ç¥¨ç»„åˆ',
                stock_codes=self.test_stocks["batch"],
                priority_level=5,
                data_types=['financial', 'announcements', 'shareholders', 'longhubang'],
                auto_batch=True
            )

            result = await service.create_watchlist(watchlist_data)
            batch_results["batch_001"] = {
                "success": True,
                "watchlist_id": getattr(result, "id", None),
                "stock_codes": getattr(result, "stock_codes", None),
                "priority_level": getattr(result, "priority_level", None)
            }
            logger.info(f"âœ“ è‡ªé€‰è‚¡åˆ—è¡¨åˆ›å»ºæˆåŠŸ: ID={getattr(result, 'id', None)}")
            logger.info(f"  åŒ…å«è‚¡ç¥¨: {getattr(result, 'stock_codes', None)}")

        except Exception as e:
            batch_results["batch_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— è‡ªé€‰è‚¡åˆ—è¡¨åˆ›å»ºå¤±è´¥: {e}")

        # æµ‹è¯•ç”¨ä¾‹ BATCH-002: è‡ªé€‰è‚¡æ‰¹å¤„ç†æ‰§è¡Œ
        logger.info("\næµ‹è¯•ç”¨ä¾‹ BATCH-002: è‡ªé€‰è‚¡æ‰¹å¤„ç†æ‰§è¡Œ")

        try:
            from batch_processor.processors.watchlist_processor import WatchlistProcessor

            processor = WatchlistProcessor()

            # è·å–é«˜ä¼˜å…ˆçº§è‡ªé€‰è‚¡åˆ—è¡¨
            watchlists = await processor.get_priority_watchlists(min_priority=4)
            logger.info(f"å‘ç° {len(watchlists)} ä¸ªé«˜ä¼˜å…ˆçº§è‡ªé€‰è‚¡åˆ—è¡¨")

            if watchlists:
                # æ‰§è¡Œæ‰¹å¤„ç†
                watchlist = watchlists[0]
                start_time = time.time()
                result = await processor.process_single_watchlist(
                    watchlist_id=watchlist.id,
                    force_refresh=True
                )
                processing_time = time.time() - start_time

                batch_results["batch_002"] = {
                    "success": True,
                    "processing_time": processing_time,
                    "processed_stocks": result.get("processed_stocks"),
                    "successful_stocks": result.get("successful_stocks"),
                    "failed_stocks": result.get("failed_stocks")
                }
                logger.info(f"âœ“ æ‰¹å¤„ç†æ‰§è¡ŒæˆåŠŸ: {processing_time:.2f}s")
                logger.info(f"  å¤„ç†è‚¡ç¥¨: {result.get('processed_stocks')} åª")
                logger.info(f"  æˆåŠŸ: {result.get('successful_stocks')}, å¤±è´¥: {result.get('failed_stocks')}")
            else:
                batch_results["batch_002"] = {
                    "success": False,
                    "error": "æ²¡æœ‰æ‰¾åˆ°é«˜ä¼˜å…ˆçº§è‡ªé€‰è‚¡åˆ—è¡¨"
                }
                logger.error("âœ— æ²¡æœ‰æ‰¾åˆ°é«˜ä¼˜å…ˆçº§è‡ªé€‰è‚¡åˆ—è¡¨")

        except Exception as e:
            batch_results["batch_002"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {e}")

        # æµ‹è¯•ç”¨ä¾‹ RAG-001: RAGæ‰¹é‡åŒæ­¥æµ‹è¯•
        logger.info("\næµ‹è¯•ç”¨ä¾‹ RAG-001: RAGæ‰¹é‡åŒæ­¥æµ‹è¯•")

        try:
            from batch_processor.processors.rag_sync_processor import RAGSyncProcessor

            processor = RAGSyncProcessor()

            start_time = time.time()
            result = await processor.sync_batch_data_to_rag(
                stock_codes=self.test_stocks["batch"],
                data_types=['financial', 'announcements'],
                force_refresh=True
            )
            sync_time = time.time() - start_time

            batch_results["rag_001"] = {
                "success": True,
                "sync_time": sync_time,
                "successful_syncs": result.get("successful_syncs"),
                "failed_syncs": result.get("failed_syncs"),
                "new_versions_created": result.get("new_versions_created")
            }
            logger.info(f"âœ“ RAGåŒæ­¥æˆåŠŸ: {sync_time:.2f}s")
            logger.info(f"  æˆåŠŸåŒæ­¥: {result.get('successful_syncs')}")
            logger.info(f"  å¤±è´¥åŒæ­¥: {result.get('failed_syncs')}")
            logger.info(f"  æ–°ç‰ˆæœ¬: {result.get('new_versions_created')}")

        except Exception as e:
            batch_results["rag_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— RAGåŒæ­¥å¤±è´¥: {e}")

        self.test_results["batch_processing_tests"] = batch_results

    async def run_data_consistency_tests(self):
        """æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Part 3: æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•")
        logger.info("="*60)

        consistency_results = {}

        # æµ‹è¯•ç”¨ä¾‹ DATA-001: ä¸‰å±‚æ¶æ„æ•°æ®æµéªŒè¯
        logger.info("æµ‹è¯•ç”¨ä¾‹ DATA-001: ä¸‰å±‚æ¶æ„æ•°æ®æµéªŒè¯")

        test_stock = self.test_stocks["online"]
        data_types = ['financial', 'announcements', 'shareholders']

        try:
            conn = self.get_pg_connection()
            cursor = conn.cursor()

            consistency_data = {}

            for data_type in data_types:
                logger.info(f"\næ£€æŸ¥ {test_stock} - {data_type}:")

                # 1. æ£€æŸ¥Redisç¼“å­˜
                cache_key = f'{data_type}:{test_stock}'
                redis_data = self.redis_client.get(cache_key)
                redis_status = "æœ‰æ•°æ®" if redis_data else "æ— æ•°æ®"
                redis_size = len(redis_data) if redis_data else 0

                # 2. æ£€æŸ¥PostgreSQLå­˜å‚¨
                table_map = {
                    'financial': ('stock_financial', 'summary_data'),
                    'announcements': ('stock_announcements', 'announcement_data'),
                    'shareholders': ('stock_shareholders', 'shareholders_data')
                }

                table_name, data_column = table_map[data_type]
                cursor.execute(f'SELECT {data_column}, updated_at FROM {table_name} WHERE stock_code = %s ORDER BY updated_at DESC LIMIT 1', (test_stock,))
                pg_result = cursor.fetchone()
                pg_status = "æœ‰æ•°æ®" if pg_result else "æ— æ•°æ®"
                pg_update_time = pg_result["updated_at"] if pg_result else None

                # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
                consistency_rate = 0.0
                if redis_data and pg_result:
                    try:
                        redis_obj = json.loads(redis_data)
                        pg_obj = pg_result[data_column]

                        # å¯¹äºä¸åŒç±»å‹çš„æ•°æ®ï¼Œä½¿ç”¨ä¸åŒçš„ä¸€è‡´æ€§æ£€æŸ¥ç­–ç•¥
                        if data_type == "financial":
                            # è´¢åŠ¡æ•°æ®ï¼šæ¯”è¾ƒæ•°å€¼å­—æ®µçš„å­˜åœ¨æ€§
                            if isinstance(redis_obj, dict) and isinstance(pg_obj, dict):
                                redis_keys = set(redis_obj.keys())
                                pg_keys = set(pg_obj.keys())
                                common_keys = redis_keys & pg_keys
                                total_keys = max(len(redis_keys), len(pg_keys))
                                consistency_rate = len(common_keys) / total_keys if total_keys > 0 else 0.0
                            elif redis_obj and pg_obj:
                                # å¦‚æœä¸¤è¾¹éƒ½æœ‰æ•°æ®ä½†æ ¼å¼ä¸åŒï¼Œè®¤ä¸º50%ä¸€è‡´
                                consistency_rate = 0.5
                        else:
                            # å…¶ä»–æ•°æ®ç±»å‹ï¼šç›´æ¥æ¯”è¾ƒ
                            if isinstance(redis_obj, dict) and isinstance(pg_obj, dict):
                                redis_keys = set(redis_obj.keys())
                                pg_keys = set(pg_obj.keys())
                                common_keys = redis_keys & pg_keys
                                total_keys = max(len(redis_keys), len(pg_keys))
                                consistency_rate = len(common_keys) / total_keys if total_keys > 0 else 0.0
                            elif str(redis_obj) == str(pg_obj):
                                consistency_rate = 1.0
                            elif redis_obj and pg_obj:
                                consistency_rate = 0.5
                    except Exception as consistency_error:
                        # å¦‚æœä¸¤è¾¹éƒ½æœ‰æ•°æ®ä½†æ— æ³•è§£æï¼Œè®¤ä¸ºéƒ¨åˆ†ä¸€è‡´
                        if redis_data and pg_result[data_column]:
                            consistency_rate = 0.3
                        else:
                            consistency_rate = 0.0

                consistency_data[data_type] = {
                    "redis_status": redis_status,
                    "redis_size": redis_size,
                    "postgresql_status": pg_status,
                    "postgresql_update_time": str(pg_update_time) if pg_update_time else None,
                    "consistency_rate": consistency_rate
                }

                logger.info(f"  Redis: {redis_status} ({redis_size} å­—ç¬¦)")
                logger.info(f"  PostgreSQL: {pg_status} (æ›´æ–°æ—¶é—´: {pg_update_time})")
                logger.info(f"  æ•°æ®ä¸€è‡´æ€§: {consistency_rate:.2%}")

            conn.close()

            consistency_results["data_001"] = {
                "success": True,
                "test_stock": test_stock,
                "data_types": consistency_data
            }

        except Exception as e:
            consistency_results["data_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")

        # æµ‹è¯•ç”¨ä¾‹ DATA-002: æ‰¹å¤„ç†æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        logger.info("\næµ‹è¯•ç”¨ä¾‹ DATA-002: æ‰¹å¤„ç†æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")

        try:
            conn = self.get_pg_connection()
            cursor = conn.cursor()

            # æ£€æŸ¥æµ‹è¯•è‚¡ç¥¨æ•°æ®è¦†ç›–ç‡
            all_test_stocks = [self.test_stocks["online"]] + self.test_stocks["batch"]
            coverage_data = {}

            for stock_code in all_test_stocks:
                cursor.execute("SELECT COUNT(*) FROM stock_financial WHERE stock_code = %s", (stock_code,))
                result = cursor.fetchone()
                financial_count = result['count'] if result else 0

                cursor.execute("SELECT COUNT(*) FROM stock_announcements WHERE stock_code = %s", (stock_code,))
                result = cursor.fetchone()
                announcements_count = result['count'] if result else 0

                cursor.execute("SELECT COUNT(*) FROM stock_shareholders WHERE stock_code = %s", (stock_code,))
                result = cursor.fetchone()
                shareholders_count = result['count'] if result else 0

                cursor.execute("SELECT COUNT(*) FROM stock_longhubang WHERE stock_code = %s", (stock_code,))
                result = cursor.fetchone()
                longhubang_count = result['count'] if result else 0

                coverage_data[stock_code] = {
                    "financial": "âœ“" if financial_count > 0 else "âœ—",
                    "announcements": "âœ“" if announcements_count > 0 else "âœ—",
                    "shareholders": "âœ“" if shareholders_count > 0 else "âœ—",
                    "longhubang": "âœ“" if longhubang_count > 0 else "âœ—"
                }

                logger.info(f"  {stock_code}: è´¢åŠ¡{coverage_data[stock_code]['financial']} å…¬å‘Š{coverage_data[stock_code]['announcements']} è‚¡ä¸œ{coverage_data[stock_code]['shareholders']} é¾™è™æ¦œ{coverage_data[stock_code]['longhubang']}")

            # æ£€æŸ¥RAGç‰ˆæœ¬æ•°æ®
            cursor.execute("""
                SELECT stock_code, data_type, vector_status, chunk_count
                FROM rag_data_versions
                WHERE stock_code = ANY(%s) AND vector_status = 'active'
                ORDER BY stock_code, data_type
            """, (all_test_stocks,))
            rag_versions = cursor.fetchall()

            rag_data = {}
            for row in rag_versions:
                stock_code = row['stock_code']
                if stock_code not in rag_data:
                    rag_data[stock_code] = {}
                rag_data[stock_code][row['data_type']] = {
                    "status": row['vector_status'],
                    "chunk_count": row['chunk_count']
                }

            logger.info(f"\nRAGç‰ˆæœ¬æ•°æ®: {len(rag_versions)} ä¸ªæ´»è·ƒç‰ˆæœ¬")
            for stock_code, types in rag_data.items():
                logger.info(f"  {stock_code}: {len(types)} ç§æ•°æ®ç±»å‹")

            conn.close()

            consistency_results["data_002"] = {
                "success": True,
                "coverage_data": coverage_data,
                "rag_versions_count": len(rag_versions),
                "rag_data": rag_data
            }

        except Exception as e:
            error_msg = str(e) if str(e) != "0" else "æ•°æ®åº“è¿æ¥æˆ–æŸ¥è¯¢å¤±è´¥"
            consistency_results["data_002"] = {
                "success": False,
                "error": error_msg
            }
            logger.error(f"âœ— æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {error_msg}")

        self.test_results["data_consistency_tests"] = consistency_results

    async def run_performance_tests(self):
        """æ€§èƒ½ä¸å‹åŠ›æµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ Part 4: æ€§èƒ½ä¸å‹åŠ›æµ‹è¯•")
        logger.info("="*60)

        performance_results = {}

        # æµ‹è¯•ç”¨ä¾‹ PERF-001: APIå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•
        logger.info("æµ‹è¯•ç”¨ä¾‹ PERF-001: APIå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•")

        try:
            test_stock = self.test_stocks["online"]
            payload = {
                "stock_code": test_stock,
                "data_types": ["realtime", "kline", "basic_info"],
                "kline_period": "daily",
                "kline_days": 60
            }

            # å†·å¯åŠ¨æµ‹è¯•ï¼ˆæ¸…ç©ºç¼“å­˜ï¼‰
            self.redis_client.delete(f"*{test_stock}*")

            start_time = time.time()
            response = requests.post(f"{self.api_base_url}/api/v1/stocks/dashboard", json=payload, timeout=30)
            cold_start_time = time.time() - start_time

            # çƒ­ç¼“å­˜æµ‹è¯•
            start_time = time.time()
            response = requests.post(f"{self.api_base_url}/api/v1/stocks/dashboard", json=payload, timeout=10)
            hot_cache_time = time.time() - start_time

            # å¹¶å‘æµ‹è¯•
            concurrent_times = []
            for i in range(3):  # 3ä¸ªå¹¶å‘è¯·æ±‚
                start_time = time.time()
                response = requests.post(f"{self.api_base_url}/api/v1/stocks/dashboard", json=payload, timeout=10)
                concurrent_times.append(time.time() - start_time)

            avg_concurrent_time = sum(concurrent_times) / len(concurrent_times)

            performance_results["perf_001"] = {
                "success": True,
                "cold_start_time": cold_start_time,
                "hot_cache_time": hot_cache_time,
                "avg_concurrent_time": avg_concurrent_time,
                "performance_grade": {
                    "cold_start": "ä¼˜ç§€" if cold_start_time < 5 else "ä¸€èˆ¬" if cold_start_time < 10 else "å·®",
                    "hot_cache": "ä¼˜ç§€" if hot_cache_time < 1 else "ä¸€èˆ¬" if hot_cache_time < 2 else "å·®"
                }
            }

            logger.info(f"âœ“ APIæ€§èƒ½æµ‹è¯•å®Œæˆ:")
            logger.info(f"  å†·å¯åŠ¨: {cold_start_time:.2f}s")
            logger.info(f"  çƒ­ç¼“å­˜: {hot_cache_time:.2f}s")
            logger.info(f"  å¹¶å‘å¹³å‡: {avg_concurrent_time:.2f}s")

        except Exception as e:
            performance_results["perf_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— APIæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

        self.test_results["performance_tests"] = performance_results

    async def run_integration_tests(self):
        """ç»¼åˆé›†æˆæµ‹è¯•"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‹ Part 5: ç»¼åˆé›†æˆæµ‹è¯•")
        logger.info("="*60)

        integration_results = {}

        # æµ‹è¯•ç”¨ä¾‹ E2E-001: å®Œæ•´ä¸šåŠ¡æµç¨‹æµ‹è¯•
        logger.info("æµ‹è¯•ç”¨ä¾‹ E2E-001: å®Œæ•´ä¸šåŠ¡æµç¨‹æµ‹è¯•")

        try:
            test_stock = self.test_stocks["batch"][0]  # ä½¿ç”¨603993

            # æ­¥éª¤1: ç¯å¢ƒæ¸…ç†
            logger.info("æ­¥éª¤1: ç¯å¢ƒæ¸…ç†")
            cache_keys = self.redis_client.keys(f"*{test_stock}*")
            if cache_keys:
                self.redis_client.delete(*cache_keys)

            conn = self.get_pg_connection()
            cursor = conn.cursor()
            cursor.execute("DELETE FROM rag_data_versions WHERE stock_code = %s", (test_stock,))
            conn.commit()
            conn.close()

            # æ­¥éª¤2: APIé¦–æ¬¡è°ƒç”¨ï¼ˆå†·å¯åŠ¨ï¼‰
            logger.info("æ­¥éª¤2: APIå†·å¯åŠ¨è°ƒç”¨")
            payload = {
                "stock_code": test_stock,
                "data_types": ["financial", "announcements"]
            }

            start_time = time.time()
            response = requests.post(f"{self.api_base_url}/api/v1/stocks/dashboard", json=payload, timeout=30)
            cold_response_time = time.time() - start_time

            cold_success = response.status_code == 200
            cold_cache_info = response.json().get("cache_info", {}) if cold_success else {}

            # æ­¥éª¤3: æ‰§è¡Œæ‰¹å¤„ç†
            logger.info("æ­¥éª¤3: æ‰§è¡Œæ‰¹å¤„ç†")
            from batch_processor.processors.watchlist_processor import WatchlistProcessor

            processor = WatchlistProcessor()
            batch_start = time.time()
            batch_result = await processor.process_stock_batch([test_stock])
            batch_time = time.time() - batch_start
            batch_success = batch_result.get("success", 0) > 0

            # æ­¥éª¤4: æ‰§è¡ŒRAGåŒæ­¥
            logger.info("æ­¥éª¤4: æ‰§è¡ŒRAGåŒæ­¥")
            from batch_processor.processors.rag_sync_processor import RAGSyncProcessor

            rag_processor = RAGSyncProcessor()
            rag_start = time.time()
            rag_result = await rag_processor.sync_batch_data_to_rag(
                stock_codes=[test_stock],
                data_types=['financial', 'announcements']
            )
            rag_time = time.time() - rag_start
            rag_success = rag_result.get("successful_syncs", 0) > 0

            # æ­¥éª¤5: APIå†æ¬¡è°ƒç”¨ï¼ˆçƒ­ç¼“å­˜ï¼‰
            logger.info("æ­¥éª¤5: APIçƒ­ç¼“å­˜è°ƒç”¨")
            start_time = time.time()
            response = requests.post(f"{self.api_base_url}/api/v1/stocks/dashboard", json=payload, timeout=10)
            hot_response_time = time.time() - start_time

            hot_success = response.status_code == 200
            hot_cache_info = response.json().get("cache_info", {}) if hot_success else {}

            integration_results["e2e_001"] = {
                "success": all([cold_success, batch_success, rag_success, hot_success]),
                "cold_start": {
                    "success": cold_success,
                    "response_time": cold_response_time,
                    "cache_info": cold_cache_info
                },
                "batch_processing": {
                    "success": batch_success,
                    "processing_time": batch_time,
                    "result": batch_result
                },
                "rag_sync": {
                    "success": rag_success,
                    "sync_time": rag_time,
                    "result": rag_result
                },
                "hot_cache": {
                    "success": hot_success,
                    "response_time": hot_response_time,
                    "cache_info": hot_cache_info
                }
            }

            logger.info(f"âœ“ ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ:")
            logger.info(f"  å†·å¯åŠ¨API: {cold_success} ({cold_response_time:.2f}s)")
            logger.info(f"  æ‰¹å¤„ç†: {batch_success} ({batch_time:.2f}s)")
            logger.info(f"  RAGåŒæ­¥: {rag_success} ({rag_time:.2f}s)")
            logger.info(f"  çƒ­ç¼“å­˜API: {hot_success} ({hot_response_time:.2f}s)")

        except Exception as e:
            integration_results["e2e_001"] = {
                "success": False,
                "error": str(e)
            }
            logger.error(f"âœ— ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")

        self.test_results["integration_tests"] = integration_results

    async def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        logger.info("="*60)

        end_time = datetime.now()
        total_time = (end_time - self.test_results["start_time"]).total_seconds()

        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = 0
        passed_tests = 0

        # æ’é™¤éæµ‹è¯•é¡¹ç›®çš„æ¡ç›®
        excluded_items = ["start_time", "summary", "database_storage", "redis_cache"]

        for category, tests in self.test_results.items():
            if category in excluded_items:
                continue

            for test_name, result in tests.items():
                # è·³è¿‡éæµ‹è¯•æ¡ç›®
                if test_name in excluded_items:
                    continue

                total_tests += 1
                if result.get("success", False):
                    passed_tests += 1

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "test_execution_time": total_time,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": success_rate,
            "test_stocks": self.test_stocks,
            "overall_status": "PASS" if success_rate >= 80 else "FAIL"
        }

        self.test_results["summary"] = summary
        self.test_results["end_time"] = end_time

        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = f"/home/wyatt/prism2/docs/Backend-Test-Report-{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)

        # è¾“å‡ºæµ‹è¯•æŠ¥å‘Š
        logger.info(f"ğŸ¯ æµ‹è¯•æ€»ç»“:")
        logger.info(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.info(f"  å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        logger.info(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        logger.info(f"  æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        logger.info(f"  æµ‹è¯•æŠ¥å‘Š: {report_file}")

        if success_rate >= 80:
            logger.info("ğŸ‰ Backendç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼")
        else:
            logger.warning("âš ï¸ Backendç³»ç»Ÿæµ‹è¯•æœªè¾¾æ ‡ï¼Œéœ€è¦æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")

        return summary

async def main():
    """ä¸»å‡½æ•°"""
    try:
        runner = ComprehensiveTestRunner()
        await runner.run_all_tests()
    except KeyboardInterrupt:
        logger.info("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())