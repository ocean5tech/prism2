# Data Service - å†…éƒ¨è®¾è®¡æ–‡æ¡£

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯

- **æ¨¡å—åç§°**: Data Service (æ•°æ®é‡‡é›†æœåŠ¡)
- **æŠ€æœ¯æ ˆ**: FastAPI + Scrapy + APScheduler + Celery
- **éƒ¨ç½²ç«¯å£**: 8003
- **ä¾æ®**: å¤–éƒ¨è®¾è®¡æ–‡æ¡£è§„èŒƒ

---

## ğŸ“ æ–‡ä»¶ç»“æ„å’Œæƒé™

```
/home/wyatt/prism2/data-service/
â”œâ”€â”€ app/                                  # åº”ç”¨æºä»£ç  (755)
â”‚   â”œâ”€â”€ __init__.py                       # (644)
â”‚   â”œâ”€â”€ main.py                           # FastAPIåº”ç”¨å…¥å£ (644)
â”‚   â”œâ”€â”€ core/                             # æ ¸å¿ƒé…ç½® (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ config.py                     # ç¯å¢ƒé…ç½® (644)
â”‚   â”‚   â”œâ”€â”€ dependencies.py               # ä¾èµ–æ³¨å…¥ (644)
â”‚   â”‚   â””â”€â”€ security.py                   # å®‰å…¨é…ç½® (644)
â”‚   â”œâ”€â”€ api/                              # APIè·¯ç”± (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â””â”€â”€ v1/                           # APIç‰ˆæœ¬1 (755)
â”‚   â”‚       â”œâ”€â”€ __init__.py               # (644)
â”‚   â”‚       â”œâ”€â”€ data.py                   # æ•°æ®ç›¸å…³ç«¯ç‚¹ (644)
â”‚   â”‚       â”œâ”€â”€ sources.py                # æ•°æ®æºç®¡ç†ç«¯ç‚¹ (644)
â”‚   â”‚       â””â”€â”€ health.py                 # å¥åº·æ£€æŸ¥ç«¯ç‚¹ (644)
â”‚   â”œâ”€â”€ services/                         # ä¸šåŠ¡æœåŠ¡å±‚ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ data_collector.py             # æ•°æ®é‡‡é›†æœåŠ¡ (644)
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py               # æ•°æ®æ¸…æ´—æœåŠ¡ (644)
â”‚   â”‚   â”œâ”€â”€ source_manager.py             # æ•°æ®æºç®¡ç†æœåŠ¡ (644)
â”‚   â”‚   â””â”€â”€ scheduler_service.py          # å®šæ—¶ä»»åŠ¡æœåŠ¡ (644)
â”‚   â”œâ”€â”€ crawlers/                         # çˆ¬è™«æ¨¡å— (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ base_crawler.py               # çˆ¬è™«åŸºç±» (644)
â”‚   â”‚   â”œâ”€â”€ stock_crawler.py              # è‚¡ç¥¨æ•°æ®çˆ¬è™« (644)
â”‚   â”‚   â”œâ”€â”€ news_crawler.py               # æ–°é—»æ•°æ®çˆ¬è™« (644)
â”‚   â”‚   â””â”€â”€ announcement_crawler.py       # å…¬å‘Šæ•°æ®çˆ¬è™« (644)
â”‚   â”œâ”€â”€ models/                           # æ•°æ®æ¨¡å‹ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ requests.py                   # è¯·æ±‚æ¨¡å‹ (644)
â”‚   â”‚   â”œâ”€â”€ responses.py                  # å“åº”æ¨¡å‹ (644)
â”‚   â”‚   â””â”€â”€ database.py                   # æ•°æ®åº“æ¨¡å‹ (644)
â”‚   â”œâ”€â”€ tasks/                            # Celeryä»»åŠ¡ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ data_tasks.py                 # æ•°æ®é‡‡é›†ä»»åŠ¡ (644)
â”‚   â”‚   â””â”€â”€ cleanup_tasks.py              # æ•°æ®æ¸…ç†ä»»åŠ¡ (644)
â”‚   â””â”€â”€ utils/                            # å·¥å…·å‡½æ•° (755)
â”‚       â”œâ”€â”€ __init__.py                   # (644)
â”‚       â”œâ”€â”€ data_parser.py                # æ•°æ®è§£æå·¥å…· (644)
â”‚       â”œâ”€â”€ data_validator.py             # æ•°æ®éªŒè¯å·¥å…· (644)
â”‚       â””â”€â”€ retry_helper.py               # é‡è¯•æœºåˆ¶å·¥å…· (644)
â”œâ”€â”€ scrapy_project/                       # Scrapyé¡¹ç›® (755)
â”‚   â”œâ”€â”€ scrapy.cfg                        # Scrapyé…ç½® (644)
â”‚   â”œâ”€â”€ spiders/                          # çˆ¬è™«è„šæœ¬ (755)
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # (644)
â”‚   â”‚   â”œâ”€â”€ sina_spider.py                # æ–°æµªè´¢ç»çˆ¬è™« (644)
â”‚   â”‚   â”œâ”€â”€ eastmoney_spider.py           # ä¸œæ–¹è´¢å¯Œçˆ¬è™« (644)
â”‚   â”‚   â””â”€â”€ cninfo_spider.py              # å·¨æ½®èµ„è®¯çˆ¬è™« (644)
â”‚   â”œâ”€â”€ items.py                          # æ•°æ®é¡¹å®šä¹‰ (644)
â”‚   â”œâ”€â”€ pipelines.py                      # æ•°æ®ç®¡é“ (644)
â”‚   â””â”€â”€ settings.py                       # Scrapyè®¾ç½® (644)
â”œâ”€â”€ requirements.txt                      # Pythonä¾èµ– (644)
â”œâ”€â”€ Dockerfile                           # å®¹å™¨åŒ–é…ç½® (644)
â””â”€â”€ .env                                 # ç¯å¢ƒå˜é‡ (600)
```

---

## ğŸ—„ï¸ æ•°æ®åº“è¡¨ç»“æ„ (PostgreSQL + TimescaleDB)

### æ•°æ®æºé…ç½®è¡¨ (data_sources)
```sql
CREATE TABLE data_sources (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,              -- æ•°æ®æºåç§°
    source_type VARCHAR(50) NOT NULL,        -- ç±»å‹: rss/api/scraper
    source_url TEXT NOT NULL,                -- æ•°æ®æºURL
    poll_interval INTEGER DEFAULT 300,       -- è½®è¯¢é—´éš”(ç§’)
    enabled BOOLEAN DEFAULT true,            -- æ˜¯å¦å¯ç”¨
    config JSONB,                           -- é…ç½®å‚æ•°
    last_poll_time TIMESTAMP,               -- æœ€åè½®è¯¢æ—¶é—´
    success_count INTEGER DEFAULT 0,        -- æˆåŠŸæ¬¡æ•°
    error_count INTEGER DEFAULT 0,          -- é”™è¯¯æ¬¡æ•°
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_data_sources_type ON data_sources(source_type);
CREATE INDEX idx_data_sources_enabled ON data_sources(enabled);
```

### åŸå§‹æ•°æ®è¡¨ (raw_data) - TimescaleDBä¼˜åŒ–
```sql
CREATE TABLE raw_data (
    id BIGSERIAL,
    source_id INTEGER REFERENCES data_sources(id),
    data_type VARCHAR(50) NOT NULL,          -- æ•°æ®ç±»å‹: stock/news/announcement
    content JSONB NOT NULL,                  -- åŸå§‹æ•°æ®å†…å®¹
    checksum VARCHAR(64),                    -- æ•°æ®æ ¡éªŒå’Œ(å»é‡ç”¨)
    status VARCHAR(20) DEFAULT 'pending',    -- çŠ¶æ€: pending/processed/failed
    collected_at TIMESTAMP NOT NULL DEFAULT NOW(),
    processed_at TIMESTAMP,

    -- TimescaleDBåˆ†åŒºé”®
    PRIMARY KEY (collected_at, id)
);

-- åˆ›å»ºTimescaleDBè¶…è¡¨
SELECT create_hypertable('raw_data', 'collected_at', chunk_time_interval => INTERVAL '1 day');

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_raw_data_type ON raw_data(data_type, collected_at);
CREATE INDEX idx_raw_data_status ON raw_data(status);
CREATE INDEX idx_raw_data_checksum ON raw_data(checksum);
```

### å¤„ç†ä»»åŠ¡è¡¨ (processing_tasks)
```sql
CREATE TABLE processing_tasks (
    id SERIAL PRIMARY KEY,
    task_type VARCHAR(50) NOT NULL,          -- ä»»åŠ¡ç±»å‹
    task_config JSONB,                       -- ä»»åŠ¡é…ç½®
    status VARCHAR(20) DEFAULT 'pending',    -- pending/running/completed/failed
    priority INTEGER DEFAULT 5,             -- ä¼˜å…ˆçº§ 1-10
    retry_count INTEGER DEFAULT 0,          -- é‡è¯•æ¬¡æ•°
    max_retries INTEGER DEFAULT 3,          -- æœ€å¤§é‡è¯•æ¬¡æ•°
    error_message TEXT,                      -- é”™è¯¯ä¿¡æ¯
    created_at TIMESTAMP DEFAULT NOW(),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,

    -- ç´¢å¼•
    INDEX idx_tasks_status (status),
    INDEX idx_tasks_priority (priority, created_at)
);
```

### æ•°æ®è´¨é‡ç›‘æ§è¡¨ (data_quality_metrics)
```sql
CREATE TABLE data_quality_metrics (
    id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES data_sources(id),
    metric_name VARCHAR(100),                -- æŒ‡æ ‡åç§°
    metric_value DECIMAL(10,4),              -- æŒ‡æ ‡å€¼
    metric_unit VARCHAR(20),                 -- å•ä½
    threshold_min DECIMAL(10,4),             -- æœ€å°é˜ˆå€¼
    threshold_max DECIMAL(10,4),             -- æœ€å¤§é˜ˆå€¼
    is_within_threshold BOOLEAN,             -- æ˜¯å¦åœ¨é˜ˆå€¼å†…
    measured_at TIMESTAMP DEFAULT NOW(),

    -- TimescaleDBåˆ†åŒº
    PRIMARY KEY (measured_at, id)
);

SELECT create_hypertable('data_quality_metrics', 'measured_at', chunk_time_interval => INTERVAL '1 hour');
```

---

## ğŸ”Œ APIæ¥å£å®šä¹‰ (ä¸¥æ ¼æŒ‰ç…§å¤–éƒ¨è®¾è®¡)

### åŸºç¡€é…ç½®
```python
# æœåŠ¡é…ç½®
DATA_SERVICE_PORT = int(os.getenv('DATA_SERVICE_PORT', '8003'))
CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/3')
SCRAPY_PROJECT_PATH = os.getenv('SCRAPY_PROJECT_PATH', './scrapy_project')
MAX_CONCURRENT_CRAWLERS = int(os.getenv('MAX_CONCURRENT_CRAWLERS', '5'))
```

### API1: æ•°æ®æºé…ç½®ç®¡ç† (å¯¹æ¥å¤–éƒ¨è®¾è®¡æ¥å£)
- **URL**: `POST /api/data/configure`
- **è¾“å…¥å‚æ•°**: ä¸¥æ ¼æŒ‰ç…§å¤–éƒ¨è®¾è®¡çš„æ•°æ®æºé…ç½®è¯·æ±‚
  ```python
  class DataSourceConfigRequest(BaseModel):
      source_type: str                      # rss/api/scraper/manual
      source_url: str                       # æ•°æ®æºURL
      poll_interval: int = 300              # è½®è¯¢é—´éš”(ç§’)
      keywords: Optional[List[str]] = None  # å…³é”®è¯è¿‡æ»¤
      config: Optional[Dict[str, Any]] = None  # æ‰©å±•é…ç½®
      enabled: bool = True                  # æ˜¯å¦å¯ç”¨
  ```
- **è¾“å‡ºç»“æœ**: å¤–éƒ¨è®¾è®¡çš„é…ç½®å“åº”
  ```python
  class DataSourceConfigResponse(BaseModel):
      source_id: str                        # æ•°æ®æºå”¯ä¸€ID
      status: str                          # success/failed
      message: str                         # çŠ¶æ€æè¿°
      next_poll_time: datetime             # ä¸‹æ¬¡è½®è¯¢æ—¶é—´
  ```
- **èµ„æº**: data_sourcesè¡¨ã€Celeryä»»åŠ¡é˜Ÿåˆ—
- **é€»è¾‘**: æ¥æ”¶æ•°æ®æºé…ç½®è¯·æ±‚ï¼ŒéªŒè¯URLå¯è®¿é—®æ€§ï¼Œåˆ›å»ºæ•°æ®æºè®°å½•ï¼Œå¯åŠ¨å®šæ—¶é‡‡é›†ä»»åŠ¡ï¼Œè¿”å›é…ç½®çŠ¶æ€å’Œä¸‹æ¬¡æ‰§è¡Œæ—¶é—´

### API2: OHLCVå†å²æ•°æ®è·å– (å¯¹æ¥Stock Service)
- **URL**: `GET /api/data/stocks/{stock_code}/ohlcv`
- **è¾“å…¥å‚æ•°**: å¤–éƒ¨è®¾è®¡çš„OHLCVæ•°æ®è¯·æ±‚
  ```python
  class OHLCVRequest(BaseModel):
      stock_code: str                       # è‚¡ç¥¨ä»£ç 
      period: str = "1d"                   # å‘¨æœŸ: 1m/5m/15m/30m/1h/1d/1w/1M
      limit: int = 100                     # è¿”å›æ¡æ•°
      start_date: Optional[str] = None     # å¼€å§‹æ—¥æœŸ
      end_date: Optional[str] = None       # ç»“æŸæ—¥æœŸ
  ```
- **è¾“å‡ºç»“æœ**: å¤–éƒ¨è®¾è®¡çš„OHLCVæ•°æ®å“åº”
  ```python
  class OHLCVData(BaseModel):
      stock_code: str
      timestamp: datetime
      open: float
      high: float
      low: float
      close: float
      volume: int
      adj_close: float

  class OHLCVResponse(BaseModel):
      data: List[OHLCVData]
      total_count: int
      period: str
      data_quality: float                  # æ•°æ®è´¨é‡è¯„åˆ† 0-1
  ```
- **èµ„æº**: TimescaleDB OHLCVè¡¨ã€æ•°æ®è´¨é‡ç›‘æ§
- **é€»è¾‘**: æ ¹æ®è‚¡ç¥¨ä»£ç å’Œæ—¶é—´èŒƒå›´æŸ¥è¯¢OHLCVå†å²æ•°æ®ï¼Œæ”¯æŒå¤šç§æ—¶é—´å‘¨æœŸèšåˆï¼Œæ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡ï¼Œè¿”å›å¸¦è´¨é‡è¯„åˆ†çš„æ•°æ®åˆ—è¡¨

### API3: æ–°é—»æ•°æ®é‡‡é›†çŠ¶æ€æŸ¥è¯¢
- **URL**: `GET /api/data/news/status`
- **è¾“å…¥å‚æ•°**: æ–°é—»é‡‡é›†çŠ¶æ€æŸ¥è¯¢
  ```python
  class NewsCollectionStatusRequest(BaseModel):
      source_ids: Optional[List[str]] = None  # æŒ‡å®šæ•°æ®æºID
      date_range: Optional[Dict[str, str]] = None  # æ—¶é—´èŒƒå›´
      status_filter: Optional[str] = None   # çŠ¶æ€è¿‡æ»¤
  ```
- **è¾“å‡ºç»“æœ**: æ–°é—»é‡‡é›†çŠ¶æ€æŠ¥å‘Š
  ```python
  class NewsData(BaseModel):
      id: str
      title: str
      content: str
      source: str
      publish_time: datetime
      stock_codes: List[str]
      sentiment: str                        # positive/negative/neutral

  class NewsCollectionStatusResponse(BaseModel):
      total_collected: int                  # æ€»é‡‡é›†æ•°é‡
      successful: int                       # æˆåŠŸå¤„ç†æ•°é‡
      failed: int                          # å¤±è´¥æ•°é‡
      latest_items: List[NewsData]         # æœ€æ–°é‡‡é›†çš„æ•°æ®
      collection_rate: float               # é‡‡é›†æ•ˆç‡
  ```
- **èµ„æº**: raw_dataè¡¨ã€processing_tasksè¡¨
- **é€»è¾‘**: ç»Ÿè®¡æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–°é—»é‡‡é›†æƒ…å†µï¼Œè®¡ç®—é‡‡é›†æˆåŠŸç‡å’Œå¤„ç†æ•ˆç‡ï¼Œè¿”å›æœ€æ–°é‡‡é›†çš„æ–°é—»æ ·æœ¬å’Œæ•´ä½“çŠ¶æ€æŒ‡æ ‡

### API4: æ•°æ®æ¸…æ´—å’Œè´¨é‡æ£€æŸ¥
- **URL**: `POST /api/data/quality/check`
- **è¾“å…¥å‚æ•°**: æ•°æ®è´¨é‡æ£€æŸ¥è¯·æ±‚
  ```python
  class DataQualityCheckRequest(BaseModel):
      data_type: str                        # æ£€æŸ¥çš„æ•°æ®ç±»å‹
      batch_id: Optional[str] = None       # æ‰¹æ¬¡ID
      quality_rules: List[str]             # è´¨é‡è§„åˆ™åˆ—è¡¨
      auto_fix: bool = False               # æ˜¯å¦è‡ªåŠ¨ä¿®å¤
  ```
- **è¾“å‡ºç»“æœ**: æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š
  ```python
  class QualityIssue(BaseModel):
      issue_type: str                       # é—®é¢˜ç±»å‹
      severity: str                        # ä¸¥é‡ç¨‹åº¦
      affected_records: int                # å—å½±å“è®°å½•æ•°
      description: str                     # é—®é¢˜æè¿°

  class DataQualityResponse(BaseModel):
      total_records: int                    # æ€»è®°å½•æ•°
      valid_records: int                   # æœ‰æ•ˆè®°å½•æ•°
      quality_score: float                 # è´¨é‡è¯„åˆ† 0-1
      issues: List[QualityIssue]           # è´¨é‡é—®é¢˜åˆ—è¡¨
      recommendations: List[str]           # æ”¹è¿›å»ºè®®
  ```
- **èµ„æº**: æ•°æ®éªŒè¯å¼•æ“ã€æ•°æ®è´¨é‡è§„åˆ™åº“
- **é€»è¾‘**: å¯¹æŒ‡å®šæ‰¹æ¬¡çš„æ•°æ®æ‰§è¡Œè´¨é‡æ£€æŸ¥è§„åˆ™ï¼Œè¯†åˆ«æ•°æ®å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§é—®é¢˜ï¼Œç”Ÿæˆè´¨é‡è¯„åˆ†å’Œè¯¦ç»†é—®é¢˜æŠ¥å‘Šï¼Œæä¾›æ•°æ®æ”¹è¿›å»ºè®®

---

## ğŸ”§ æ ¸å¿ƒæœåŠ¡å®ç°

### 1. DataCollector (æ•°æ®é‡‡é›†æœåŠ¡)
- **æ–‡ä»¶**: `app/services/data_collector.py`
- **åŠŸèƒ½**: ç»Ÿä¸€çš„æ•°æ®é‡‡é›†æœåŠ¡æ¥å£
- **è¾“å…¥**: æ•°æ®æºé…ç½®å’Œé‡‡é›†å‚æ•°
- **è¾“å‡º**: åŸå§‹æ•°æ®å­˜å‚¨å’Œé‡‡é›†çŠ¶æ€
- **èµ„æº**: Scrapyçˆ¬è™«å¼•æ“ã€Celeryä»»åŠ¡é˜Ÿåˆ—
- **é€»è¾‘**: æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©åˆé€‚çš„é‡‡é›†å™¨ï¼Œç®¡ç†å¹¶å‘é‡‡é›†ä»»åŠ¡ï¼Œç›‘æ§é‡‡é›†è¿›åº¦å’Œé”™è¯¯ç‡ï¼Œå®ç°å¢é‡é‡‡é›†å’Œé‡å¤æ•°æ®æ£€æµ‹

### 2. DataCleaner (æ•°æ®æ¸…æ´—æœåŠ¡)
- **æ–‡ä»¶**: `app/services/data_cleaner.py`
- **åŠŸèƒ½**: åŸå§‹æ•°æ®æ ‡å‡†åŒ–å’Œè´¨é‡æå‡
- **è¾“å…¥**: åŸå§‹æ•°æ®è®°å½•
- **è¾“å‡º**: æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®
- **èµ„æº**: æ•°æ®éªŒè¯è§„åˆ™ã€æ¸…æ´—ç®—æ³•åº“
- **é€»è¾‘**: æ‰§è¡Œæ•°æ®æ ¼å¼æ ‡å‡†åŒ–ã€å¼‚å¸¸å€¼æ£€æµ‹å’Œä¿®å¤ã€ç¼ºå¤±å€¼å¡«å……ã€æ•°æ®ç±»å‹è½¬æ¢ï¼Œç¡®ä¿æ•°æ®è´¨é‡æ»¡è¶³ä¸‹æ¸¸æœåŠ¡éœ€æ±‚

### 3. SourceManager (æ•°æ®æºç®¡ç†æœåŠ¡)
- **æ–‡ä»¶**: `app/services/source_manager.py`
- **åŠŸèƒ½**: æ•°æ®æºç”Ÿå‘½å‘¨æœŸç®¡ç†
- **è¾“å…¥**: æ•°æ®æºé…ç½®å’ŒçŠ¶æ€æ›´æ–°
- **è¾“å‡º**: æ•°æ®æºå¥åº·çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
- **é€»è¾‘**: ç®¡ç†æ•°æ®æºçš„æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤æ“ä½œï¼Œç›‘æ§æ•°æ®æºå¯ç”¨æ€§å’Œå“åº”æ—¶é—´ï¼Œå®ç°è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤ï¼Œç»´æŠ¤æ•°æ®æºé…ç½®å†å²

### 4. SchedulerService (å®šæ—¶ä»»åŠ¡æœåŠ¡)
- **æ–‡ä»¶**: `app/services/scheduler_service.py`
- **åŠŸèƒ½**: åŸºäºAPSchedulerçš„æ™ºèƒ½ä»»åŠ¡è°ƒåº¦
- **è¾“å…¥**: ä»»åŠ¡é…ç½®å’Œè°ƒåº¦è§„åˆ™
- **è¾“å‡º**: ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œè°ƒåº¦æ—¥å¿—
- **é€»è¾‘**: æ ¹æ®æ•°æ®æºé…ç½®åˆ›å»ºå®šæ—¶é‡‡é›†ä»»åŠ¡ï¼Œå®ç°ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†å’Œè´Ÿè½½å‡è¡¡ï¼Œæ”¯æŒä»»åŠ¡å¤±è´¥é‡è¯•å’Œä¾èµ–å…³ç³»å¤„ç†

---

## ğŸ•·ï¸ Scrapyçˆ¬è™«æ¶æ„

### çˆ¬è™«é¡¹ç›®ç»“æ„
```python
# scrapy_project/spiders/base_spider.py
class BaseFinancialSpider(scrapy.Spider):
    """é‡‘èæ•°æ®çˆ¬è™«åŸºç±»"""
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 8,
        'USER_AGENT': 'Mozilla/5.0 Financial Data Collector'
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.setup_proxy_middleware()
        self.setup_retry_middleware()
```

### æ•°æ®ç®¡é“é…ç½®
```python
# scrapy_project/pipelines.py
class DataValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""
    def process_item(self, item, spider):
        # æ•°æ®æ ¼å¼éªŒè¯
        # é‡å¤æ•°æ®æ£€æµ‹
        # æ•°æ®è´¨é‡è¯„åˆ†
        return item

class DatabasePipeline:
    """æ•°æ®åº“å­˜å‚¨ç®¡é“"""
    def process_item(self, item, spider):
        # å­˜å‚¨åˆ°raw_dataè¡¨
        # è§¦å‘åç»­å¤„ç†ä»»åŠ¡
        return item
```

---

## ğŸŒ æ•°æ®æµè®¾è®¡

### å®æ—¶æ•°æ®é‡‡é›†æµç¨‹
1. **ä»»åŠ¡è°ƒåº¦** â†’ APSchedulerè§¦å‘é‡‡é›†ä»»åŠ¡ â†’ Celeryæ‰§è¡Œå™¨æ¥æ”¶
2. **æ•°æ®è·å–** â†’ Scrapyçˆ¬è™«å¯åŠ¨ â†’ å¤–éƒ¨APIè°ƒç”¨ â†’ åŸå§‹æ•°æ®è·å–
3. **åˆæ­¥å¤„ç†** â†’ æ ¼å¼éªŒè¯ â†’ é‡å¤æ£€æµ‹ â†’ raw_dataè¡¨å­˜å‚¨
4. **åç»­å¤„ç†** â†’ æ•°æ®æ¸…æ´— â†’ è´¨é‡æ£€æŸ¥ â†’ æ¨é€åˆ°ä¸‹æ¸¸æœåŠ¡

### æ‰¹é‡æ•°æ®å¤„ç†æµç¨‹
1. **æ‰¹æ¬¡åˆ›å»º** â†’ å®šä¹‰å¤„ç†æ‰¹æ¬¡ â†’ è®¾ç½®å¤„ç†è§„åˆ™ â†’ ä»»åŠ¡åˆ†å‘
2. **å¹¶è¡Œå¤„ç†** â†’ å¤šWorkerå¤„ç† â†’ è¿›åº¦ç›‘æ§ â†’ é”™è¯¯å¤„ç†
3. **è´¨é‡æ§åˆ¶** â†’ æ•°æ®éªŒè¯ â†’ å¼‚å¸¸æ ‡è®° â†’ è´¨é‡æŠ¥å‘Šç”Ÿæˆ
4. **ç»“æœè¾“å‡º** â†’ æ¸…æ´—æ•°æ®å­˜å‚¨ â†’ çŠ¶æ€æ›´æ–° â†’ ä¸‹æ¸¸é€šçŸ¥

---

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### é‡‡é›†æ€§èƒ½ä¼˜åŒ–
- **å¹¶å‘æ§åˆ¶**: åŸºäºç›®æ ‡ç½‘ç«™è´Ÿè½½èƒ½åŠ›åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
- **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
- **ç¼“å­˜æœºåˆ¶**: é‡å¤URLæ£€æµ‹ï¼Œé¿å…é‡å¤é‡‡é›†
- **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹éƒ¨ç½²ï¼Œä»»åŠ¡æ™ºèƒ½åˆ†å‘

### æ•°æ®å¤„ç†ä¼˜åŒ–
- **æµå¼å¤„ç†**: å¤§æ•°æ®é›†é‡‡ç”¨æµå¼å¤„ç†ï¼Œå‡å°‘å†…å­˜å ç”¨
- **æ‰¹é‡æ“ä½œ**: æ•°æ®åº“æ‰¹é‡æ’å…¥ï¼Œæé«˜å†™å…¥æ•ˆç‡
- **ç´¢å¼•ä¼˜åŒ–**: TimescaleDBæ—¶é—´åˆ†åŒºå’Œç´¢å¼•ä¼˜åŒ–
- **å¼‚æ­¥å¤„ç†**: Celeryå¼‚æ­¥ä»»åŠ¡å¤„ç†ï¼Œæé«˜å“åº”é€Ÿåº¦

---

## ğŸ”’ ç¯å¢ƒé…ç½®

### ç¯å¢ƒå˜é‡ (.env)
```bash
# æœåŠ¡é…ç½®
DATA_SERVICE_PORT=8003
MAX_CONCURRENT_CRAWLERS=5
DATA_RETENTION_DAYS=90

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:pass@localhost:5432/prism2
TIMESCALEDB_ENABLED=true

# Celeryé…ç½®
CELERY_BROKER_URL=redis://localhost:6379/3
CELERY_RESULT_BACKEND=redis://localhost:6379/3
CELERY_WORKER_CONCURRENCY=4

# Scrapyé…ç½®
SCRAPY_PROJECT_PATH=./scrapy_project
USER_AGENT=Mozilla/5.0 Financial Data Collector
DOWNLOAD_DELAY=1
CONCURRENT_REQUESTS=8

# ä»£ç†é…ç½® (å¯é€‰)
PROXY_ENABLED=false
PROXY_LIST=[]

# ç›‘æ§é…ç½®
SENTRY_DSN=
LOG_LEVEL=INFO
METRICS_ENABLED=true
```

### ä¾èµ–é…ç½® (requirements.txt)
```txt
fastapi==0.104.1
uvicorn==0.24.0
scrapy==2.11.0
celery==5.3.4
redis==5.0.1
psycopg2-binary==2.9.7
apscheduler==3.10.4
sqlalchemy==2.0.23
alembic==1.12.1
pydantic==2.5.0
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.3
feedparser==6.0.10
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### å…³é”®æ€§èƒ½æŒ‡æ ‡
- **é‡‡é›†é€Ÿç‡**: æ¯åˆ†é’ŸæˆåŠŸé‡‡é›†çš„æ•°æ®æ¡æ•°
- **æ•°æ®è´¨é‡**: å„æ•°æ®æºçš„æ•°æ®è´¨é‡è¯„åˆ†
- **é”™è¯¯ç‡**: é‡‡é›†å¤±è´¥ç‡å’Œé”™è¯¯ç±»å‹åˆ†å¸ƒ
- **å“åº”æ—¶é—´**: APIå“åº”æ—¶é—´å’Œæ•°æ®åº“æŸ¥è¯¢æ—¶é—´
- **èµ„æºä½¿ç”¨**: CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡

### å‘Šè­¦è§„åˆ™é…ç½®
```python
ALERT_RULES = {
    'collection_failure_rate': {
        'threshold': 0.1,              # å¤±è´¥ç‡è¶…è¿‡10%
        'action': 'email+slack'
    },
    'data_quality_score': {
        'threshold': 0.8,              # è´¨é‡è¯„åˆ†ä½äº80%
        'action': 'slack'
    },
    'source_unavailable': {
        'threshold': 300,              # æ•°æ®æºä¸å¯ç”¨è¶…è¿‡5åˆ†é’Ÿ
        'action': 'email+sms'
    }
}
```

---

*æ–‡æ¡£æ›´æ–°æ—¶é—´: 2025-09-16*
*ä¸¥æ ¼éµå¾ªå¤–éƒ¨è®¾è®¡è§„èŒƒï¼Œç¡®ä¿æ¥å£ä¸€è‡´æ€§*