#!/usr/bin/env python3
"""
å¸¦ç¿»è¯‘åŠŸèƒ½çš„RSSç›‘æ§ç³»ç»Ÿ
è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘è‹±æ–‡æ–°é—»ä¸ºä¸­æ–‡ï¼Œæé«˜ä¸­æ–‡RAGæ£€ç´¢æ•ˆæœ
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import chromadb
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import hashlib
import time
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
import schedule
import threading
from bs4 import BeautifulSoup
import re
from archive_manager import ArchiveManager
from googletrans import Translator
import langdetect
from langdetect import detect

class TranslatedRSSMonitor:
    def __init__(self):
        # Clear proxy variables
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        self.client = None
        self.collection = None
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.archive_manager = ArchiveManager()

        # ç¿»è¯‘å™¨åˆå§‹åŒ–
        self.translator = Translator()
        self.translation_cache = {}  # ç¿»è¯‘ç¼“å­˜

        # çœŸå®RSSæºé…ç½®
        self.rss_feeds = {
            # Bloomberg è´¢ç»æ–°é—» (è‹±æ–‡)
            'Bloomberg Markets': 'https://feeds.bloomberg.com/markets/news.rss',
            'Bloomberg Economics': 'https://feeds.bloomberg.com/economics/news.rss',
            'Bloomberg Industries': 'https://feeds.bloomberg.com/industries/news.rss',
            'Bloomberg Green': 'https://feeds.bloomberg.com/green/news.rss',

            # è·¯é€ç¤¾ (è‹±æ–‡)
            'Thomson Reuters': 'https://ir.thomsonreuters.com/rss/news-releases.xml?items=15',

            # å’Œè®¯è´¢ç» (ä¸­æ–‡)
            'å’Œè®¯è´¢ç»': 'https://news.hexun.com/rss/',

            # ä¸­å›½å®˜æ–¹æ•°æ®æº (ä¸­æ–‡)
            'ä¸­å›½è¯åˆ¸ç½‘': 'http://news.cnstock.com/rss/news_zzkx.xml',
            'æ–°æµªè´¢ç»': 'https://finance.sina.com.cn/roll/index.d.html?cid=56588'
        }

        # ç›‘æ§é…ç½®
        self.update_interval = 20  # 20åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ï¼ˆç¿»è¯‘éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
        self.max_articles_per_feed = 15  # å‡å°‘æ•°é‡ä»¥æé«˜ç¿»è¯‘è´¨é‡
        self.processed_urls = set()
        self.session_timeout = 30

        # ç¿»è¯‘é…ç½®
        self.translate_english = True  # æ˜¯å¦ç¿»è¯‘è‹±æ–‡å†…å®¹
        self.max_translation_length = 1500  # æœ€å¤§ç¿»è¯‘é•¿åº¦
        self.batch_translation_size = 3  # æ‰¹é‡ç¿»è¯‘å¤§å°

        # æ•°æ®è´¨é‡è¿‡æ»¤é…ç½®
        self.min_content_length = 80
        self.blacklist_keywords = ['å¹¿å‘Š', 'æ¨å¹¿', 'å‹æƒ…é“¾æ¥', 'ç‰ˆæƒå£°æ˜', 'advertisement', 'sponsored']

    def connect_to_chromadb(self):
        """è¿æ¥åˆ°ChromaDB"""
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()

            collection_name = "translated_financial_news"
            try:
                self.collection = self.client.get_collection(collection_name)
                print(f"ğŸ“ ä½¿ç”¨ç°æœ‰é›†åˆ: {collection_name}")
            except Exception:
                self.collection = self.client.create_collection(
                    name=collection_name,
                    metadata={"description": "Translated financial news from multilingual RSS feeds"}
                )
                print(f"ğŸ“ åˆ›å»ºæ–°é›†åˆ: {collection_name}")

            print("âœ… è¿æ¥åˆ°ChromaDBæˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ è¿æ¥ChromaDBå¤±è´¥: {e}")
            return False

    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            # æ¸…ç†æ–‡æœ¬
            clean_text = re.sub(r'[^\w\s]', ' ', text).strip()
            if len(clean_text) < 10:
                return 'unknown'

            lang = detect(clean_text)
            return lang
        except Exception as e:
            print(f"âš ï¸ è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
            # ç®€å•çš„å¯å‘å¼æ£€æµ‹
            if re.search(r'[\u4e00-\u9fff]', text):
                return 'zh'
            elif re.search(r'[a-zA-Z]', text):
                return 'en'
            else:
                return 'unknown'

    def should_translate(self, text: str, detected_lang: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘"""
        if not self.translate_english:
            return False

        # åªç¿»è¯‘è‹±æ–‡å†…å®¹
        if detected_lang != 'en':
            return False

        # æ£€æŸ¥é•¿åº¦é™åˆ¶
        if len(text) > self.max_translation_length:
            return False

        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.translation_cache:
            return False

        return True

    def translate_text(self, text: str, source_lang: str = 'en', target_lang: str = 'zh') -> Optional[str]:
        """ç¿»è¯‘æ–‡æœ¬"""
        if not text or not text.strip():
            return text

        # æ£€æŸ¥ç¼“å­˜
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.translation_cache:
            return self.translation_cache[text_hash]

        try:
            # åˆ†æ®µç¿»è¯‘é•¿æ–‡æœ¬
            if len(text) > 800:
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
                translated_sentences = []

                for sentence in sentences:
                    if sentence.strip():
                        try:
                            result = self.translator.translate(
                                sentence.strip(),
                                src=source_lang,
                                dest=target_lang
                            )
                            translated_sentences.append(result.text)
                            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                        except Exception as e:
                            print(f"âš ï¸ å¥å­ç¿»è¯‘å¤±è´¥: {e}")
                            translated_sentences.append(sentence.strip())

                translated_text = 'ã€‚'.join(translated_sentences)
            else:
                # çŸ­æ–‡æœ¬ç›´æ¥ç¿»è¯‘
                result = self.translator.translate(text, src=source_lang, dest=target_lang)
                translated_text = result.text

            # ç¼“å­˜ç¿»è¯‘ç»“æœ
            self.translation_cache[text_hash] = translated_text
            return translated_text

        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
            return text  # è¿”å›åŸæ–‡

    def process_article_translation(self, article: Dict) -> Dict:
        """å¤„ç†æ–‡ç« ç¿»è¯‘"""
        title = article.get('title', '')
        content = article.get('content', '')
        summary = article.get('summary', '')

        # æ£€æµ‹è¯­è¨€
        title_lang = self.detect_language(title)
        content_lang = self.detect_language(content)

        # ç¿»è¯‘é€»è¾‘
        translated_article = article.copy()

        # ç¿»è¯‘æ ‡é¢˜
        if self.should_translate(title, title_lang):
            print(f"   ğŸŒ ç¿»è¯‘æ ‡é¢˜: {title[:50]}...")
            translated_title = self.translate_text(title)
            if translated_title:
                translated_article['title'] = translated_title
                translated_article['original_title'] = title
                translated_article['title_translated'] = True

        # ç¿»è¯‘å†…å®¹
        if self.should_translate(content, content_lang):
            print(f"   ğŸŒ ç¿»è¯‘å†…å®¹: {len(content)} å­—ç¬¦")
            translated_content = self.translate_text(content)
            if translated_content:
                translated_article['content'] = translated_content
                translated_article['original_content'] = content
                translated_article['content_translated'] = True

        # ç¿»è¯‘æ‘˜è¦
        if summary and self.should_translate(summary, self.detect_language(summary)):
            translated_summary = self.translate_text(summary)
            if translated_summary:
                translated_article['summary'] = translated_summary
                translated_article['original_summary'] = summary

        # æ·»åŠ è¯­è¨€ä¿¡æ¯
        translated_article['detected_language'] = {
            'title': title_lang,
            'content': content_lang
        }

        return translated_article

    def chinese_tokenize(self, text: str) -> str:
        """ä¸­æ–‡åˆ†è¯"""
        return ' '.join(jieba.cut(text))

    def create_tfidf_vector(self, text: str) -> List[float]:
        """åˆ›å»ºTF-IDFå‘é‡"""
        try:
            processed_text = self.chinese_tokenize(text)
            try:
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
            except:
                self.vectorizer.fit([processed_text])
                vector = self.vectorizer.transform([processed_text])
                return vector.toarray()[0].tolist()
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            return [0.0] * 1000

    def clean_html_content(self, html_content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
        if not html_content:
            return ""

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style"]):
                script.decompose()

            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            return text

        except Exception as e:
            print(f"âš ï¸ HTMLæ¸…ç†å¤±è´¥: {e}")
            text = re.sub(r'<[^>]+>', '', html_content)
            text = re.sub(r'\s+', ' ', text).strip()
            return text

    def extract_keywords(self, title: str, content: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        combined_text = f"{title} {content}"

        # è‹±æ–‡å…³é”®è¯æå–
        english_keywords = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', combined_text)

        # ä¸­æ–‡å…³é”®è¯æå–
        chinese_words = jieba.cut(combined_text)
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        chinese_keywords = [word for word in chinese_words if len(word) > 1 and word not in stop_words]

        # åˆå¹¶å¹¶å»é‡
        all_keywords = list(set(english_keywords + chinese_keywords))
        return all_keywords[:15]

    def calculate_importance_score(self, article: Dict) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§åˆ†æ•°ï¼ˆè€ƒè™‘ç¿»è¯‘è´¨é‡ï¼‰"""
        score = 5

        title = article.get('title', '').lower()
        content = article.get('content', '').lower()
        source = article.get('source', '').lower()

        # é‡è¦å…³é”®è¯åŠ åˆ†ï¼ˆä¸­è‹±æ–‡ï¼‰
        important_keywords = {
            # è‹±æ–‡å…³é”®è¯
            'fed': 3, 'federal reserve': 3, 'interest rate': 2, 'inflation': 2,
            'gdp': 2, 'unemployment': 2, 'recession': 2, 'earnings': 2,
            'merger': 2, 'acquisition': 2, 'ipo': 2, 'dividend': 1,

            # ä¸­æ–‡å…³é”®è¯
            'å¤®è¡Œ': 3, 'åˆ©ç‡': 2, 'GDP': 2, 'PMI': 2, 'é€šèƒ€': 2,
            'è‚¡å¸‚': 2, 'åŸºé‡‘': 1, 'å€ºåˆ¸': 1, 'å¤–æ±‡': 1,
            'æ”¿ç­–': 2, 'ç›‘ç®¡': 2, 'æ”¹é©': 2, 'åˆ›æ–°': 1,
            'ä¸šç»©': 2, 'è´¢æŠ¥': 2, 'å¹¶è´­': 2, 'ä¸Šå¸‚': 2, 'æ”¶ç›Š': 1
        }

        for keyword, points in important_keywords.items():
            if keyword in title:
                score += points * 2
            elif keyword in content:
                score += points

        # ç¿»è¯‘è´¨é‡åŠ åˆ†
        if article.get('title_translated') or article.get('content_translated'):
            score += 1  # ç¿»è¯‘å†…å®¹å¯èƒ½æ›´æœ‰ä»·å€¼

        # æ•°æ®æºæƒé‡
        source_weights = {
            'bloomberg': 3,
            'thomson reuters': 2,
            'å’Œè®¯': 1,
            'ä¸­å›½è¯åˆ¸ç½‘': 2
        }

        for src, weight in source_weights.items():
            if src in source:
                score += weight

        # å†…å®¹é•¿åº¦è°ƒæ•´
        content_length = len(content)
        if content_length > 1000:
            score += 2
        elif content_length > 500:
            score += 1
        elif content_length < 100:
            score -= 2

        return max(1, min(10, score))

    def is_high_quality_content(self, article: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜è´¨é‡å†…å®¹"""
        title = article.get('title', '')
        content = article.get('content', '')
        url = article.get('url', '')

        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(content) < self.min_content_length:
            return False

        if not title.strip():
            return False

        # é»‘åå•è¯æ±‡æ£€æŸ¥
        combined_text = f"{title} {content}".lower()
        for keyword in self.blacklist_keywords:
            if keyword in combined_text:
                return False

        # é‡å¤å†…å®¹æ£€æŸ¥
        content_hash = hashlib.md5(f"{title}{content}".encode()).hexdigest()
        if content_hash in self.processed_urls:
            return False

        if url in self.processed_urls:
            return False

        self.processed_urls.add(content_hash)
        self.processed_urls.add(url)
        return True

    async def fetch_rss_feed(self, session: aiohttp.ClientSession, feed_name: str, feed_url: str) -> List[Dict]:
        """è·å–RSSæºå†…å®¹"""
        articles = []

        try:
            print(f"ğŸ” è·å–RSSæº: {feed_name}")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Cache-Control': 'no-cache'
            }

            async with session.get(feed_url, timeout=self.session_timeout, headers=headers) as response:
                if response.status == 200:
                    rss_content = await response.text()
                    feed = feedparser.parse(rss_content)

                    if feed.entries:
                        print(f"   ğŸ“° æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")

                        for entry in feed.entries[:self.max_articles_per_feed]:
                            try:
                                # è·å–å‘å¸ƒæ—¶é—´
                                pub_time = None
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6])
                                elif hasattr(entry, 'published'):
                                    try:
                                        pub_time = datetime.fromisoformat(entry.published.replace('Z', '+00:00'))
                                    except:
                                        pub_time = datetime.now()
                                else:
                                    pub_time = datetime.now()

                                # è·å–å†…å®¹
                                content = ""
                                if hasattr(entry, 'content') and entry.content:
                                    content = self.clean_html_content(entry.content[0].value)
                                elif hasattr(entry, 'summary'):
                                    content = self.clean_html_content(entry.summary)
                                elif hasattr(entry, 'description'):
                                    content = self.clean_html_content(entry.description)

                                article = {
                                    'title': getattr(entry, 'title', 'No Title'),
                                    'content': content,
                                    'url': getattr(entry, 'link', ''),
                                    'source': feed_name,
                                    'published_time': pub_time.isoformat() if pub_time else datetime.now().isoformat(),
                                    'summary': getattr(entry, 'summary', '')[:200] + '...' if hasattr(entry, 'summary') else '',
                                }

                                # è´¨é‡æ£€æŸ¥
                                if self.is_high_quality_content(article):
                                    # ç¿»è¯‘å¤„ç†
                                    translated_article = self.process_article_translation(article)

                                    # è®¡ç®—é‡è¦æ€§å’Œæå–å…³é”®è¯
                                    translated_article['importance'] = self.calculate_importance_score(translated_article)
                                    translated_article['keywords'] = self.extract_keywords(
                                        translated_article['title'],
                                        translated_article['content']
                                    )

                                    articles.append(translated_article)
                                    print(f"   âœ… æ”¶é›†: {translated_article['title'][:50]}...")

                            except Exception as e:
                                print(f"   âš ï¸ å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                                continue
                    else:
                        print(f"   âŒ RSSæºæ— æœ‰æ•ˆå†…å®¹")
                else:
                    print(f"   âŒ è·å–å¤±è´¥: HTTP {response.status}")

        except asyncio.TimeoutError:
            print(f"   â° RSSæºè¶…æ—¶: {feed_name}")
        except Exception as e:
            print(f"   âŒ RSSæºè·å–å¤±è´¥: {e}")

        return articles

    async def collect_translated_rss_articles(self) -> List[Dict]:
        """æ”¶é›†å¹¶ç¿»è¯‘RSSæºçš„æ–‡ç« """
        all_articles = []

        print(f"ğŸš€ å¼€å§‹å¤šè¯­è¨€RSSç›‘æ§ä»»åŠ¡ï¼ˆå«ç¿»è¯‘ï¼‰")
        print(f"ğŸ“¡ ç›‘æ§ {len(self.rss_feeds)} ä¸ªRSSæº")
        print(f"ğŸŒ ç¿»è¯‘åŠŸèƒ½: {'å¼€å¯' if self.translate_english else 'å…³é—­'}")
        print("-" * 70)

        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=120)  # å¢åŠ è¶…æ—¶æ—¶é—´ç»™ç¿»è¯‘é¢„ç•™æ—¶é—´

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            for feed_name, feed_url in self.rss_feeds.items():
                task = self.fetch_rss_feed(session, feed_name, feed_url)
                tasks.append(task)

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for i, result in enumerate(results):
                feed_name = list(self.rss_feeds.keys())[i]
                if isinstance(result, list):
                    all_articles.extend(result)
                    translated_count = sum(1 for a in result if a.get('title_translated') or a.get('content_translated'))
                    print(f"   âœ… {feed_name}: {len(result)} ç¯‡æ–‡ç«  (ç¿»è¯‘: {translated_count}ç¯‡)")
                else:
                    print(f"   âŒ {feed_name}: {result}")

        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        all_articles.sort(key=lambda x: (x['importance'], x['published_time']), reverse=True)

        total_translated = sum(1 for a in all_articles if a.get('title_translated') or a.get('content_translated'))
        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {total_translated} ç¯‡å·²ç¿»è¯‘")

        return all_articles

    def store_articles_to_rag(self, articles: List[Dict]):
        """å°†ç¿»è¯‘åçš„æ–‡ç« å­˜å‚¨åˆ°RAGæ•°æ®åº“"""
        if not self.collection:
            print("âŒ RAGæ•°æ®åº“æœªè¿æ¥")
            return

        stored_count = 0

        for article in articles:
            try:
                doc_id = f"translated_rss_{hashlib.md5(article['url'].encode()).hexdigest()[:12]}"

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                try:
                    existing = self.collection.get(ids=[doc_id])
                    if existing['ids']:
                        continue
                except:
                    pass

                # å‡†å¤‡æ–‡æ¡£å†…å®¹ï¼ˆåŒ…å«åŸæ–‡å’Œè¯‘æ–‡ä¿¡æ¯ï¼‰
                document_content = f"""æ ‡é¢˜: {article['title']}

å†…å®¹: {article['content']}

æ‘˜è¦: {article['summary']}

å…³é”®è¯: {', '.join(article['keywords'])}

æ¥æº: {article['source']}
å‘å¸ƒæ—¶é—´: {article['published_time']}"""

                # å¦‚æœæœ‰ç¿»è¯‘ï¼Œæ·»åŠ åŸæ–‡ä¿¡æ¯
                if article.get('title_translated'):
                    document_content += f"\nåŸæ–‡æ ‡é¢˜: {article.get('original_title', '')}"

                if article.get('content_translated'):
                    document_content += f"\nç¿»è¯‘è¯´æ˜: æœ¬æ–‡ç”±è‹±æ–‡ç¿»è¯‘è€Œæ¥"

                # åˆ›å»ºå‘é‡
                embedding = self.create_tfidf_vector(document_content)

                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'source_type': 'translated_rss_news',
                    'source_name': article['source'],
                    'url': article['url'],
                    'timestamp': datetime.now().isoformat(),
                    'published_time': article['published_time'],
                    'importance': article['importance'],
                    'keywords': json.dumps(article['keywords'], ensure_ascii=False),
                    'category': 'multilingual_financial_news',
                    'content_length': len(article['content']),
                    'title_translated': article.get('title_translated', False),
                    'content_translated': article.get('content_translated', False),
                    'detected_language': json.dumps(article.get('detected_language', {}))
                }

                # å­˜å‚¨åˆ°ChromaDB
                self.collection.add(
                    documents=[document_content],
                    embeddings=[embedding],
                    metadatas=[metadata],
                    ids=[doc_id]
                )

                stored_count += 1
                translation_note = "ğŸ“ç¿»è¯‘" if (article.get('title_translated') or article.get('content_translated')) else "ğŸ“„åŸæ–‡"
                print(f"   âœ… å­˜å‚¨({translation_note}): {article['title'][:40]}...")

            except Exception as e:
                print(f"âš ï¸ å­˜å‚¨æ–‡ç« å¤±è´¥: {e}")
                continue

        print(f"âœ… æˆåŠŸå­˜å‚¨ {stored_count} ç¯‡æ–‡ç« åˆ°RAGæ•°æ®åº“")

        # ä¿å­˜åˆ°å½’æ¡£
        if articles:
            archive_metadata = {
                'source': 'translated_rss_monitor',
                'collection_name': 'translated_financial_news',
                'stored_count': stored_count,
                'total_articles': len(articles),
                'translated_articles': sum(1 for a in articles if a.get('title_translated') or a.get('content_translated')),
                'feed_sources': list(self.rss_feeds.keys()),
                'translation_enabled': self.translate_english
            }

            self.archive_manager.save_archive(
                data=articles,
                data_type='rss_news',
                metadata=archive_metadata,
                custom_suffix='translated'
            )

    async def run_monitoring_cycle(self):
        """æ‰§è¡Œä¸€æ¬¡ç›‘æ§å‘¨æœŸ"""
        start_time = time.time()

        # æ”¶é›†å¹¶ç¿»è¯‘RSSæ–‡ç« 
        articles = await self.collect_translated_rss_articles()

        # å­˜å‚¨åˆ°RAGæ•°æ®åº“
        if articles:
            self.store_articles_to_rag(articles)

        end_time = time.time()
        duration = end_time - start_time

        print(f"â±ï¸ ç›‘æ§å‘¨æœŸå®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ”„ ä¸‹æ¬¡æ›´æ–°æ—¶é—´: {(datetime.now() + timedelta(minutes=self.update_interval)).strftime('%H:%M:%S')}")
        print("=" * 80)

    def start_monitoring(self):
        """å¯åŠ¨ç¿»è¯‘RSSç›‘æ§"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ¯ å¤šè¯­è¨€RSSç›‘æ§ç³»ç»Ÿå¯åŠ¨")
        print(f"â° æ›´æ–°é—´éš”: {self.update_interval} åˆ†é’Ÿ")
        print(f"ğŸ“¡ ç›‘æ§æºæ•°é‡: {len(self.rss_feeds)}")
        print(f"ğŸŒ ç¿»è¯‘åŠŸèƒ½: {'å¼€å¯' if self.translate_english else 'å…³é—­'}")
        print("="*80)

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        asyncio.run(self.run_monitoring_cycle())

        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(self.update_interval).minutes.do(
            lambda: asyncio.run(self.run_monitoring_cycle())
        )

        print(f"âš¡ å¤šè¯­è¨€RSSå®šæ—¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ å¤šè¯­è¨€RSSç›‘æ§å·²åœæ­¢")

    def run_once(self):
        """è¿è¡Œä¸€æ¬¡ç›‘æ§ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if not self.connect_to_chromadb():
            return

        print(f"ğŸ§ª æ‰§è¡Œä¸€æ¬¡å¤šè¯­è¨€RSSç›‘æ§æµ‹è¯•")
        asyncio.run(self.run_monitoring_cycle())

def main():
    """ä¸»å‡½æ•°"""
    monitor = TranslatedRSSMonitor()

    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        monitor.run_once()
    else:
        monitor.start_monitoring()

if __name__ == "__main__":
    main()