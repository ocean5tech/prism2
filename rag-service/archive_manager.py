#!/usr/bin/env python3
"""
ç»Ÿä¸€å½’æ¡£ç®¡ç†ç³»ç»Ÿ
è§„èŒƒåŒ–æ•°æ®å½’æ¡£å‘½åå’Œå­˜å‚¨
"""

import os
import json
from datetime import datetime
from typing import Dict, Any, List
import hashlib

class ArchiveManager:
    def __init__(self):
        # ç¡®ä¿å½’æ¡£ç›®å½•å­˜åœ¨
        self.archive_dir = "/home/wyatt/prism2/rag-service/archive_data"
        os.makedirs(self.archive_dir, exist_ok=True)

        # ç»Ÿä¸€å‘½åè§„èŒƒ
        self.naming_convention = {
            'rss_news': 'financial_rss_{timestamp}.json',
            'akshare_data': 'market_data_{timestamp}.json',
            'high_value_content': 'investment_insights_{timestamp}.json',
            'macro_analysis': 'economic_analysis_{timestamp}.json',
            'company_fundamentals': 'company_analysis_{timestamp}.json',
            'industry_research': 'industry_reports_{timestamp}.json',
            'complete_archive': 'complete_dataset_{timestamp}.json'
        }

    def get_timestamp(self) -> str:
        """è·å–æ ‡å‡†æ—¶é—´æˆ³æ ¼å¼"""
        return datetime.now().strftime('%Y%m%d_%H%M%S')

    def generate_archive_filename(self, data_type: str, custom_suffix: str = None) -> str:
        """ç”Ÿæˆæ ‡å‡†å½’æ¡£æ–‡ä»¶å"""
        timestamp = self.get_timestamp()

        if data_type in self.naming_convention:
            filename = self.naming_convention[data_type].format(timestamp=timestamp)
        else:
            # æœªçŸ¥ç±»å‹ä½¿ç”¨é€šç”¨å‘½å
            filename = f"rag_data_{data_type}_{timestamp}.json"

        if custom_suffix:
            # æ’å…¥è‡ªå®šä¹‰åç¼€
            name, ext = os.path.splitext(filename)
            filename = f"{name}_{custom_suffix}{ext}"

        return os.path.join(self.archive_dir, filename)

    def save_archive(self, data: Any, data_type: str, metadata: Dict = None, custom_suffix: str = None) -> str:
        """ä¿å­˜æ•°æ®åˆ°å½’æ¡£"""
        filename = self.generate_archive_filename(data_type, custom_suffix)

        # å‡†å¤‡å½’æ¡£è®°å½•
        archive_record = {
            'metadata': {
                'data_type': data_type,
                'created_at': datetime.now().isoformat(),
                'file_size': 0,  # å°†åœ¨ä¿å­˜åæ›´æ–°
                'record_count': 0,
                'data_hash': '',
                **(metadata or {})
            },
            'data': data
        }

        # è®¡ç®—æ•°æ®ç»Ÿè®¡
        if isinstance(data, list):
            archive_record['metadata']['record_count'] = len(data)
        elif isinstance(data, dict):
            archive_record['metadata']['record_count'] = len(data.get('documents', []))

        # è®¡ç®—æ•°æ®å“ˆå¸Œ
        data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        archive_record['metadata']['data_hash'] = hashlib.md5(data_str.encode()).hexdigest()

        # ä¿å­˜æ–‡ä»¶
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(archive_record, f, ensure_ascii=False, indent=2, default=str)

            # æ›´æ–°æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filename)
            archive_record['metadata']['file_size'] = file_size

            # é‡æ–°ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(archive_record, f, ensure_ascii=False, indent=2, default=str)

            print(f"âœ… å½’æ¡£ä¿å­˜: {os.path.basename(filename)}")
            print(f"   ğŸ“Š è®°å½•æ•°: {archive_record['metadata']['record_count']}")
            print(f"   ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size:,} bytes")

            return filename

        except Exception as e:
            print(f"âŒ å½’æ¡£ä¿å­˜å¤±è´¥: {e}")
            return None

    def list_archives(self, data_type: str = None) -> List[Dict]:
        """åˆ—å‡ºå½’æ¡£æ–‡ä»¶"""
        archives = []

        for filename in os.listdir(self.archive_dir):
            if filename.endswith('.json'):
                file_path = os.path.join(self.archive_dir, filename)

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        archive = json.load(f)

                    # è¿‡æ»¤æ•°æ®ç±»å‹
                    if data_type and archive.get('metadata', {}).get('data_type') != data_type:
                        continue

                    archive_info = {
                        'filename': filename,
                        'filepath': file_path,
                        'metadata': archive.get('metadata', {}),
                        'file_size': os.path.getsize(file_path)
                    }

                    archives.append(archive_info)

                except Exception as e:
                    print(f"âš ï¸ è¯»å–å½’æ¡£å¤±è´¥ {filename}: {e}")

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        archives.sort(key=lambda x: x['metadata'].get('created_at', ''), reverse=True)
        return archives

    def load_archive(self, filename: str) -> Dict:
        """åŠ è½½å½’æ¡£æ•°æ®"""
        if not filename.endswith('.json'):
            filename += '.json'

        file_path = os.path.join(self.archive_dir, filename)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½å½’æ¡£å¤±è´¥: {e}")
            return None

    def get_archive_stats(self) -> Dict:
        """è·å–å½’æ¡£ç»Ÿè®¡"""
        archives = self.list_archives()

        stats = {
            'total_files': len(archives),
            'total_size': sum(a['file_size'] for a in archives),
            'data_types': {},
            'oldest_archive': None,
            'newest_archive': None
        }

        # æŒ‰æ•°æ®ç±»å‹ç»Ÿè®¡
        for archive in archives:
            data_type = archive['metadata'].get('data_type', 'unknown')
            if data_type not in stats['data_types']:
                stats['data_types'][data_type] = {
                    'count': 0,
                    'total_records': 0,
                    'total_size': 0
                }

            stats['data_types'][data_type]['count'] += 1
            stats['data_types'][data_type]['total_records'] += archive['metadata'].get('record_count', 0)
            stats['data_types'][data_type]['total_size'] += archive['file_size']

        # æœ€æ–°å’Œæœ€æ—§å½’æ¡£
        if archives:
            stats['newest_archive'] = archives[0]['filename']
            stats['oldest_archive'] = archives[-1]['filename']

        return stats

    def show_archive_summary(self):
        """æ˜¾ç¤ºå½’æ¡£æ‘˜è¦"""
        print(f"\nğŸ“ RAGå½’æ¡£æ•°æ®æ‘˜è¦")
        print(f"ğŸ“‚ å½’æ¡£ç›®å½•: {self.archive_dir}")
        print("="*60)

        stats = self.get_archive_stats()

        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ–‡ä»¶æ•°é‡: {stats['total_files']}")
        print(f"   æ€»å¤§å°: {stats['total_size']:,} bytes ({stats['total_size']/1024/1024:.1f} MB)")

        if stats['data_types']:
            print(f"\nğŸ“‹ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
            for data_type, info in stats['data_types'].items():
                print(f"   {data_type}: {info['count']} ä¸ªæ–‡ä»¶, {info['total_records']} æ¡è®°å½•")

        if stats['newest_archive']:
            print(f"\nâ° æ—¶é—´èŒƒå›´:")
            print(f"   æœ€æ–°: {stats['newest_archive']}")
            print(f"   æœ€æ—§: {stats['oldest_archive']}")

    def cleanup_old_archives(self, keep_count: int = 10, data_type: str = None):
        """æ¸…ç†æ—§å½’æ¡£ï¼ˆä¿ç•™æœ€æ–°Nä¸ªï¼‰"""
        archives = self.list_archives(data_type)

        if len(archives) <= keep_count:
            print(f"ğŸ“ å½’æ¡£æ•°é‡ {len(archives)} <= {keep_count}ï¼Œæ— éœ€æ¸…ç†")
            return

        # åˆ é™¤æ—§æ–‡ä»¶
        to_delete = archives[keep_count:]
        deleted_count = 0

        for archive in to_delete:
            try:
                os.remove(archive['filepath'])
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§å½’æ¡£: {archive['filename']}")
                deleted_count += 1
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥ {archive['filename']}: {e}")

        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å½’æ¡£")

def main():
    """æ¼”ç¤ºå½’æ¡£ç®¡ç†åŠŸèƒ½"""
    manager = ArchiveManager()

    # æ˜¾ç¤ºå½“å‰å½’æ¡£çŠ¶æ€
    manager.show_archive_summary()

    # æ¼”ç¤ºä¿å­˜æ–°å½’æ¡£
    print(f"\nğŸ§ª æ¼”ç¤ºå½’æ¡£ä¿å­˜:")
    test_data = {
        'articles': [
            {'title': 'æµ‹è¯•æ–°é—»1', 'content': 'æµ‹è¯•å†…å®¹1'},
            {'title': 'æµ‹è¯•æ–°é—»2', 'content': 'æµ‹è¯•å†…å®¹2'}
        ]
    }

    manager.save_archive(
        data=test_data,
        data_type='rss_news',
        metadata={'source': 'test', 'quality': 'high'},
        custom_suffix='demo'
    )

    # æ›´æ–°åçš„æ‘˜è¦
    print(f"\nğŸ“Š æ›´æ–°åå½’æ¡£çŠ¶æ€:")
    manager.show_archive_summary()

if __name__ == "__main__":
    main()