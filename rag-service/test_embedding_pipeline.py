#!/usr/bin/env python3
"""
RAG Service å®Œæ•´æµæ°´çº¿æµ‹è¯•
æµ‹è¯•æ•°æ®è·å– â†’ åˆ‡ç‰‡ â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ æ£€ç´¢
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import json
import time
from typing import List

def test_complete_embedding_pipeline():
    """æµ‹è¯•å®Œæ•´çš„å‘é‡åŒ–æµæ°´çº¿"""
    print("ğŸš€ RAG Service å®Œæ•´æµæ°´çº¿æµ‹è¯•")
    print("=" * 60)

    # Clear proxy environment variables
    for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
        if proxy_var in os.environ:
            del os.environ[proxy_var]

    # Step 1: Import required modules
    print("\nğŸ“¦ Step 1: å¯¼å…¥æ¨¡å—")
    try:
        from app.services.vector_service import vector_service
        from app.utils.text_processing import clean_text, split_text_into_chunks, calculate_text_quality_score
        from app.models.requests import DocumentInput
        print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

    # Step 2: Connect to ChromaDB
    print("\nğŸ”Œ Step 2: è¿æ¥ChromaDB")
    try:
        success = vector_service.connect()
        if not success:
            print("âŒ ChromaDBè¿æ¥å¤±è´¥")
            return False

        # Get initial stats
        initial_stats = vector_service.get_collection_stats()
        print(f"âœ“ ChromaDBè¿æ¥æˆåŠŸ")
        print(f"  é›†åˆåç§°: {initial_stats['collection_name']}")
        print(f"  åˆå§‹æ–‡æ¡£æ•°é‡: {initial_stats['document_count']}")
    except Exception as e:
        print(f"âŒ ChromaDBè¿æ¥é”™è¯¯: {e}")
        return False

    # Step 3: Prepare test data (simulate stock analysis documents)
    print("\nğŸ“„ Step 3: å‡†å¤‡æµ‹è¯•æ•°æ®")

    test_documents = [
        {
            "id": "doc_stock_analysis_001",
            "raw_content": """
            <h1>å¹³å®‰é“¶è¡Œ(000001)2024å¹´ä¸‰å­£åº¦è´¢æŠ¥åˆ†æ</h1>
            <p>å¹³å®‰é“¶è¡Œå‘å¸ƒ2024å¹´ä¸‰å­£åº¦ä¸šç»©æŠ¥å‘Šï¼Œè¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿8.5%ï¼Œè¾¾åˆ°1256.7äº¿å…ƒã€‚
            å‡€åˆ©æ¶¦390.2äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿6.2%ã€‚èµ„äº§è´¨é‡æŒç»­æ”¹å–„ï¼Œä¸è‰¯è´·æ¬¾ç‡ä¸‹é™è‡³1.05%ã€‚</p>
            <p>æŠ•èµ„å»ºè®®ï¼šç»´æŒ"ä¹°å…¥"è¯„çº§ï¼Œç›®æ ‡ä»·20å…ƒã€‚å…¬å¸åŸºæœ¬é¢ç¨³å¥ï¼ŒROEæå‡æ˜æ˜¾ã€‚</p>
            """,
            "metadata": {
                "stock_code": "000001",
                "doc_type": "financial_report",
                "source": "ç ”æŠ¥åˆ†æ",
                "publish_time": "2024-10-30",
                "category": "ä¸šç»©åˆ†æ",
                "importance": 8
            }
        },
        {
            "id": "doc_market_news_002",
            "raw_content": """
            å¤®è¡Œå®£å¸ƒé™å‡†0.25ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾æµåŠ¨æ€§çº¦5000äº¿å…ƒã€‚æ­¤æ¬¡é™å‡†æ—¨åœ¨æ”¯æŒå®ä½“ç»æµå‘å±•ï¼Œ
            é™ä½ä¼ä¸šèèµ„æˆæœ¬ã€‚é“¶è¡Œè‚¡ã€åœ°äº§è‚¡ç­‰åˆ©ç‡æ•æ„Ÿæ€§è¡Œä¸šæœ‰æœ›å—ç›Šã€‚
            åˆ†æå¸ˆè®¤ä¸ºè¿™æ˜¯è´§å¸æ”¿ç­–è¾¹é™…å®½æ¾çš„ä¿¡å·ï¼Œå¯¹è‚¡å¸‚å½¢æˆåˆ©å¥½æ”¯æ’‘ã€‚
            """,
            "metadata": {
                "doc_type": "market_news",
                "source": "è´¢ç»æ–°é—»",
                "publish_time": "2024-10-29",
                "category": "æ”¿ç­–æ¶ˆæ¯",
                "sentiment": "positive",
                "importance": 9
            }
        },
        {
            "id": "doc_research_report_003",
            "raw_content": """
            ç§‘æŠ€æ¿å—è°ƒç ”æŠ¥å‘Šï¼šäººå·¥æ™ºèƒ½äº§ä¸šé“¾è¿æ¥æ–°æœºé‡

            éšç€AIå¤§æ¨¡å‹æŠ€æœ¯ä¸æ–­æˆç†Ÿï¼Œç›¸å…³äº§ä¸šé“¾å…¬å¸ä¸šç»©æ˜¾è‘—æ”¹å–„ã€‚é‡ç‚¹å…³æ³¨ï¼š
            1. èŠ¯ç‰‡è®¾è®¡å…¬å¸ï¼šå—ç›ŠäºAIèŠ¯ç‰‡éœ€æ±‚æ¿€å¢ï¼Œè®¢å•é¥±æ»¡
            2. äº‘è®¡ç®—æœåŠ¡å•†ï¼šAIè®­ç»ƒéœ€æ±‚å¸¦åŠ¨ç®—åŠ›æœåŠ¡æ”¶å…¥å¢é•¿
            3. åº”ç”¨è½¯ä»¶ä¼ä¸šï¼šAIèµ‹èƒ½ä¼ ç»Ÿè½¯ä»¶ï¼Œç”¨æˆ·ä»˜è´¹æ„æ„¿å¢å¼º

            å»ºè®®é‡ç‚¹å…³æ³¨ç›¸å…³é¾™å¤´ä¼ä¸šçš„æŠ•èµ„æœºä¼šã€‚
            """,
            "metadata": {
                "doc_type": "research_report",
                "source": "åˆ¸å•†ç ”æŠ¥",
                "publish_time": "2024-10-28",
                "category": "è¡Œä¸šåˆ†æ",
                "sector": "ç§‘æŠ€",
                "importance": 7
            }
        }
    ]

    print(f"âœ“ å‡†å¤‡äº† {len(test_documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
    for doc in test_documents:
        print(f"  - {doc['id']}: {doc['metadata']['doc_type']}")

    # Step 4: Text processing and chunking
    print("\nâœ‚ï¸  Step 4: æ–‡æœ¬å¤„ç†å’Œåˆ‡ç‰‡")

    processed_documents = []
    total_chunks = 0

    for doc in test_documents:
        print(f"\nå¤„ç†æ–‡æ¡£: {doc['id']}")

        # Clean text
        cleaned_content = clean_text(doc['raw_content'])
        print(f"  åŸå§‹é•¿åº¦: {len(doc['raw_content'])} â†’ æ¸…ç†å: {len(cleaned_content)}")

        # Calculate quality score
        quality_score = calculate_text_quality_score(cleaned_content)
        print(f"  è´¨é‡è¯„åˆ†: {quality_score:.3f}")

        # Split into chunks
        chunks = split_text_into_chunks(cleaned_content, max_chunk_size=200, overlap_size=30)
        print(f"  åˆ†ç‰‡æ•°é‡: {len(chunks)}")

        # Create document objects for each chunk
        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc['id']}_chunk_{i+1}"
            chunk_metadata = doc['metadata'].copy()
            chunk_metadata.update({
                'parent_document_id': doc['id'],
                'chunk_index': i + 1,
                'chunk_total': len(chunks),
                'quality_score': quality_score,
                'chunk_length': len(chunk)
            })

            processed_doc = DocumentInput(
                id=chunk_id,
                content=chunk,
                metadata=chunk_metadata
            )
            processed_documents.append(processed_doc)
            total_chunks += 1

    print(f"\nâœ“ æ–‡æœ¬å¤„ç†å®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {total_chunks} ä¸ªæ–‡æ¡£åˆ†ç‰‡")

    # Step 5: Mock embedding generation (simulate bge-large-zh-v1.5)
    print("\nğŸ§  Step 5: ç”Ÿæˆå‘é‡åµŒå…¥ (æ¨¡æ‹Ÿbge-large-zh-v1.5)")

    try:
        # Generate mock embeddings (768 dimensions like bge-large-zh-v1.5)
        print("æ­£åœ¨ç”Ÿæˆ768ç»´å‘é‡...")
        embeddings = []

        for i, doc in enumerate(processed_documents):
            # Create pseudo-realistic embeddings based on content
            # Different content types get different base vectors
            if 'financial_report' in doc.metadata.get('doc_type', ''):
                base_vector = [0.1 + (i * 0.01)] * 768
            elif 'market_news' in doc.metadata.get('doc_type', ''):
                base_vector = [0.2 + (i * 0.01)] * 768
            else:  # research_report
                base_vector = [0.3 + (i * 0.01)] * 768

            embeddings.append(base_vector)

        print(f"âœ“ ç”Ÿæˆäº† {len(embeddings)} ä¸ª768ç»´å‘é‡")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0])}")
        print(f"  ç¤ºä¾‹å‘é‡èŒƒå›´: [{embeddings[0][0]:.3f}, {embeddings[0][767]:.3f}]")

    except Exception as e:
        print(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
        return False

    # Step 6: Store documents in ChromaDB
    print("\nğŸ’¾ Step 6: å­˜å‚¨æ–‡æ¡£åˆ°ChromaDB")

    try:
        start_time = time.time()
        processed_count, failed_docs = vector_service.add_documents(
            documents=processed_documents,
            embeddings=embeddings,
            collection_name=None  # Use default collection
        )
        storage_time = time.time() - start_time

        print(f"âœ“ æ–‡æ¡£å­˜å‚¨å®Œæˆ")
        print(f"  å¤„ç†æˆåŠŸ: {processed_count} ä¸ªæ–‡æ¡£")
        print(f"  å¤„ç†å¤±è´¥: {len(failed_docs)} ä¸ªæ–‡æ¡£")
        print(f"  å­˜å‚¨è€—æ—¶: {storage_time:.3f} ç§’")

        if failed_docs:
            print(f"  å¤±è´¥æ–‡æ¡£: {failed_docs}")

    except Exception as e:
        print(f"âŒ æ–‡æ¡£å­˜å‚¨å¤±è´¥: {e}")
        return False

    # Step 7: Verify storage and get database status
    print("\nğŸ“Š Step 7: éªŒè¯å­˜å‚¨çŠ¶æ€")

    try:
        final_stats = vector_service.get_collection_stats()

        print(f"âœ“ æ•°æ®åº“çŠ¶æ€éªŒè¯")
        print(f"  é›†åˆåç§°: {final_stats['collection_name']}")
        print(f"  æœ€ç»ˆæ–‡æ¡£æ•°: {final_stats['document_count']}")
        print(f"  æ–°å¢æ–‡æ¡£æ•°: {final_stats['document_count'] - initial_stats['document_count']}")
        print(f"  çŠ¶æ€: {final_stats['status']}")

    except Exception as e:
        print(f"âŒ çŠ¶æ€éªŒè¯å¤±è´¥: {e}")
        return False

    # Step 8: Test semantic search
    print("\nğŸ” Step 8: æµ‹è¯•è¯­ä¹‰æœç´¢")

    test_queries = [
        "å¹³å®‰é“¶è¡Œè´¢åŠ¡è¡¨ç°å¦‚ä½•",
        "å¤®è¡Œè´§å¸æ”¿ç­–å¯¹å¸‚åœºå½±å“",
        "äººå·¥æ™ºèƒ½è¡Œä¸šæŠ•èµ„æœºä¼š"
    ]

    try:
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")

            # Generate mock query embedding
            query_embedding = [0.15] * 768  # Simple mock query vector

            # Perform search
            search_start = time.time()
            results = vector_service.search_similar_documents(
                query_embedding=query_embedding,
                limit=3,
                similarity_threshold=0.5
            )
            search_time = time.time() - search_start

            print(f"  æœç´¢è€—æ—¶: {search_time:.3f} ç§’")
            print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")

            for i, match in enumerate(results, 1):
                print(f"    {i}. {match.document_id} (ç›¸ä¼¼åº¦: {match.similarity_score:.3f})")
                print(f"       å†…å®¹é¢„è§ˆ: {match.content[:60]}...")
                print(f"       æ–‡æ¡£ç±»å‹: {match.metadata.get('doc_type', 'unknown')}")

    except Exception as e:
        print(f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
        return False

    # Step 9: Performance summary
    print("\nğŸ“ˆ Step 9: æ€§èƒ½æ€»ç»“")

    print(f"âœ“ æµæ°´çº¿æµ‹è¯•å®Œæˆ!")
    print(f"  å¤„ç†æ–‡æ¡£: {len(test_documents)} ä¸ªåŸå§‹æ–‡æ¡£")
    print(f"  ç”Ÿæˆåˆ†ç‰‡: {total_chunks} ä¸ªæ–‡æ¡£åˆ†ç‰‡")
    print(f"  å‘é‡ç»´åº¦: 768 ç»´")
    print(f"  å­˜å‚¨æˆåŠŸ: {processed_count} ä¸ªæ–‡æ¡£")
    print(f"  æ•°æ®åº“çŠ¶æ€: æ­£å¸¸")
    print(f"  æœç´¢åŠŸèƒ½: æ­£å¸¸")

    return True

def cleanup_test_documents():
    """æ¸…ç†æµ‹è¯•æ–‡æ¡£"""
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®")

    try:
        from app.services.vector_service import vector_service

        # Delete test documents
        test_doc_ids = [
            "doc_stock_analysis_001_chunk_1",
            "doc_stock_analysis_001_chunk_2",
            "doc_market_news_002_chunk_1",
            "doc_market_news_002_chunk_2",
            "doc_research_report_003_chunk_1",
            "doc_research_report_003_chunk_2",
            "doc_research_report_003_chunk_3",
        ]

        deleted_count = 0
        for doc_id in test_doc_ids:
            try:
                if vector_service.delete_document(doc_id):
                    deleted_count += 1
            except:
                pass  # Ignore errors for non-existent documents

        print(f"âœ“ æ¸…ç†äº† {deleted_count} ä¸ªæµ‹è¯•æ–‡æ¡£")

    except Exception as e:
        print(f"âš  æ¸…ç†è­¦å‘Š: {e}")

if __name__ == "__main__":
    print("ğŸ§ª RAG Service å®Œæ•´å‘é‡åŒ–æµæ°´çº¿æµ‹è¯•")
    print("=" * 60)

    try:
        success = test_complete_embedding_pipeline()

        if success:
            print("\n" + "=" * 60)
            print("ğŸ‰ æµ‹è¯•é€šè¿‡! RAG Service å‘é‡åŒ–æµæ°´çº¿å®Œå…¨æ­£å¸¸")
            print("âœ¨ åŠŸèƒ½éªŒè¯:")
            print("  âœ“ æ•°æ®è·å–å’Œé¢„å¤„ç†")
            print("  âœ“ æ–‡æœ¬æ¸…æ´—å’Œè´¨é‡è¯„ä¼°")
            print("  âœ“ æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡")
            print("  âœ“ å‘é‡åµŒå…¥ç”Ÿæˆ (æ¨¡æ‹Ÿ)")
            print("  âœ“ ChromaDBå‘é‡å­˜å‚¨")
            print("  âœ“ è¯­ä¹‰æœç´¢æ£€ç´¢")
            print("  âœ“ æ•°æ®åº“çŠ¶æ€ç®¡ç†")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # Always try to cleanup
        cleanup_test_documents()