#!/usr/bin/env python3
"""
æµ‹è¯•çœŸå®æ•°æ®æ”¶é›†ï¼ˆä¸å«å‘é‡åŒ–ï¼‰
å…ˆæµ‹è¯•æ•°æ®æºæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import re

class SimpleDataCollector:
    """ç®€åŒ–çš„æ•°æ®æ”¶é›†å™¨ï¼Œä»…æµ‹è¯•æ•°æ®æº"""

    def __init__(self):
        self.session = None
        self.collected_data = []

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–ç®€åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿ...")

        # è®¾ç½®HTTPä¼šè¯
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Linux; U; Android 4.0.3; ko-kr; LG-L160L Build/IML74K) AppleWebkit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'
            }
        )

        print("âœ… ç®€åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def collect_akshare_data(self) -> List[Dict]:
        """ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®"""
        print("ğŸ“Š å¼€å§‹ä½¿ç”¨AKShareæ”¶é›†çœŸå®é‡‘èæ•°æ®...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # 1. è·å–Aè‚¡å®æ—¶æ–°é—»
            print("   è·å–Aè‚¡æ–°é—»...")
            try:
                news_data = ak.news_cctv()  # å¤®è§†è´¢ç»æ–°é—»
                print(f"   âœ… è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                for _, news in news_data.head(3).iterrows():  # å–å‰3æ¡
                    doc = {
                        'id': f"akshare_news_{int(time.time())}_{len(collected_docs)}",
                        'content': f"{news.get('title', '')} {news.get('content', '')}",
                        'metadata': {
                            'source': 'AKShare-CCTVè´¢ç»',
                            'doc_type': 'financial_news',
                            'title': news.get('title', ''),
                            'publish_date': str(news.get('date', datetime.now().date())),
                            'data_source': 'akshare_cctv_news',
                            'importance': 7,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)

            except Exception as e:
                print(f"   âš ï¸ è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")

            # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            print("   è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            try:
                # è·å–å¹³å®‰é“¶è¡Œç®€å•è‚¡ç¥¨æ•°æ®
                stock_info = ak.stock_zh_a_hist(symbol="000001", period="daily", start_date="20241101", end_date="20241101")
                if not stock_info.empty:
                    latest_data = stock_info.iloc[-1]
                    doc = {
                        'id': f"akshare_stock_000001_{int(time.time())}",
                        'content': f"å¹³å®‰é“¶è¡Œ(000001)æœ€æ–°äº¤æ˜“æ•°æ®ï¼šå¼€ç›˜ä»·{latest_data.get('å¼€ç›˜', 'N/A')}ï¼Œæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}ã€‚",
                        'metadata': {
                            'source': 'AKShare-è‚¡ç¥¨æ•°æ®',
                            'doc_type': 'stock_data',
                            'stock_code': '000001',
                            'company_name': 'å¹³å®‰é“¶è¡Œ',
                            'open_price': float(latest_data.get('å¼€ç›˜', 0)),
                            'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                            'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                            'date': str(latest_data.name),
                            'data_source': 'akshare_stock_hist',
                            'importance': 8,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)
                    print(f"   âœ… è·å–åˆ°å¹³å®‰é“¶è¡Œè‚¡ç¥¨æ•°æ®")

            except Exception as e:
                print(f"   âš ï¸ è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… AKShareæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…ï¼Œè·³è¿‡AKShareæ•°æ®æ”¶é›†")
            return []
        except Exception as e:
            print(f"âŒ AKShareæ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    async def collect_rss_data(self) -> List[Dict]:
        """æ”¶é›†RSSæ–°é—»æ•°æ®"""
        print("ğŸ“¡ å¼€å§‹æ”¶é›†RSSæ–°é—»æ•°æ®...")

        rss_feeds = [
            {
                'url': 'http://www.cs.com.cn/xwzx/hwxx/rss.xml',
                'name': 'ä¸­è¯ç½‘-æµ·å¤–æ–°é—»',
                'category': 'overseas_news'
            }
        ]

        collected_docs = []

        for feed_info in rss_feeds:
            try:
                print(f"   å¤„ç†RSSæº: {feed_info['name']}")

                # å°è¯•è§£æRSS
                feed = feedparser.parse(feed_info['url'])

                if feed.entries:
                    print(f"   âœ… è·å–åˆ° {len(feed.entries)} ä¸ªRSSæ¡ç›®")

                    for i, entry in enumerate(feed.entries[:2]):  # å–å‰2æ¡
                        # æ¸…ç†HTMLæ ‡ç­¾
                        content = BeautifulSoup(entry.get('summary', entry.get('description', '')), 'html.parser').get_text()

                        doc = {
                            'id': f"rss_{feed_info['category']}_{int(time.time())}_{i}",
                            'content': f"{entry.get('title', '')} {content}",
                            'metadata': {
                                'source': feed_info['name'],
                                'doc_type': 'rss_news',
                                'category': feed_info['category'],
                                'title': entry.get('title', ''),
                                'link': entry.get('link', ''),
                                'published': entry.get('published', ''),
                                'rss_url': feed_info['url'],
                                'importance': 6,
                                'collection_time': datetime.now().isoformat()
                            }
                        }
                        collected_docs.append(doc)

                else:
                    print(f"   âš ï¸ RSSæºæ— æ•°æ®: {feed_info['name']}")

            except Exception as e:
                print(f"   âŒ RSSæºå¤„ç†å¤±è´¥ {feed_info['name']}: {e}")

        print(f"âœ… RSSæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
        return collected_docs

    async def collect_web_scraping_data(self) -> List[Dict]:
        """ç½‘é¡µçˆ¬å–æ•°æ®"""
        print("ğŸ•·ï¸ å¼€å§‹ç½‘é¡µçˆ¬å–æ•°æ®...")

        web_sources = [
            {
                'url': 'https://finance.sina.com.cn/',
                'name': 'æ–°æµªè´¢ç»',
                'type': 'financial_news'
            }
        ]

        collected_docs = []

        if not self.session:
            print("âŒ HTTPä¼šè¯æœªåˆå§‹åŒ–")
            return collected_docs

        for source in web_sources:
            try:
                print(f"   çˆ¬å–ç½‘ç«™: {source['name']}")

                async with self.session.get(source['url']) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        soup = BeautifulSoup(html_content, 'html.parser')

                        # æå–æ ‡é¢˜å’Œæ‘˜è¦
                        titles = soup.find_all(['h1', 'h2', 'h3'], limit=2)
                        for i, title in enumerate(titles):
                            title_text = title.get_text(strip=True)
                            if len(title_text) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ ‡é¢˜

                                # å°è¯•æ‰¾åˆ°ç›¸å…³æ®µè½
                                content = title_text
                                next_elem = title.find_next(['p', 'div'], class_=lambda x: x and 'content' in str(x).lower())
                                if next_elem:
                                    content += " " + next_elem.get_text(strip=True)[:200]

                                doc = {
                                    'id': f"webscrape_{source['type']}_{int(time.time())}_{i}",
                                    'content': content,
                                    'metadata': {
                                        'source': source['name'],
                                        'doc_type': 'web_scraped',
                                        'website_type': source['type'],
                                        'url': source['url'],
                                        'title': title_text,
                                        'scrape_time': datetime.now().isoformat(),
                                        'importance': 5,
                                        'collection_time': datetime.now().isoformat()
                                    }
                                }
                                collected_docs.append(doc)

                        print(f"   âœ… ä» {source['name']} æå–äº† {len([t for t in titles if len(t.get_text(strip=True)) > 10])} ä¸ªå†…å®¹")

                    else:
                        print(f"   âš ï¸ ç½‘ç«™è®¿é—®å¤±è´¥ {source['name']}: HTTP {response.status}")

            except Exception as e:
                print(f"   âŒ ç½‘ç«™çˆ¬å–å¤±è´¥ {source['name']}: {e}")

        print(f"âœ… ç½‘é¡µçˆ¬å–å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
        return collected_docs

    async def collect_all_real_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®æº...")
        start_time = time.time()

        all_docs = []

        # 1. AKShareæ•°æ®
        akshare_docs = await self.collect_akshare_data()
        all_docs.extend(akshare_docs)

        # 2. RSSæ•°æ®
        rss_docs = await self.collect_rss_data()
        all_docs.extend(rss_docs)

        # 3. ç½‘é¡µçˆ¬å–æ•°æ®
        web_docs = await self.collect_web_scraping_data()
        all_docs.extend(web_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ çœŸå®æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ æ•°æ®æºåˆ†å¸ƒ:")
        print(f"      - AKShare: {len(akshare_docs)} ä¸ª")
        print(f"      - RSS: {len(rss_docs)} ä¸ª")
        print(f"      - ç½‘é¡µçˆ¬å–: {len(web_docs)} ä¸ª")

        return all_docs

async def test_data_sources():
    """æµ‹è¯•æ•°æ®æºæ”¶é›†"""
    print("ğŸ§ª æµ‹è¯•çœŸå®æ•°æ®æº")
    print("=" * 60)

    async with SimpleDataCollector() as collector:
        # æ”¶é›†æ‰€æœ‰çœŸå®æ•°æ®
        real_docs = await collector.collect_all_real_data()

        if real_docs:
            print(f"\nğŸ“‹ æ”¶é›†åˆ°çš„çœŸå®æ•°æ®é¢„è§ˆ:")
            print("=" * 60)

            for i, doc in enumerate(real_docs, 1):
                print(f"\nğŸ“„ æ–‡æ¡£ {i}: {doc['id']}")
                print(f"   æ¥æº: {doc['metadata']['source']}")
                print(f"   ç±»å‹: {doc['metadata']['doc_type']}")
                print(f"   å†…å®¹: {doc['content'][:100]}...")
                print("-" * 50)

            # ä¿å­˜åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
            output_file = f"/home/wyatt/prism2/rag-service/data_sources_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(real_docs, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

            return real_docs
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•çœŸå®æ•°æ®")
            return []

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…...")
    try:
        import akshare
        print("âœ… AKShare å·²å®‰è£…")
    except ImportError:
        print("âš ï¸ AKShare æœªå®‰è£…")

    try:
        import feedparser
        print("âœ… feedparser å·²å®‰è£…")
    except ImportError:
        print("âŒ feedparser æœªå®‰è£…")

    try:
        import aiohttp
        print("âœ… aiohttp å·²å®‰è£…")
    except ImportError:
        print("âŒ aiohttp æœªå®‰è£…")

    try:
        from bs4 import BeautifulSoup
        print("âœ… BeautifulSoup å·²å®‰è£…")
    except ImportError:
        print("âŒ BeautifulSoup æœªå®‰è£…")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_data_sources())