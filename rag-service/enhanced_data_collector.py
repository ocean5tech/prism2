#!/usr/bin/env python3
"""
å¢å¼ºçš„çœŸå®æ•°æ®æ”¶é›†å™¨
æ‰©å¤§æ•°æ®æºï¼šæ›´å¤šRSSæºã€Aè‚¡è¡Œä¸šæ•°æ®ã€è‹±æ–‡ç¿»è¯‘
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import re
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
from googletrans import Translator

class EnhancedDataCollector:
    """å¢å¼ºçš„æ•°æ®æ”¶é›†å™¨ï¼šæ›´å¤šæ•°æ®æº + è‹±æ–‡ç¿»è¯‘"""

    def __init__(self):
        self.session = None
        self.collected_data = []
        self.vectorizer = None
        self.client = None
        self.collection = None
        self.translator = Translator()

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–å¢å¼ºæ•°æ®æ”¶é›†ç³»ç»Ÿ...")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        print("ğŸ“¥ åˆå§‹åŒ–TF-IDFä¸­æ–‡å‘é‡åŒ–å™¨...")
        self.vectorizer = TfidfVectorizer(
            max_features=768,
            analyzer='word',
            tokenizer=lambda x: jieba.lcut(x),
            token_pattern=None,
            lowercase=False,
            stop_words=None
        )

        # è¿æ¥åˆ°ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        self.client = chromadb.HttpClient(host='localhost', port=8000)
        self.client.heartbeat()

        try:
            self.client.delete_collection("financial_documents")
            print("   ğŸ—‘ï¸ åˆ é™¤ç°æœ‰é›†åˆ")
        except:
            pass

        self.collection = self.client.create_collection("financial_documents")
        print("âœ… ChromaDBè¿æ¥æˆåŠŸï¼Œåˆ›å»ºæ–°é›†åˆ")

        # è®¾ç½®HTTPä¼šè¯
        connector = aiohttp.TCPConnector(limit=20)
        timeout = aiohttp.ClientTimeout(total=60)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )

        print("âœ… å¢å¼ºæ•°æ®æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def translate_to_chinese(self, text: str, max_length: int = 5000) -> str:
        """å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡"""
        try:
            if len(text) > max_length:
                text = text[:max_length]

            # æ£€æµ‹è¯­è¨€
            detected = self.translator.detect(text)
            if detected.lang == 'zh' or detected.lang == 'zh-cn':
                return text

            # ç¿»è¯‘æˆä¸­æ–‡
            translated = self.translator.translate(text, dest='zh-cn')
            return translated.text
        except Exception as e:
            print(f"   âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
            return text  # è¿”å›åŸæ–‡

    async def collect_enhanced_rss_data(self) -> List[Dict]:
        """æ”¶é›†å¢å¼ºçš„RSSæ–°é—»æ•°æ®"""
        print("ğŸ“¡ å¼€å§‹æ”¶é›†å¢å¼ºRSSæ–°é—»æ•°æ®...")

        rss_feeds = [
            # ä¸­æ–‡è´¢ç»æº
            {
                'url': 'http://www.cs.com.cn/xwzx/hwxx/rss.xml',
                'name': 'ä¸­è¯ç½‘-æµ·å¤–æ–°é—»',
                'category': 'overseas_news',
                'language': 'zh'
            },
            {
                'url': 'http://finance.sina.com.cn/roll/index.d.html?cid=56588&page=1',
                'name': 'æ–°æµªè´¢ç»',
                'category': 'financial_news',
                'language': 'zh'
            },
            # å‡¤å‡°è´¢ç»
            {
                'url': 'http://finance.ifeng.com/rss/index.xml',
                'name': 'å‡¤å‡°è´¢ç»',
                'category': 'financial_news',
                'language': 'zh'
            },
            # è‹±æ–‡è´¢ç»æºï¼ˆéœ€è¦ç¿»è¯‘ï¼‰
            {
                'url': 'https://feeds.bloomberg.com/markets/news.rss',
                'name': 'Bloomberg Markets',
                'category': 'international_markets',
                'language': 'en'
            },
            {
                'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
                'name': 'Yahoo Finance',
                'category': 'international_finance',
                'language': 'en'
            }
        ]

        collected_docs = []

        for feed_info in rss_feeds:
            try:
                print(f"   å¤„ç†RSSæº: {feed_info['name']} ({feed_info['language']})")

                # å°è¯•è§£æRSS
                feed = feedparser.parse(feed_info['url'])

                if feed.entries:
                    print(f"   âœ… è·å–åˆ° {len(feed.entries)} ä¸ªRSSæ¡ç›®")

                    for i, entry in enumerate(feed.entries[:5]):  # å¢åŠ åˆ°å‰5æ¡
                        # æ¸…ç†HTMLæ ‡ç­¾
                        content = BeautifulSoup(entry.get('summary', entry.get('description', '')), 'html.parser').get_text()
                        title = entry.get('title', '')

                        full_content = f"{title} {content}"

                        # å¦‚æœæ˜¯è‹±æ–‡ï¼Œç¿»è¯‘æˆä¸­æ–‡
                        if feed_info['language'] == 'en':
                            print(f"   ğŸ”„ ç¿»è¯‘è‹±æ–‡å†…å®¹: {title[:50]}...")
                            full_content = self.translate_to_chinese(full_content)

                        doc = {
                            'id': f"rss_{feed_info['category']}_{int(time.time())}_{i}",
                            'content': full_content,
                            'metadata': {
                                'source': feed_info['name'],
                                'doc_type': 'rss_news',
                                'category': feed_info['category'],
                                'title': title,
                                'link': entry.get('link', ''),
                                'published': entry.get('published', ''),
                                'rss_url': feed_info['url'],
                                'language': feed_info['language'],
                                'importance': 6,
                                'collection_time': datetime.now().isoformat()
                            }
                        }
                        collected_docs.append(doc)

                else:
                    print(f"   âš ï¸ RSSæºæ— æ•°æ®: {feed_info['name']}")

            except Exception as e:
                print(f"   âŒ RSSæºå¤„ç†å¤±è´¥ {feed_info['name']}: {e}")

        print(f"âœ… RSSæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
        return collected_docs

    async def collect_sector_stocks_data(self) -> List[Dict]:
        """æ”¶é›†Aè‚¡è¡Œä¸šè‚¡ç¥¨æ•°æ®ï¼šè®¡ç®—æœºã€èŠ¯ç‰‡ã€æœ‰è‰²é‡‘å±"""
        print("ğŸ“Š å¼€å§‹æ”¶é›†Aè‚¡è¡Œä¸šè‚¡ç¥¨æ•°æ®...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # å®šä¹‰ç›®æ ‡è¡Œä¸šå’Œç›¸å…³è‚¡ç¥¨
            sectors = {
                'è®¡ç®—æœº': ['002230', '000938', '002415', '300014', '000977'],  # ç§‘å¤§å›½åˆ›ã€ç´«å…‰è‚¡ä»½ã€æ±‡çº³ç§‘æŠ€ã€äº¿çº¬é”‚èƒ½ã€æµªæ½®ä¿¡æ¯
                'èŠ¯ç‰‡': ['000725', '002049', '002938', '688981', '688012'],    # äº¬ä¸œæ–¹ã€ç´«å…‰å›½å¾®ã€é¹é¼æ§è‚¡ã€ä¸­èŠ¯å›½é™…ã€ä¸­å¾®å…¬å¸
                'æœ‰è‰²é‡‘å±': ['000831', '002460', '600362', '000878', '600219'] # äº”çŸ¿å‘å±•ã€èµ£é”‹é”‚ä¸šã€æ±Ÿè¥¿é“œä¸šã€äº‘å—é“œä¸šã€å—å±±é“ä¸š
            }

            for sector_name, stock_codes in sectors.items():
                print(f"   æ”¶é›†{sector_name}è¡Œä¸šè‚¡ç¥¨æ•°æ®...")

                for stock_code in stock_codes:
                    try:
                        # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                        stock_info = ak.stock_zh_a_hist(
                            symbol=stock_code,
                            period="daily",
                            start_date="20241101",
                            end_date="20241101"
                        )

                        if not stock_info.empty:
                            latest_data = stock_info.iloc[-1]

                            # è·å–è‚¡ç¥¨åç§°
                            try:
                                stock_name_df = ak.stock_zh_a_spot_em()
                                stock_name = stock_name_df[stock_name_df['ä»£ç '] == stock_code]['åç§°'].iloc[0]
                            except:
                                stock_name = f"è‚¡ç¥¨{stock_code}"

                            content = f"{sector_name}è¡Œä¸šè‚¡ç¥¨{stock_name}({stock_code})æœ€æ–°äº¤æ˜“æ•°æ®ï¼šå¼€ç›˜ä»·{latest_data.get('å¼€ç›˜', 'N/A')}ï¼Œæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}ã€‚è¯¥è‚¡å±äº{sector_name}æ¿å—ï¼Œä¸ºé‡è¦çš„ç§‘æŠ€ç±»æŠ•èµ„æ ‡çš„ã€‚"

                            doc = {
                                'id': f"akshare_stock_{sector_name}_{stock_code}_{int(time.time())}",
                                'content': content,
                                'metadata': {
                                    'source': f'AKShare-{sector_name}è‚¡ç¥¨',
                                    'doc_type': 'sector_stock_data',
                                    'sector': sector_name,
                                    'stock_code': stock_code,
                                    'company_name': stock_name,
                                    'open_price': float(latest_data.get('å¼€ç›˜', 0)),
                                    'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                                    'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                                    'date': str(latest_data.name),
                                    'data_source': 'akshare_sector_stocks',
                                    'importance': 7,
                                    'collection_time': datetime.now().isoformat()
                                }
                            }
                            collected_docs.append(doc)
                            print(f"   âœ… è·å–åˆ°{sector_name}-{stock_name}æ•°æ®")

                    except Exception as e:
                        print(f"   âš ï¸ è·å–è‚¡ç¥¨{stock_code}æ•°æ®å¤±è´¥: {e}")

            # è·å–è¡Œä¸šæŒ‡æ•°ä¿¡æ¯
            print("   æ”¶é›†è¡Œä¸šæŒ‡æ•°æ•°æ®...")
            try:
                # ç§‘æŠ€100æŒ‡æ•°
                tech_index = ak.stock_zh_index_daily(symbol="sh000903")
                if not tech_index.empty:
                    latest_tech = tech_index.iloc[-1]
                    doc = {
                        'id': f"akshare_index_tech100_{int(time.time())}",
                        'content': f"ä¸­è¯ç§‘æŠ€100æŒ‡æ•°æœ€æ–°æ•°æ®ï¼šæ”¶ç›˜ç‚¹ä½{latest_tech.get('close', 'N/A')}ï¼Œæ¶¨è·Œå¹…åæ˜ ç§‘æŠ€è‚¡æ•´ä½“è¡¨ç°ã€‚è¯¥æŒ‡æ•°åŒ…å«è®¡ç®—æœºã€èŠ¯ç‰‡ã€é€šä¿¡ç­‰ç§‘æŠ€é¾™å¤´è‚¡ç¥¨ï¼Œæ˜¯ç§‘æŠ€æ¿å—é‡è¦é£å‘æ ‡ã€‚",
                        'metadata': {
                            'source': 'AKShare-è¡Œä¸šæŒ‡æ•°',
                            'doc_type': 'index_data',
                            'index_name': 'ä¸­è¯ç§‘æŠ€100',
                            'index_code': 'sh000903',
                            'close_price': float(latest_tech.get('close', 0)),
                            'data_source': 'akshare_index',
                            'importance': 8,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)

            except Exception as e:
                print(f"   âš ï¸ è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… Aè‚¡è¡Œä¸šæ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…ï¼Œè·³è¿‡è‚¡ç¥¨æ•°æ®æ”¶é›†")
            return []
        except Exception as e:
            print(f"âŒ è‚¡ç¥¨æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    async def collect_financial_news_data(self) -> List[Dict]:
        """æ”¶é›†æ›´å¤šè´¢ç»æ–°é—»æ•°æ®"""
        print("ğŸ“° å¼€å§‹æ”¶é›†æ›´å¤šè´¢ç»æ–°é—»æ•°æ®...")

        try:
            import akshare as ak

            collected_docs = []

            # 1. è·å–æ›´å¤šæ–°é—»ç±»å‹
            news_sources = [
                ('news_cctv', 'å¤®è§†è´¢ç»æ–°é—»'),
                ('news_sina', 'æ–°æµªè´¢ç»æ–°é—»'),
                ('news_163', 'ç½‘æ˜“è´¢ç»æ–°é—»')
            ]

            for source_func, source_name in news_sources:
                try:
                    print(f"   è·å–{source_name}...")

                    if source_func == 'news_cctv':
                        news_data = ak.news_cctv()
                    elif source_func == 'news_sina':
                        news_data = ak.news_sina()
                    else:
                        continue  # è·³è¿‡ä¸æ”¯æŒçš„æº

                    if not news_data.empty:
                        print(f"   âœ… è·å–åˆ° {len(news_data)} æ¡{source_name}")

                        for _, news in news_data.head(10).iterrows():  # å¢åŠ åˆ°å‰10æ¡
                            doc = {
                                'id': f"akshare_{source_func}_{int(time.time())}_{len(collected_docs)}",
                                'content': f"{news.get('title', '')} {news.get('content', '')}",
                                'metadata': {
                                    'source': source_name,
                                    'doc_type': 'financial_news',
                                    'title': news.get('title', ''),
                                    'publish_date': str(news.get('date', datetime.now().date())),
                                    'data_source': source_func,
                                    'importance': 7,
                                    'collection_time': datetime.now().isoformat()
                                }
                            }
                            collected_docs.append(doc)

                except Exception as e:
                    print(f"   âš ï¸ è·å–{source_name}å¤±è´¥: {e}")

            # 2. è·å–å®è§‚ç»æµæ•°æ®
            print("   è·å–å®è§‚ç»æµæ•°æ®...")
            try:
                # PMIæ•°æ®
                pmi_data = ak.macro_china_pmi()
                if not pmi_data.empty:
                    latest_pmi = pmi_data.iloc[-1]
                    doc = {
                        'id': f"akshare_macro_pmi_{int(time.time())}",
                        'content': f"ä¸­å›½åˆ¶é€ ä¸šPMIæ•°æ®ï¼š{latest_pmi.get('æœˆä»½', 'N/A')}æœˆPMIæŒ‡æ•°ä¸º{latest_pmi.get('PMI', 'N/A')}ï¼Œåæ˜ åˆ¶é€ ä¸šæ™¯æ°”ç¨‹åº¦ã€‚PMIé«˜äº50è¡¨æ˜åˆ¶é€ ä¸šæ‰©å¼ ï¼Œæ˜¯é‡è¦çš„ç»æµå…ˆè¡ŒæŒ‡æ ‡ã€‚",
                        'metadata': {
                            'source': 'AKShare-å®è§‚æ•°æ®',
                            'doc_type': 'macro_economic',
                            'indicator': 'PMIåˆ¶é€ ä¸šæŒ‡æ•°',
                            'value': str(latest_pmi.get('PMI', '')),
                            'period': str(latest_pmi.get('æœˆä»½', '')),
                            'data_source': 'akshare_macro_pmi',
                            'importance': 9,
                            'collection_time': datetime.now().isoformat()
                        }
                    }
                    collected_docs.append(doc)

            except Exception as e:
                print(f"   âš ï¸ è·å–å®è§‚æ•°æ®å¤±è´¥: {e}")

            print(f"âœ… è´¢ç»æ–°é—»æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            return collected_docs

        except Exception as e:
            print(f"âŒ è´¢ç»æ–°é—»æ”¶é›†å¤±è´¥: {e}")
            return []

    def store_documents_to_vector_db(self, documents: List[Dict]) -> int:
        """å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        if not documents:
            return 0

        print(f"ğŸ§  å¼€å§‹å‘é‡åŒ–å­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£...")

        try:
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            print("   ğŸ”„ ç”ŸæˆTF-IDFå‘é‡åµŒå…¥...")
            start_time = time.time()
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            embeddings = tfidf_matrix.toarray()
            embedding_time = time.time() - start_time

            print(f"   âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’")
            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]}")

            print("   ğŸ’¾ å­˜å‚¨åˆ°ChromaDB...")
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=doc_ids
            )

            print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return len(documents)

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            return 0

    async def collect_all_enhanced_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰å¢å¼ºæ•°æ®"""
        print("ğŸš€ å¼€å§‹æ”¶é›†å¤§è§„æ¨¡å¢å¼ºæ•°æ®...")
        start_time = time.time()

        all_docs = []

        # 1. å¢å¼ºRSSæ•°æ®
        rss_docs = await self.collect_enhanced_rss_data()
        all_docs.extend(rss_docs)

        # 2. Aè‚¡è¡Œä¸šè‚¡ç¥¨æ•°æ®
        stock_docs = await self.collect_sector_stocks_data()
        all_docs.extend(stock_docs)

        # 3. æ›´å¤šè´¢ç»æ–°é—»
        news_docs = await self.collect_financial_news_data()
        all_docs.extend(news_docs)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ å¤§è§„æ¨¡æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   ğŸ“„ æ•°æ®æºåˆ†å¸ƒ:")
        print(f"      - RSSæ–°é—»: {len(rss_docs)} ä¸ª")
        print(f"      - Aè‚¡è¡Œä¸šæ•°æ®: {len(stock_docs)} ä¸ª")
        print(f"      - è´¢ç»æ–°é—»: {len(news_docs)} ä¸ª")

        # å‘é‡åŒ–å­˜å‚¨
        if all_docs:
            print(f"\nğŸ§  å¼€å§‹å¤§è§„æ¨¡å‘é‡åŒ–å­˜å‚¨...")
            stored_count = self.store_documents_to_vector_db(all_docs)
            print(f"âœ… å¤§è§„æ¨¡å‘é‡åŒ–å­˜å‚¨å®Œæˆ: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")

        return all_docs

async def test_enhanced_pipeline():
    """æµ‹è¯•å¢å¼ºæ•°æ®ç®¡é“"""
    print("ğŸ§ª æµ‹è¯•å¤§è§„æ¨¡å¢å¼ºæ•°æ®ç®¡é“")
    print("=" * 80)

    async with EnhancedDataCollector() as collector:
        # æ”¶é›†å¤§è§„æ¨¡æ•°æ®
        all_docs = await collector.collect_all_enhanced_data()

        if all_docs:
            # ä¿å­˜æ•°æ®
            output_file = f"/home/wyatt/prism2/rag-service/enhanced_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_docs, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¤§è§„æ¨¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

            # éªŒè¯æ•°æ®åº“
            count = collector.collection.count()
            print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“ç»Ÿè®¡: {count} ä¸ªæ–‡æ¡£")

            return all_docs
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
            return []

if __name__ == "__main__":
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–åŒ…...")
    required_packages = ['akshare', 'feedparser', 'aiohttp', 'beautifulsoup4', 'scikit-learn', 'jieba', 'googletrans']

    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package} æœªå®‰è£…")

    asyncio.run(test_enhanced_pipeline())