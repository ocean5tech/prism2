#!/usr/bin/env python3
"""
å¢å¼ºçš„æ•°æ®å½’æ¡£æ”¶é›†å™¨
å®Œæ•´ä¿å­˜åŸæ–‡æ•°æ®ï¼ŒåŒ…æ‹¬URLã€åŸå§‹å†…å®¹ã€ä½œè€…ã€æ—¥æœŸç­‰æ‰€æœ‰ä¿¡æ¯
ç”¨äºæ„å»ºå¯é‡ç”¨çš„RAGæ•°æ®æ¡£æ¡ˆ
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import asyncio
import time
import json
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

class EnhancedArchivalCollector:
    """å¢å¼ºçš„æ•°æ®å½’æ¡£æ”¶é›†å™¨ - å®Œæ•´ä¿å­˜åŸæ–‡ä¿¡æ¯"""

    def __init__(self):
        self.vectorizer = None
        self.client = None
        self.collection = None
        self.archive_data = []  # ç”¨äºä¿å­˜å®Œæ•´çš„å½’æ¡£æ•°æ®

    async def __aenter__(self):
        print("ğŸš€ åˆå§‹åŒ–å¢å¼ºæ•°æ®å½’æ¡£æ”¶é›†ç³»ç»Ÿ...")

        # æ¸…ç†ä»£ç†å˜é‡
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=768,
            analyzer='word',
            tokenizer=lambda x: jieba.lcut(x),
            token_pattern=None,
            lowercase=False,
            stop_words=None
        )

        # è¿æ¥åˆ°ChromaDB
        print("ğŸ”— è¿æ¥åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        self.client = chromadb.HttpClient(host='localhost', port=8000)
        self.client.heartbeat()

        try:
            self.client.delete_collection("financial_documents")
            print("   ğŸ—‘ï¸ åˆ é™¤ç°æœ‰é›†åˆ")
        except:
            pass

        self.collection = self.client.create_collection("financial_documents")
        print("âœ… ChromaDBè¿æ¥æˆåŠŸï¼Œåˆ›å»ºæ–°é›†åˆ")

        print("âœ… å¢å¼ºæ•°æ®å½’æ¡£æ”¶é›†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

    def create_document_hash(self, content: str) -> str:
        """ä¸ºæ–‡æ¡£å†…å®¹åˆ›å»ºå”¯ä¸€å“ˆå¸Œæ ‡è¯†"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]

    def create_archive_record(self, raw_data: Any, processed_content: str, metadata: Dict, data_source_info: Dict) -> Dict:
        """åˆ›å»ºå®Œæ•´çš„å½’æ¡£è®°å½•"""
        doc_hash = self.create_document_hash(processed_content)
        timestamp = datetime.now().isoformat()

        archive_record = {
            # å”¯ä¸€æ ‡è¯†
            'document_id': f"archive_{data_source_info['source_type']}_{doc_hash}",
            'document_hash': doc_hash,
            'collection_timestamp': timestamp,

            # åŸå§‹æ•°æ®ä¿å­˜
            'raw_data': {
                'original_response': raw_data,  # å®Œæ•´çš„APIå“åº”
                'raw_content_length': len(str(raw_data)),
                'data_encoding': 'utf-8'
            },

            # æ•°æ®æºè¯¦ç»†ä¿¡æ¯
            'source_info': {
                'source_type': data_source_info['source_type'],
                'api_endpoint': data_source_info.get('api_endpoint', ''),
                'data_provider': data_source_info.get('data_provider', ''),
                'access_method': data_source_info.get('access_method', ''),
                'source_url': data_source_info.get('source_url', ''),
                'source_description': data_source_info.get('source_description', ''),
                'data_license': data_source_info.get('data_license', 'Unknown'),
                'update_frequency': data_source_info.get('update_frequency', 'Unknown'),
                'collection_method': data_source_info.get('collection_method', '')
            },

            # å†…å®¹ä¿¡æ¯
            'content_info': {
                'processed_content': processed_content,
                'content_language': 'zh-cn',
                'content_length': len(processed_content),
                'processing_method': 'text_extraction_and_analysis'
            },

            # å…ƒæ•°æ®
            'metadata': metadata,

            # ä½œè€…å’Œå‘å¸ƒä¿¡æ¯
            'publication_info': {
                'author': metadata.get('author', 'æœªçŸ¥'),
                'publish_date': metadata.get('publish_date', ''),
                'original_title': metadata.get('title', ''),
                'publisher': metadata.get('source', ''),
                'publication_url': metadata.get('link', ''),
                'content_type': metadata.get('doc_type', '')
            },

            # è´¨é‡å’Œé‡è¦æ€§è¯„ä¼°
            'quality_metrics': {
                'importance_score': metadata.get('importance', 5),
                'content_completeness': 'complete' if len(processed_content) > 100 else 'partial',
                'data_reliability': data_source_info.get('reliability_score', 'medium'),
                'freshness_score': self.calculate_freshness_score(metadata.get('publish_date', ''))
            },

            # æŠ€æœ¯ä¿¡æ¯
            'technical_info': {
                'collection_version': '1.0.0',
                'collector_name': 'EnhancedArchivalCollector',
                'processing_pipeline': 'akshare_api -> text_extraction -> jieba_tokenization -> tfidf_vectorization',
                'vector_dimension': 768,
                'embedding_method': 'TF-IDF',
                'storage_format': 'ChromaDB'
            }
        }

        return archive_record

    def calculate_freshness_score(self, publish_date: str) -> float:
        """è®¡ç®—æ•°æ®æ–°é²œåº¦åˆ†æ•°"""
        try:
            if not publish_date:
                return 0.0

            pub_date = datetime.strptime(publish_date, '%Y%m%d')
            days_old = (datetime.now() - pub_date).days

            if days_old <= 1:
                return 1.0
            elif days_old <= 7:
                return 0.8
            elif days_old <= 30:
                return 0.6
            elif days_old <= 90:
                return 0.4
            else:
                return 0.2
        except:
            return 0.0

    async def collect_stocks_with_full_archive(self) -> List[Dict]:
        """æ”¶é›†è‚¡ç¥¨æ•°æ®å¹¶åˆ›å»ºå®Œæ•´å½’æ¡£"""
        print("ğŸ“Š å¼€å§‹æ”¶é›†è‚¡ç¥¨æ•°æ®å¹¶åˆ›å»ºå®Œæ•´å½’æ¡£...")

        try:
            import akshare as ak
            print("âœ… AKShareæ¨¡å—åŠ è½½æˆåŠŸ")

            collected_docs = []

            # ç²¾é€‰é‡è¦è‚¡ç¥¨è¿›è¡Œè¯¦ç»†æ”¶é›†
            important_stocks = {
                '002230': {'name': 'ç§‘å¤§è®¯é£', 'sector': 'äººå·¥æ™ºèƒ½', 'description': 'ä¸­å›½æ™ºèƒ½è¯­éŸ³é¢†å¯¼è€…'},
                '000725': {'name': 'äº¬ä¸œæ–¹A', 'sector': 'æ˜¾ç¤ºæŠ€æœ¯', 'description': 'å…¨çƒåŠå¯¼ä½“æ˜¾ç¤ºé¾™å¤´'},
                '002460': {'name': 'èµ£é”‹é”‚ä¸š', 'sector': 'é”‚ç”µæ± ææ–™', 'description': 'å…¨çƒé”‚åŒ–åˆç‰©é¢†å†›ä¼ä¸š'},
                '300750': {'name': 'å®å¾·æ—¶ä»£', 'sector': 'åŠ¨åŠ›ç”µæ± ', 'description': 'å…¨çƒåŠ¨åŠ›ç”µæ± é¾™å¤´'},
                '300229': {'name': 'æ‹“å°”æ€', 'sector': 'å¤§æ•°æ®AI', 'description': 'ä¸“ä¸šå¤§æ•°æ®å’ŒAIæœåŠ¡å•†'}
            }

            for stock_code, stock_info in important_stocks.items():
                try:
                    print(f"   è¯¦ç»†æ”¶é›† {stock_info['name']}({stock_code}) æ•°æ®...")

                    # æ•°æ®æºä¿¡æ¯
                    data_source_info = {
                        'source_type': 'akshare_stock_data',
                        'api_endpoint': f'ak.stock_zh_a_hist(symbol="{stock_code}")',
                        'data_provider': 'AKShare',
                        'access_method': 'Python API',
                        'source_url': f'https://akshare.akfamily.xyz/data/stock/stock.html#{stock_code}',
                        'source_description': f'AKShareæä¾›çš„{stock_info["name"]}è‚¡ç¥¨å†å²äº¤æ˜“æ•°æ®',
                        'data_license': 'AKShare License',
                        'update_frequency': 'Daily',
                        'collection_method': 'akshare.stock_zh_a_hist API',
                        'reliability_score': 'high'
                    }

                    # è·å–åŸå§‹æ•°æ®
                    raw_stock_data = ak.stock_zh_a_hist(
                        symbol=stock_code,
                        period="daily",
                        start_date="20241001",
                        end_date="20241101"
                    )

                    if not raw_stock_data.empty:
                        latest_data = raw_stock_data.iloc[-1]

                        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        if len(raw_stock_data) > 1:
                            prev_close = raw_stock_data.iloc[-2]['æ”¶ç›˜']
                            change_pct = ((latest_data['æ”¶ç›˜'] - prev_close) / prev_close) * 100
                        else:
                            change_pct = 0

                        # å¤„ç†åçš„å†…å®¹
                        processed_content = f"{stock_info['name']}({stock_code}) - {stock_info['description']}ã€‚æœ€æ–°äº¤æ˜“æ•°æ®ï¼šæ”¶ç›˜ä»·{latest_data.get('æ”¶ç›˜', 'N/A')}å…ƒï¼Œæ¶¨è·Œå¹…{change_pct:.2f}%ï¼Œæˆäº¤é‡{latest_data.get('æˆäº¤é‡', 'N/A')}è‚¡ï¼Œæˆäº¤é¢{latest_data.get('æˆäº¤é¢', 'N/A')}å…ƒã€‚æ‰€å±{stock_info['sector']}æ¿å—ï¼Œä¸º{stock_info['sector']}é¢†åŸŸé‡è¦æŠ•èµ„æ ‡çš„ã€‚"

                        # å…ƒæ•°æ®
                        metadata = {
                            'source': f'AKShare-{stock_info["sector"]}',
                            'doc_type': 'archived_stock_analysis',
                            'sector': stock_info['sector'],
                            'stock_code': stock_code,
                            'company_name': stock_info['name'],
                            'close_price': float(latest_data.get('æ”¶ç›˜', 0)),
                            'volume': int(latest_data.get('æˆäº¤é‡', 0)),
                            'amount': float(latest_data.get('æˆäº¤é¢', 0)),
                            'change_pct': round(change_pct, 2),
                            'publish_date': str(latest_data.name).replace('-', ''),
                            'title': f"{stock_info['name']}è‚¡ç¥¨äº¤æ˜“æ•°æ®",
                            'author': 'AKShareæ•°æ®æº',
                            'link': f'https://akshare.akfamily.xyz/data/stock/stock.html#{stock_code}',
                            'data_source': 'akshare_archived_stocks',
                            'importance': 9,
                            'collection_time': datetime.now().isoformat()
                        }

                        # åˆ›å»ºå®Œæ•´å½’æ¡£è®°å½•
                        archive_record = self.create_archive_record(
                            raw_data=raw_stock_data.to_dict(),
                            processed_content=processed_content,
                            metadata=metadata,
                            data_source_info=data_source_info
                        )

                        # ä¿å­˜åˆ°å½’æ¡£
                        self.archive_data.append(archive_record)

                        # åˆ›å»ºç”¨äºå‘é‡åŒ–çš„æ–‡æ¡£
                        doc = {
                            'id': archive_record['document_id'],
                            'content': processed_content,
                            'metadata': metadata
                        }
                        collected_docs.append(doc)

                        print(f"   âœ… {stock_info['name']} å®Œæ•´å½’æ¡£åˆ›å»ºæˆåŠŸ")
                        await asyncio.sleep(1)  # é¿å…APIé¢‘ç‡é™åˆ¶

                except Exception as e:
                    print(f"   âš ï¸ è·å–è‚¡ç¥¨{stock_code}æ•°æ®å¤±è´¥: {e}")

            # æ”¶é›†å¤®è§†è´¢ç»æ–°é—»å¹¶å½’æ¡£
            print("   æ”¶é›†å¤®è§†è´¢ç»æ–°é—»å¹¶åˆ›å»ºå½’æ¡£...")
            try:
                raw_news_data = ak.news_cctv()
                data_source_info = {
                    'source_type': 'akshare_cctv_news',
                    'api_endpoint': 'ak.news_cctv()',
                    'data_provider': 'AKShare + CCTV',
                    'access_method': 'Python API',
                    'source_url': 'http://www.cctv.com/finance/',
                    'source_description': 'AKShareæä¾›çš„å¤®è§†è´¢ç»æ–°é—»æ•°æ®',
                    'data_license': 'AKShare License + CCTV',
                    'update_frequency': 'Hourly',
                    'collection_method': 'akshare.news_cctv API',
                    'reliability_score': 'very_high'
                }

                if not raw_news_data.empty:
                    for _, news in raw_news_data.head(5).iterrows():
                        processed_content = f"{news.get('title', '')} {news.get('content', '')}"

                        metadata = {
                            'source': 'å¤®è§†è´¢ç»å®˜æ–¹',
                            'doc_type': 'archived_financial_news',
                            'news_type': 'æƒå¨è´¢ç»èµ„è®¯',
                            'title': news.get('title', ''),
                            'author': 'å¤®è§†è´¢ç»è®°è€…',
                            'publish_date': str(news.get('date', datetime.now().date())).replace('-', ''),
                            'link': 'http://www.cctv.com/finance/',
                            'data_source': 'akshare_cctv_archived',
                            'importance': 9,
                            'collection_time': datetime.now().isoformat()
                        }

                        # åˆ›å»ºå½’æ¡£è®°å½•
                        archive_record = self.create_archive_record(
                            raw_data=news.to_dict(),
                            processed_content=processed_content,
                            metadata=metadata,
                            data_source_info=data_source_info
                        )

                        self.archive_data.append(archive_record)

                        doc = {
                            'id': archive_record['document_id'],
                            'content': processed_content,
                            'metadata': metadata
                        }
                        collected_docs.append(doc)

                    print(f"   âœ… å¤®è§†è´¢ç»æ–°é—»å½’æ¡£: {len(raw_news_data.head(5))} æ¡")

            except Exception as e:
                print(f"   âš ï¸ å¤®è§†æ–°é—»å½’æ¡£å¤±è´¥: {e}")

            print(f"âœ… å®Œæ•´æ•°æ®å½’æ¡£æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(collected_docs)} ä¸ªæ–‡æ¡£")
            print(f"ğŸ“ å½’æ¡£è®°å½•: {len(self.archive_data)} ä¸ªå®Œæ•´æ¡£æ¡ˆ")
            return collected_docs

        except ImportError:
            print("âŒ AKShareæœªå®‰è£…")
            return []
        except Exception as e:
            print(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []

    def store_documents_to_vector_db(self, documents: List[Dict]) -> int:
        """å­˜å‚¨æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        if not documents:
            return 0

        print(f"ğŸ§  å¼€å§‹å‘é‡åŒ–å­˜å‚¨ {len(documents)} ä¸ªå½’æ¡£æ–‡æ¡£...")

        try:
            texts = [doc['content'] for doc in documents]
            doc_ids = [doc['id'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]

            print("   ğŸ”„ ç”ŸæˆTF-IDFå‘é‡åµŒå…¥...")
            start_time = time.time()
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            embeddings = tfidf_matrix.toarray()
            embedding_time = time.time() - start_time

            print(f"   âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’")
            print(f"   ğŸ“Š å‘é‡ç»´åº¦: {embeddings.shape[1]}")

            print("   ğŸ’¾ å­˜å‚¨åˆ°ChromaDB...")
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=doc_ids
            )

            print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªå½’æ¡£æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return len(documents)

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            return 0

    def save_complete_archive(self) -> str:
        """ä¿å­˜å®Œæ•´çš„æ•°æ®å½’æ¡£"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¸»å½’æ¡£æ–‡ä»¶
        archive_file = f"/home/wyatt/prism2/rag-service/complete_data_archive_{timestamp}.json"

        # å½’æ¡£å…ƒä¿¡æ¯
        archive_metadata = {
            'archive_info': {
                'creation_date': datetime.now().isoformat(),
                'archive_version': '1.0.0',
                'total_documents': len(self.archive_data),
                'data_sources': list(set([record['source_info']['source_type'] for record in self.archive_data])),
                'archive_purpose': 'Complete financial data archive for RAG system reconstruction',
                'archive_format': 'JSON with full metadata and raw data',
                'reconstruction_instructions': 'Use this archive to rebuild RAG database without re-downloading'
            },
            'documents': self.archive_data
        }

        # ä¿å­˜å®Œæ•´å½’æ¡£ (å¤„ç†æ—¥æœŸåºåˆ—åŒ–)
        with open(archive_file, 'w', encoding='utf-8') as f:
            json.dump(archive_metadata, f, ensure_ascii=False, indent=2, default=str)

        print(f"ğŸ“ å®Œæ•´æ•°æ®å½’æ¡£å·²ä¿å­˜: {archive_file}")
        return archive_file

    async def collect_all_with_archive(self) -> tuple:
        """æ”¶é›†æ‰€æœ‰æ•°æ®å¹¶åˆ›å»ºå®Œæ•´å½’æ¡£"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®æ”¶é›†å’Œå½’æ¡£...")
        start_time = time.time()

        # æ”¶é›†æ•°æ®
        all_docs = await self.collect_stocks_with_full_archive()

        total_time = time.time() - start_time

        print(f"\nğŸ‰ å®Œæ•´æ•°æ®æ”¶é›†å’Œå½’æ¡£å®Œæˆ!")
        print(f"   ğŸ“Š å‘é‡åŒ–æ–‡æ¡£æ•°: {len(all_docs)}")
        print(f"   ğŸ“ å½’æ¡£è®°å½•æ•°: {len(self.archive_data)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")

        # å‘é‡åŒ–å­˜å‚¨
        if all_docs:
            print(f"\nğŸ§  å¼€å§‹å‘é‡åŒ–å­˜å‚¨...")
            stored_count = self.store_documents_to_vector_db(all_docs)
            print(f"âœ… å‘é‡åŒ–å­˜å‚¨å®Œæˆ: {stored_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")

        # ä¿å­˜å®Œæ•´å½’æ¡£
        archive_file = self.save_complete_archive()

        return all_docs, archive_file

async def test_archival_system():
    """æµ‹è¯•å®Œæ•´å½’æ¡£ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºæ•°æ®å½’æ¡£ç³»ç»Ÿ")
    print("=" * 80)

    async with EnhancedArchivalCollector() as collector:
        # æ”¶é›†å¹¶å½’æ¡£æ•°æ®
        docs, archive_file = await collector.collect_all_with_archive()

        if docs:
            # éªŒè¯æ•°æ®åº“
            count = collector.collection.count()
            print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“ç»Ÿè®¡: {count} ä¸ªæ–‡æ¡£")
            print(f"ğŸ“ å®Œæ•´å½’æ¡£æ–‡ä»¶: {archive_file}")

            print(f"\nâœ… æ•°æ®å½’æ¡£ç‰¹æ€§:")
            print(f"   - åŸå§‹APIå“åº”æ•°æ®: å®Œæ•´ä¿å­˜")
            print(f"   - æ•°æ®æºURLå’Œæè¿°: è¯¦ç»†è®°å½•")
            print(f"   - ä½œè€…å’Œå‘å¸ƒä¿¡æ¯: å®Œæ•´å…ƒæ•°æ®")
            print(f"   - æ•°æ®æ–°é²œåº¦è¯„åˆ†: è‡ªåŠ¨è®¡ç®—")
            print(f"   - é‡å»ºRAGåº“: æ”¯æŒç¦»çº¿é‡å»º")

            return docs, archive_file
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
            return [], ""

if __name__ == "__main__":
    asyncio.run(test_archival_system())