#!/usr/bin/env python3
"""
åˆ›å»ºRAGç¤ºä¾‹æ•°æ®ç”¨äºæŸ¥çœ‹
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import time

def create_sample_rag_data():
    """åˆ›å»ºç¤ºä¾‹RAGæ•°æ®"""
    print("ğŸ¯ åˆ›å»ºRAGç¤ºä¾‹æ•°æ®")
    print("=" * 50)

    # Clear proxy variables
    for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
        if proxy_var in os.environ:
            del os.environ[proxy_var]

    try:
        from app.services.vector_service import vector_service
        from app.models.requests import DocumentInput

        # Connect to ChromaDB
        if not vector_service.connect():
            print("âŒ æ— æ³•è¿æ¥åˆ°ChromaDB")
            return False

        print("âœ… è¿æ¥åˆ°ChromaDBæˆåŠŸ")

        # Create sample financial documents
        sample_docs = [
            DocumentInput(
                id="sample_analysis_001",
                content="å¹³å®‰é“¶è¡Œä¸‰å­£åº¦å‡€åˆ©æ¶¦390.2äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿6.2%ã€‚ä¸è‰¯è´·æ¬¾ç‡1.05%ï¼Œè¾ƒä¸Šå­£åº¦ä¸‹é™5bpã€‚ROEæŒç»­æå‡ï¼Œèµ„äº§è´¨é‡æ”¹å–„æ˜æ˜¾ã€‚ç»´æŒä¹°å…¥è¯„çº§ï¼Œç›®æ ‡ä»·20.5å…ƒã€‚",
                metadata={
                    "stock_code": "000001",
                    "company_name": "å¹³å®‰é“¶è¡Œ",
                    "doc_type": "quarterly_analysis",
                    "source": "åˆ¸å•†ç ”æŠ¥",
                    "publish_date": "2024-10-30",
                    "rating": "ä¹°å…¥",
                    "target_price": "20.5",
                    "analyst": "é‡‘èåˆ†æå¸ˆ",
                    "importance": 9
                }
            ),
            DocumentInput(
                id="sample_policy_news_001",
                content="å¤®è¡Œå®£å¸ƒé™å‡†0.25ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾æµåŠ¨æ€§çº¦5000äº¿å…ƒã€‚æ­¤æ¬¡é™å‡†ä½“ç°è´§å¸æ”¿ç­–å‰ç»æ€§è°ƒèŠ‚ï¼Œé“¶è¡Œè‚¡ã€åœ°äº§è‚¡ç­‰åˆ©ç‡æ•æ„Ÿæ¿å—æœ‰æœ›å—ç›Šã€‚å¤šå®¶æœºæ„è®¤ä¸ºè¿™æ˜¯æ”¿ç­–å®½æ¾ä¿¡å·ã€‚",
                metadata={
                    "doc_type": "policy_news",
                    "source": "è´¢ç»æ–°é—»",
                    "publish_date": "2024-10-29",
                    "category": "è´§å¸æ”¿ç­–",
                    "impact_level": "é«˜",
                    "affected_sectors": "é“¶è¡Œ,åœ°äº§,ç§‘æŠ€",
                    "sentiment": "ç§¯æ",
                    "importance": 8
                }
            ),
            DocumentInput(
                id="sample_ai_research_001",
                content="äººå·¥æ™ºèƒ½äº§ä¸šé“¾è¿æ¥å‘å±•æœºé‡ã€‚AIèŠ¯ç‰‡éœ€æ±‚çˆ†å‘ï¼Œè®¾è®¡å…¬å¸è®¢å•é¥±æ»¡ã€‚äº‘è®¡ç®—å‚å•†å—ç›ŠAIè®­ç»ƒéœ€æ±‚å¢é•¿ã€‚åº”ç”¨è½¯ä»¶ä¼ä¸šAIèµ‹èƒ½æ•ˆæœæ˜¾è‘—ï¼Œç”¨æˆ·ä»˜è´¹æ„æ„¿å¢å¼ºã€‚å»ºè®®é‡ç‚¹å…³æ³¨æŠ€æœ¯é¢†å…ˆçš„é¾™å¤´ä¼ä¸šã€‚",
                metadata={
                    "doc_type": "sector_research",
                    "source": "è¡Œä¸šç ”æŠ¥",
                    "publish_date": "2024-10-28",
                    "sector": "äººå·¥æ™ºèƒ½",
                    "sub_sectors": "èŠ¯ç‰‡è®¾è®¡,ç®—åŠ›æœåŠ¡,åº”ç”¨è½¯ä»¶",
                    "investment_theme": "æ–°è´¨ç”Ÿäº§åŠ›",
                    "time_horizon": "é•¿æœŸ",
                    "risk_level": "ä¸­ç­‰",
                    "importance": 7
                }
            ),
            DocumentInput(
                id="sample_market_trend_001",
                content="Aè‚¡å¸‚åœºå‘ˆç°ç»“æ„æ€§è¡Œæƒ…ï¼Œæ–°èƒ½æºã€åŒ»è¯ã€ç§‘æŠ€ç­‰æˆé•¿æ¿å—è¡¨ç°æ´»è·ƒã€‚æœºæ„èµ„é‡‘æŒç»­æµå…¥ä¼˜è´¨èµ›é“ã€‚åŒ—å‘èµ„é‡‘è¿ç»­å‡€æµå…¥ï¼Œå¤–èµ„çœ‹å¥½ä¸­å›½èµ„äº§é•¿æœŸä»·å€¼ã€‚å»ºè®®å…³æ³¨åŸºæœ¬é¢æ”¹å–„çš„ä¼˜è´¨æˆé•¿è‚¡ã€‚",
                metadata={
                    "doc_type": "market_analysis",
                    "source": "å¸‚åœºåˆ†æ",
                    "publish_date": "2024-10-27",
                    "market_trend": "ç»“æ„æ€§è¡Œæƒ…",
                    "hot_sectors": "æ–°èƒ½æº,åŒ»è¯,ç§‘æŠ€",
                    "fund_flow": "å‡€æµå…¥",
                    "investment_style": "æˆé•¿è‚¡",
                    "importance": 6
                }
            ),
            DocumentInput(
                id="sample_company_news_001",
                content="æ¯”äºšè¿ªå‘å¸ƒ10æœˆé”€é‡æ•°æ®ï¼Œæ–°èƒ½æºæ±½è½¦é”€é‡è¾¾åˆ°30.1ä¸‡è¾†ï¼ŒåŒæ¯”å¢é•¿15.2%ã€‚å…¶ä¸­çº¯ç”µåŠ¨è½¦å‹é”€é‡å æ¯”è¶…è¿‡60%ã€‚å…¬å¸æµ·å¤–å¸‚åœºæ‹“å±•é¡ºåˆ©ï¼Œå‡ºå£é‡åˆ›å†å²æ–°é«˜ã€‚ä¸‰å­£åº¦ä¸šç»©è¶…é¢„æœŸã€‚",
                metadata={
                    "stock_code": "002594",
                    "company_name": "æ¯”äºšè¿ª",
                    "doc_type": "company_news",
                    "source": "ä¸Šå¸‚å…¬å¸å…¬å‘Š",
                    "publish_date": "2024-11-01",
                    "news_type": "é”€é‡æ•°æ®",
                    "sector": "æ–°èƒ½æºæ±½è½¦",
                    "performance": "è¶…é¢„æœŸ",
                    "importance": 8
                }
            )
        ]

        # Generate mock embeddings (768 dimensions like bge-large-zh-v1.5)
        embeddings = []
        for i, doc in enumerate(sample_docs):
            # Different document types get different vector patterns
            if "analysis" in doc.metadata.get("doc_type", ""):
                base_vector = [0.1 + i*0.01] * 768
            elif "policy" in doc.metadata.get("doc_type", ""):
                base_vector = [0.2 + i*0.01] * 768
            elif "research" in doc.metadata.get("doc_type", ""):
                base_vector = [0.3 + i*0.01] * 768
            elif "market" in doc.metadata.get("doc_type", ""):
                base_vector = [0.4 + i*0.01] * 768
            else:
                base_vector = [0.5 + i*0.01] * 768

            embeddings.append(base_vector)

        # Store documents
        print(f"ğŸ“„ å‡†å¤‡å­˜å‚¨ {len(sample_docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£...")

        processed_count, failed_docs = vector_service.add_documents(
            documents=sample_docs,
            embeddings=embeddings
        )

        print(f"âœ… å­˜å‚¨å®Œæˆ:")
        print(f"   æˆåŠŸ: {processed_count} ä¸ªæ–‡æ¡£")
        print(f"   å¤±è´¥: {len(failed_docs)} ä¸ªæ–‡æ¡£")

        if failed_docs:
            print(f"   å¤±è´¥æ–‡æ¡£: {failed_docs}")

        # Verify storage
        stats = vector_service.get_collection_stats()
        print(f"\nğŸ“Š æ•°æ®åº“çŠ¶æ€:")
        print(f"   é›†åˆ: {stats['collection_name']}")
        print(f"   æ–‡æ¡£æ•°: {stats['document_count']}")
        print(f"   çŠ¶æ€: {stats['status']}")

        print(f"\nğŸ‰ ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ!")
        print(f"ç°åœ¨å¯ä»¥ä½¿ç”¨ RAG æ•°æ®åº“æŸ¥çœ‹å™¨æŸ¥çœ‹å†…å®¹")

        return True

    except Exception as e:
        print(f"âŒ åˆ›å»ºç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    create_sample_rag_data()