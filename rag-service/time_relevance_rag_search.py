#!/usr/bin/env python3
"""
æ—¶é—´æ€§ + ç›¸å…³æ€§RAGæœç´¢ç³»ç»Ÿ
é’ˆå¯¹è‚¡ç¥¨æŠ•èµ„åœºæ™¯ï¼Œä¼˜å…ˆè¿”å›æœ€æ–°+æœ€ç›¸å…³çš„ç»“æœ
"""

import sys
import os
sys.path.append('/home/wyatt/prism2/rag-service')

import chromadb
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import jieba

class TimeRelevanceRAGSearch:
    def __init__(self):
        # Clear proxy variables
        for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
            if proxy_var in os.environ:
                del os.environ[proxy_var]

        self.client = None
        self.connected = False
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            stop_words=None  # ä¸­æ–‡åˆ†è¯å¤„ç†
        )

    def connect(self):
        """è¿æ¥åˆ°ChromaDB"""
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            self.client.heartbeat()
            self.connected = True
            print("âœ… è¿æ¥åˆ°RAGæ•°æ®åº“æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False

    def chinese_tokenize(self, text: str) -> str:
        """ä¸­æ–‡åˆ†è¯å¤„ç†"""
        return ' '.join(jieba.cut(text))

    def parse_date_from_metadata(self, metadata: Dict) -> datetime:
        """ä»å…ƒæ•°æ®ä¸­è§£ææ—¥æœŸ"""
        # å°è¯•å¤šç§æ—¥æœŸå­—æ®µ
        date_fields = ['timestamp', 'date', 'created_at', 'publish_time', 'time']

        for field in date_fields:
            if field in metadata:
                date_str = str(metadata[field])
                try:
                    # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
                    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Y%m%d', '%Y-%m-%d %H:%M:%S.%f']:
                        try:
                            return datetime.strptime(date_str, fmt)
                        except ValueError:
                            continue

                    # å°è¯•ISOæ ¼å¼
                    if 'T' in date_str:
                        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))

                except Exception:
                    continue

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼ï¼ˆå¾ˆä¹…ä»¥å‰çš„æ—¥æœŸï¼‰
        return datetime(2020, 1, 1)

    def calculate_time_score(self, doc_date: datetime) -> float:
        """è®¡ç®—æ—¶é—´åˆ†æ•°ï¼ˆè¶Šæ–°åˆ†æ•°è¶Šé«˜ï¼‰"""
        now = datetime.now()
        days_diff = (now - doc_date).days

        # æ—¶é—´è¡°å‡å‡½æ•°ï¼š24å°æ—¶å†…=1.0ï¼Œ1å‘¨å†…=0.8ï¼Œ1æœˆå†…=0.6ï¼Œ3æœˆå†…=0.4ï¼Œæ›´ä¹…=0.2
        if days_diff <= 1:
            return 1.0
        elif days_diff <= 7:
            return 0.8
        elif days_diff <= 30:
            return 0.6
        elif days_diff <= 90:
            return 0.4
        else:
            return 0.2

    def calculate_relevance_score(self, query: str, documents: List[str]) -> List[float]:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        if not documents:
            return []

        # ä¸­æ–‡åˆ†è¯å¤„ç†
        query_processed = self.chinese_tokenize(query)
        docs_processed = [self.chinese_tokenize(doc) for doc in documents]

        try:
            # æ„å»ºTF-IDFçŸ©é˜µ
            all_texts = [query_processed] + docs_processed
            tfidf_matrix = self.vectorizer.fit_transform(all_texts)

            # è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦
            query_vector = tfidf_matrix[0:1]
            doc_vectors = tfidf_matrix[1:]

            similarities = cosine_similarity(query_vector, doc_vectors)[0]
            return similarities.tolist()

        except Exception as e:
            print(f"âš ï¸ ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çš„å…³é”®è¯åŒ¹é…
            scores = []
            query_words = set(jieba.cut(query.lower()))

            for doc in documents:
                doc_words = set(jieba.cut(doc.lower()))
                overlap = len(query_words & doc_words)
                score = overlap / max(len(query_words), 1)
                scores.append(score)

            return scores

    def search_with_time_relevance(
        self,
        collection_name: str,
        query: str,
        limit: int = 10,
        time_weight: float = 0.3,
        relevance_weight: float = 0.7
    ) -> List[Dict]:
        """
        åŸºäºæ—¶é—´æ€§å’Œç›¸å…³æ€§çš„æ··åˆæœç´¢

        Args:
            collection_name: é›†åˆåç§°
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡
            time_weight: æ—¶é—´æƒé‡ (0-1)
            relevance_weight: ç›¸å…³æ€§æƒé‡ (0-1)
        """
        if not self.connected:
            print("âŒ æœªè¿æ¥åˆ°æ•°æ®åº“")
            return []

        try:
            collection = self.client.get_collection(collection_name)

            # è·å–æ‰€æœ‰æ–‡æ¡£
            results = collection.get(include=['documents', 'metadatas'])

            if not results['ids']:
                print("ğŸ“Š é›†åˆä¸ºç©º")
                return []

            documents = results['documents']
            metadatas = results['metadatas']
            ids = results['ids']

            print(f"ğŸ” åœ¨ {len(documents)} ä¸ªæ–‡æ¡£ä¸­æœç´¢: '{query}'")

            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            relevance_scores = self.calculate_relevance_score(query, documents)

            # è®¡ç®—ç»¼åˆåˆ†æ•°
            scored_docs = []
            for i, doc_id in enumerate(ids):
                doc_date = self.parse_date_from_metadata(metadatas[i] if metadatas else {})
                time_score = self.calculate_time_score(doc_date)
                relevance_score = relevance_scores[i] if i < len(relevance_scores) else 0.0

                # ç»¼åˆåˆ†æ•° = æ—¶é—´æƒé‡ * æ—¶é—´åˆ†æ•° + ç›¸å…³æ€§æƒé‡ * ç›¸å…³æ€§åˆ†æ•°
                final_score = time_weight * time_score + relevance_weight * relevance_score

                scored_docs.append({
                    'id': doc_id,
                    'content': documents[i],
                    'metadata': metadatas[i] if metadatas else {},
                    'doc_date': doc_date,
                    'time_score': time_score,
                    'relevance_score': relevance_score,
                    'final_score': final_score
                })

            # æŒ‰ç»¼åˆåˆ†æ•°é™åºæ’åº
            scored_docs.sort(key=lambda x: x['final_score'], reverse=True)

            # è¿”å›å‰Nä¸ªç»“æœ
            return scored_docs[:limit]

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def display_search_results(self, results: List[Dict], query: str):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        if not results:
            print("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            return

        print(f"\n{'='*80}")
        print(f"ğŸ¯ æœç´¢ç»“æœ: '{query}'")
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼ˆæŒ‰æ—¶é—´æ€§+ç›¸å…³æ€§æ’åºï¼‰")
        print("="*80)

        for i, result in enumerate(results):
            print(f"\n{i+1}. ğŸ“„ {result['id']}")
            print(f"   ğŸ“… æ—¶é—´: {result['doc_date'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   â° æ—¶é—´åˆ†æ•°: {result['time_score']:.3f}")
            print(f"   ğŸ¯ ç›¸å…³æ€§åˆ†æ•°: {result['relevance_score']:.3f}")
            print(f"   ğŸ“Š ç»¼åˆåˆ†æ•°: {result['final_score']:.3f}")
            print(f"   ğŸ“ å†…å®¹: {result['content'][:200]}{'...' if len(result['content']) > 200 else ''}")

            # æ˜¾ç¤ºå…³é”®å…ƒæ•°æ®
            metadata = result['metadata']
            key_fields = ['source_type', 'importance', 'stock_code', 'category']
            meta_info = []
            for field in key_fields:
                if field in metadata:
                    meta_info.append(f"{field}: {metadata[field]}")

            if meta_info:
                print(f"   ğŸ·ï¸  {' | '.join(meta_info)}")

            print("-" * 80)

    def interactive_search(self):
        """äº¤äº’å¼æœç´¢"""
        if not self.connect():
            return

        # è·å–å¯ç”¨é›†åˆ
        try:
            collections = self.client.list_collections()
            collection_names = [col.name for col in collections]

            if not collection_names:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°é›†åˆ")
                return

            print(f"ğŸ“ å¯ç”¨é›†åˆ: {collection_names}")
            collection_name = collection_names[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé›†åˆ

        except Exception as e:
            print(f"âŒ è·å–é›†åˆå¤±è´¥: {e}")
            return

        print(f"\nğŸ›ï¸  æ—¶é—´æ€§+ç›¸å…³æ€§RAGæœç´¢")
        print(f"ğŸ“ ä½¿ç”¨é›†åˆ: {collection_name}")
        print("="*60)

        while True:
            try:
                print(f"\nè¯·è¾“å…¥æœç´¢æŸ¥è¯¢ (è¾“å…¥ 'quit' é€€å‡º):")
                query = input("ğŸ” > ").strip()

                if query.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break

                if not query:
                    continue

                # æœç´¢å‚æ•°
                print(f"\næœç´¢å‚æ•°é…ç½®:")
                print(f"1. ä½¿ç”¨é»˜è®¤æƒé‡ (æ—¶é—´30%, ç›¸å…³æ€§70%)")
                print(f"2. è‡ªå®šä¹‰æƒé‡")

                weight_choice = input("é€‰æ‹© (1/2): ").strip()

                if weight_choice == '2':
                    try:
                        time_weight = float(input("æ—¶é—´æƒé‡ (0-1): "))
                        relevance_weight = float(input("ç›¸å…³æ€§æƒé‡ (0-1): "))

                        # æ ‡å‡†åŒ–æƒé‡
                        total = time_weight + relevance_weight
                        if total > 0:
                            time_weight /= total
                            relevance_weight /= total
                        else:
                            time_weight, relevance_weight = 0.3, 0.7

                    except ValueError:
                        time_weight, relevance_weight = 0.3, 0.7
                        print("âš ï¸ ä½¿ç”¨é»˜è®¤æƒé‡")
                else:
                    time_weight, relevance_weight = 0.3, 0.7

                limit = 10
                try:
                    limit_input = input(f"è¿”å›ç»“æœæ•°é‡ (é»˜è®¤{limit}): ").strip()
                    if limit_input:
                        limit = int(limit_input)
                except ValueError:
                    pass

                print(f"\nğŸ” æœç´¢ä¸­... (æ—¶é—´æƒé‡: {time_weight:.1%}, ç›¸å…³æ€§æƒé‡: {relevance_weight:.1%})")

                # æ‰§è¡Œæœç´¢
                results = self.search_with_time_relevance(
                    collection_name,
                    query,
                    limit=limit,
                    time_weight=time_weight,
                    relevance_weight=relevance_weight
                )

                # æ˜¾ç¤ºç»“æœ
                self.display_search_results(results, query)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ æœç´¢é”™è¯¯: {e}")

def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    print("ğŸ¯ æ—¶é—´æ€§+ç›¸å…³æ€§RAGæœç´¢ç³»ç»Ÿ")
    print("ä¸“ä¸ºè‚¡ç¥¨æŠ•èµ„åœºæ™¯ä¼˜åŒ–ï¼Œä¼˜å…ˆè¿”å›æœ€æ–°+æœ€ç›¸å…³çš„ç»“æœ")
    print("="*80)

    search_engine = TimeRelevanceRAGSearch()

    if len(sys.argv) > 1:
        # å‘½ä»¤è¡Œæ¨¡å¼
        if not search_engine.connect():
            return

        query = ' '.join(sys.argv[1:])

        # è·å–çœŸå®RSSæ–°é—»é›†åˆ
        try:
            collections = search_engine.client.list_collections()
            collection_name = "real_financial_news"  # æŒ‡å®šçœŸå®RSSæ–°é—»é›†åˆ
            if collections:
                results = search_engine.search_with_time_relevance(collection_name, query)
                search_engine.display_search_results(results, query)
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°é›†åˆ")
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
    else:
        # äº¤äº’æ¨¡å¼
        search_engine.interactive_search()

if __name__ == "__main__":
    main()